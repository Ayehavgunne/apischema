{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Overview \u00b6 Apischema \u00b6 Makes your life easier when it comes to python API. JSON (de)serialization + schema generation through python typing, with some sugar and magic. Install \u00b6 pip install apischema It requires only Python 3.6+ (and dataclasses official backport for version 3.6 only) Why another library? \u00b6 This library fulfill the following goals: stay as close as possible to the standard library (dataclasses, typing, etc.) to be as accessible as possible \u2014 as a consequence do not need plugins for editor/linter/etc.; be additive and tunable, be able to work with user own types (ORM, etc.) as well as foreign libraries ones; do not need a PR for handling new types like bson.ObjectId , avoid subclassing; avoid dynamic things like using string for attribute name. No known alternative achieves that. Example \u00b6 from dataclasses import dataclass , field from typing import Set from uuid import UUID , uuid4 from pytest import raises from apischema import ValidationError , deserialize , serialize # Define a schema with standard dataclasses from apischema.json_schema import deserialization_schema @dataclass class Resource : id : UUID name : str tags : Set [ str ] = field ( default_factory = set ) # Get some data uuid = uuid4 () data = { \"id\" : str ( uuid ), \"name\" : \"wyfo\" , \"tags\" : [ \"some_tag\" ]} # Deserialize data resource = deserialize ( Resource , data ) assert resource == Resource ( uuid , \"wyfo\" , { \"some_tag\" }) # Serialize objects assert serialize ( resource ) == data # Validate during deserialization with raises ( ValidationError ) as err : # pytest check exception is raised deserialize ( Resource , { \"id\" : \"42\" , \"name\" : \"wyfo\" }) assert serialize ( err . value ) == [ # ValidationError is serializable { \"loc\" : [ \"id\" ], \"err\" : [ \"badly formed hexadecimal UUID string\" ]} ] # Generate JSON Schema assert deserialization_schema ( Resource ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"type\" : \"object\" , \"properties\" : { \"id\" : { \"type\" : \"string\" , \"format\" : \"uuid\" }, \"name\" : { \"type\" : \"string\" }, \"tags\" : { \"type\" : \"array\" , \"items\" : { \"type\" : \"string\" }, \"uniqueItems\" : True }, }, \"required\" : [ \"id\" , \"name\" ], \"additionalProperties\" : False , } Apischema works out of the box with you data model. (This example and further ones are using pytest stuff because they are in fact run as tests in the library CI) FAQ \u00b6 I already have my data model with my SQLAlchemy /ORM tables, will I have to duplicate my code, making one dataclass by table? \u00b6 Why would you have to duplicate them? Apischema can \"work with user own types as well as foreign libraries ones\". Some teasing of conversion feature: you can add default serialization for all your tables, or register different serializer that you can select according to your API endpoint, or both. So SQLAlchemy is supported? Does it support others library? \u00b6 No, in fact, no library are supported, even SQLAlchemy ; it was a choice made to be as small and generic as possible, and to support only the standard library (with types like datetime , UUID ). However, the library is flexible enough to code yourself the support you need with, I hope, the minimal effort. It's of course not excluded to add support in additional small plugin libraries. Feedbacks are welcome about the best way to do things. I need more accurate validation than \"ensure this is an integer and no a string \", can I do that? \u00b6 See the validation section. You can use standard JSON schema validation ( maxItems , pattern , etc.) that will be embedded in your schema or add custom Python validators for each class/fields/ NewType you want. Let's start the Apischema tour.","title":"Overview"},{"location":"#overview","text":"","title":"Overview"},{"location":"#apischema","text":"Makes your life easier when it comes to python API. JSON (de)serialization + schema generation through python typing, with some sugar and magic.","title":"Apischema"},{"location":"#install","text":"pip install apischema It requires only Python 3.6+ (and dataclasses official backport for version 3.6 only)","title":"Install"},{"location":"#why-another-library","text":"This library fulfill the following goals: stay as close as possible to the standard library (dataclasses, typing, etc.) to be as accessible as possible \u2014 as a consequence do not need plugins for editor/linter/etc.; be additive and tunable, be able to work with user own types (ORM, etc.) as well as foreign libraries ones; do not need a PR for handling new types like bson.ObjectId , avoid subclassing; avoid dynamic things like using string for attribute name. No known alternative achieves that.","title":"Why another library?"},{"location":"#example","text":"from dataclasses import dataclass , field from typing import Set from uuid import UUID , uuid4 from pytest import raises from apischema import ValidationError , deserialize , serialize # Define a schema with standard dataclasses from apischema.json_schema import deserialization_schema @dataclass class Resource : id : UUID name : str tags : Set [ str ] = field ( default_factory = set ) # Get some data uuid = uuid4 () data = { \"id\" : str ( uuid ), \"name\" : \"wyfo\" , \"tags\" : [ \"some_tag\" ]} # Deserialize data resource = deserialize ( Resource , data ) assert resource == Resource ( uuid , \"wyfo\" , { \"some_tag\" }) # Serialize objects assert serialize ( resource ) == data # Validate during deserialization with raises ( ValidationError ) as err : # pytest check exception is raised deserialize ( Resource , { \"id\" : \"42\" , \"name\" : \"wyfo\" }) assert serialize ( err . value ) == [ # ValidationError is serializable { \"loc\" : [ \"id\" ], \"err\" : [ \"badly formed hexadecimal UUID string\" ]} ] # Generate JSON Schema assert deserialization_schema ( Resource ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"type\" : \"object\" , \"properties\" : { \"id\" : { \"type\" : \"string\" , \"format\" : \"uuid\" }, \"name\" : { \"type\" : \"string\" }, \"tags\" : { \"type\" : \"array\" , \"items\" : { \"type\" : \"string\" }, \"uniqueItems\" : True }, }, \"required\" : [ \"id\" , \"name\" ], \"additionalProperties\" : False , } Apischema works out of the box with you data model. (This example and further ones are using pytest stuff because they are in fact run as tests in the library CI)","title":"Example"},{"location":"#faq","text":"","title":"FAQ"},{"location":"#i-already-have-my-data-model-with-my-sqlalchemyorm-tables-will-i-have-to-duplicate-my-code-making-one-dataclass-by-table","text":"Why would you have to duplicate them? Apischema can \"work with user own types as well as foreign libraries ones\". Some teasing of conversion feature: you can add default serialization for all your tables, or register different serializer that you can select according to your API endpoint, or both.","title":"I already have my data model with my SQLAlchemy/ORM tables, will I have to duplicate my code, making one dataclass by table?"},{"location":"#so-sqlalchemy-is-supported-does-it-support-others-library","text":"No, in fact, no library are supported, even SQLAlchemy ; it was a choice made to be as small and generic as possible, and to support only the standard library (with types like datetime , UUID ). However, the library is flexible enough to code yourself the support you need with, I hope, the minimal effort. It's of course not excluded to add support in additional small plugin libraries. Feedbacks are welcome about the best way to do things.","title":"So SQLAlchemy is supported? Does it support others library?"},{"location":"#i-need-more-accurate-validation-than-ensure-this-is-an-integer-and-no-a-string-can-i-do-that","text":"See the validation section. You can use standard JSON schema validation ( maxItems , pattern , etc.) that will be embedded in your schema or add custom Python validators for each class/fields/ NewType you want. Let's start the Apischema tour.","title":"I need more accurate validation than \"ensure this is an integer and no a string \", can I do that?"},{"location":"benchmark/","text":"Benchmark \u00b6 Note Benchmark presented is just Pydantic benchmark where Apischema has been \"inserted\" . Benchmark is run without Cython compilation . Below are the results of crude benchmark comparing apischema to pydantic and other validation libraries. Package Version Relative Performance Mean deserialization time apischema 0.7.0 108.0\u03bcs pydantic 1.6.1 1.1x slower 119.5\u03bcs attrs + cattrs 19.1.0 1.1x slower 123.1\u03bcs valideer 0.4.2 1.2x slower 124.5\u03bcs marshmallow 3.5.2 1.9x slower 203.9\u03bcs voluptuous 0.11.7 2.4x slower 254.2\u03bcs trafaret 1.2.0 2.9x slower 312.2\u03bcs django-rest-framework 3.10.2 8.4x slower 911.3\u03bcs cerberus 1.3.2 22.4x slower 2416.6\u03bcs Package Version Relative Performance Mean serialization time apischema 0.7.0 29.3\u03bcs pydantic 1.6.1 2.0x slower 58.2\u03bcs Benchmarks were run with Python 3.7.7 (CPython) and the package versions listed above installed via pypi on macOs 10.15.5 Note Using PyPy , Apischema lead is even more confirmed. FAQ \u00b6 Why not use Cython ? \u00b6 Cython doesn't support yet some of the modern Python features like Generic or dataclass which are intensively used in Apischema . Note With some hacks, it has been possible to cythonize some of the modules, with performance increase; it becomes then a little bit slower than cythonized Pydantic (which is optimized for Cython ). But this is too dirty to keep it, and Cython is not a priority. Why not ask for integration to pydantic benchmark? \u00b6 Done, but rejected because \"apischema doesn't have enough usage\". Let's change that!","title":"Benchmark"},{"location":"benchmark/#benchmark","text":"Note Benchmark presented is just Pydantic benchmark where Apischema has been \"inserted\" . Benchmark is run without Cython compilation . Below are the results of crude benchmark comparing apischema to pydantic and other validation libraries. Package Version Relative Performance Mean deserialization time apischema 0.7.0 108.0\u03bcs pydantic 1.6.1 1.1x slower 119.5\u03bcs attrs + cattrs 19.1.0 1.1x slower 123.1\u03bcs valideer 0.4.2 1.2x slower 124.5\u03bcs marshmallow 3.5.2 1.9x slower 203.9\u03bcs voluptuous 0.11.7 2.4x slower 254.2\u03bcs trafaret 1.2.0 2.9x slower 312.2\u03bcs django-rest-framework 3.10.2 8.4x slower 911.3\u03bcs cerberus 1.3.2 22.4x slower 2416.6\u03bcs Package Version Relative Performance Mean serialization time apischema 0.7.0 29.3\u03bcs pydantic 1.6.1 2.0x slower 58.2\u03bcs Benchmarks were run with Python 3.7.7 (CPython) and the package versions listed above installed via pypi on macOs 10.15.5 Note Using PyPy , Apischema lead is even more confirmed.","title":"Benchmark"},{"location":"benchmark/#faq","text":"","title":"FAQ"},{"location":"benchmark/#why-not-use-cython","text":"Cython doesn't support yet some of the modern Python features like Generic or dataclass which are intensively used in Apischema . Note With some hacks, it has been possible to cythonize some of the modules, with performance increase; it becomes then a little bit slower than cythonized Pydantic (which is optimized for Cython ). But this is too dirty to keep it, and Cython is not a priority.","title":"Why not use Cython?"},{"location":"benchmark/#why-not-ask-for-integration-to-pydantic-benchmark","text":"Done, but rejected because \"apischema doesn't have enough usage\". Let's change that!","title":"Why not ask for integration to pydantic benchmark?"},{"location":"changelog/","text":"Changelog \u00b6 0.7.0 \u00b6 Add changelog \ud83d\ude0a","title":"Changelog"},{"location":"changelog/#changelog","text":"","title":"Changelog"},{"location":"changelog/#070","text":"Add changelog \ud83d\ude0a","title":"0.7.0"},{"location":"configuration_management/","text":"Configuration management \u00b6 Apischema is a multi-purpose deserialization library. If it's main use should be in API endpoint, that doesn't prevent to use it somewhere else, to parse application configuration for instance. For JSON/YAML configurations, there is no need to develop further the use of library. But there is many more ways of storing configurations, with environment variables or key-value store for example. When configurations are simple, there could be no need for a library like Apischema . However when it comes to being larger with some structure, it can play its cards right. Dot-seraparated key-value \u00b6 A common way of storing complex nested structures into flat key-value stores is to use dot-separated keys. Apischema provides an utility function to unflat this kind of data into nested JSON-like data, which can then be deserialized. from dataclasses import dataclass from ipaddress import IPv4Address , IPv4Network from typing import Collection from apischema import deserialize from apischema.deserialization import unflat_key_value # Could be extracted from ini file, or environment variables with `os.environ` configuration = [ ( \"database.host\" , \"191.132.67.45\" ), ( \"database.user\" , \"guest\" ), ( \"filtered_subnets.0\" , \"247.252.191.00/24\" ), ( \"filtered_subnets.1\" , \"207.34.172.00/24\" ), ] @dataclass class Database : host : IPv4Address user : str @dataclass class Config : database : Database filtered_subnets : Collection [ IPv4Network ] # separator parameter is defaulted to \".\" but shown for example assert unflat_key_value ( configuration , separator = \".\" ) == { \"database\" : { \"host\" : \"191.132.67.45\" , \"user\" : \"guest\" }, \"filtered_subnets\" : [ \"247.252.191.00/24\" , \"207.34.172.00/24\" ], } assert deserialize ( Config , unflat_key_value ( configuration ), coercion = True , additional_properties = True ) == Config ( database = Database ( IPv4Address ( \"191.132.67.45\" ), \"guest\" ), filtered_subnets = ( IPv4Network ( \"247.252.191.00/24\" ), IPv4Network ( \"207.34.172.00/24\" ), ), )","title":"Configuration management"},{"location":"configuration_management/#configuration-management","text":"Apischema is a multi-purpose deserialization library. If it's main use should be in API endpoint, that doesn't prevent to use it somewhere else, to parse application configuration for instance. For JSON/YAML configurations, there is no need to develop further the use of library. But there is many more ways of storing configurations, with environment variables or key-value store for example. When configurations are simple, there could be no need for a library like Apischema . However when it comes to being larger with some structure, it can play its cards right.","title":"Configuration management"},{"location":"configuration_management/#dot-seraparated-key-value","text":"A common way of storing complex nested structures into flat key-value stores is to use dot-separated keys. Apischema provides an utility function to unflat this kind of data into nested JSON-like data, which can then be deserialized. from dataclasses import dataclass from ipaddress import IPv4Address , IPv4Network from typing import Collection from apischema import deserialize from apischema.deserialization import unflat_key_value # Could be extracted from ini file, or environment variables with `os.environ` configuration = [ ( \"database.host\" , \"191.132.67.45\" ), ( \"database.user\" , \"guest\" ), ( \"filtered_subnets.0\" , \"247.252.191.00/24\" ), ( \"filtered_subnets.1\" , \"207.34.172.00/24\" ), ] @dataclass class Database : host : IPv4Address user : str @dataclass class Config : database : Database filtered_subnets : Collection [ IPv4Network ] # separator parameter is defaulted to \".\" but shown for example assert unflat_key_value ( configuration , separator = \".\" ) == { \"database\" : { \"host\" : \"191.132.67.45\" , \"user\" : \"guest\" }, \"filtered_subnets\" : [ \"247.252.191.00/24\" , \"207.34.172.00/24\" ], } assert deserialize ( Config , unflat_key_value ( configuration ), coercion = True , additional_properties = True ) == Config ( database = Database ( IPv4Address ( \"191.132.67.45\" ), \"guest\" ), filtered_subnets = ( IPv4Network ( \"247.252.191.00/24\" ), IPv4Network ( \"207.34.172.00/24\" ), ), )","title":"Dot-seraparated key-value"},{"location":"conversions/","text":"(De)serialization: Customize with conversions \u00b6 Apischema covers majority of standard data types, but it's of course not enough, that's why it gives you the way to add support for all your classes and the libraries you use. Actually, Apischema uses internally its own feature to support standard library data types like UUID/datetime/etc. (see std_types.py ) ORM support can easily be achieved with this feature (see SQLAlchemy example ). Principle - Apischema conversions \u00b6 An Apischema conversion is composed of a source type, let's call it Source , a target type Target and a function of signature (Source) -> Target . When a type (actually, a non-builtin type, so not int / List[str] /etc.) is deserialized, Apischema will look if there is a conversion where this type is the target. If found, the source type of conversion will be deserialized, then conversion function will be applied to get an object of the expected type. Serialization works the same (inverted) way: look for a conversion with type as source, apply conversion (normally get the target type). Conversions are also handled in schema generation: for a deserialization schema, source schema is used (after merging target schema annotations) while target schema is used (after merging source schema annotations) for a serialization schema. Register a conversion \u00b6 Conversion is registered using deserializer / serializer for deserialization/serialization respectively. When used as a decorator, the Source / Target types are directly extracted from conversion function signature. They can also be passed as argument when the function has no type annotations (builtins like datetime.isoformat or foreign library functions). Methods can be used for serializer , as well as classmethod / staticmethod for both (especially deserializer ) from dataclasses import dataclass from typing import NewType from apischema import deserialize , deserializer , schema , serialize , serializer from apischema.json_schema import deserialization_schema , serialization_schema HexaRGB = NewType ( \"HexaRGB\" , str ) schema ( pattern = r \"^#[0-9a-fA-F] {6} $\" )( HexaRGB ) @dataclass class RGB : red : int green : int blue : int @serializer def hexa ( self ) -> HexaRGB : return HexaRGB ( f \"# { self . red : 02x }{ self . green : 02x }{ self . blue : 02x } \" ) @deserializer def from_hexa ( hexa : HexaRGB ) -> \"RGB\" : return RGB ( int ( hexa [ 1 : 3 ], 16 ), int ( hexa [ 3 : 5 ], 16 ), int ( hexa [ 5 : 7 ], 16 )) assert deserialize ( RGB , \"#000000\" ) == RGB ( 0 , 0 , 0 ) assert serialize ( RGB ( 0 , 0 , 42 )) == \"#00002a\" assert ( deserialization_schema ( RGB ) == serialization_schema ( RGB ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"type\" : \"string\" , \"pattern\" : \"^#[0-9a-fA-F] {6} $\" , } ) Multiple deserializers \u00b6 Sometimes, you want to have several possibilities to deserialize a type. If it's possible to register a deserializer with an Union param, it's not very practical. That's why Apischema make it possible to register several deserializers for the same type. They will be handled with an Union source type (ordered by deserializers registration), with the right serializer selected according to the matching alternative. import os import time from datetime import datetime from apischema import deserialize , deserializer from apischema.json_schema import deserialization_schema # Set UTC timezone for example os . environ [ \"TZ\" ] = \"UTC\" time . tzset () # There is already `deserializer(datetime.fromisoformat, str, datetime) in apischema # Let's add an other deserializer for datetime from a timestamp @deserializer def datetime_from_timestamp ( timestamp : int ) -> datetime : return datetime . fromtimestamp ( timestamp ) assert deserialization_schema ( datetime ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"anyOf\" : [{ \"type\" : \"string\" , \"format\" : \"date-time\" }, { \"type\" : \"integer\" }], } assert ( deserialize ( datetime , \"2019-10-13\" ) == datetime ( 2019 , 10 , 13 ) == deserialize ( datetime , 1570924800 ) ) Note If it seems that the deserializer declared is equivalent to deserializer(datetime.fromtimestamp, int, datetime) , there is actually a slight difference: in the example, the deserializer makes 2 function calls ( datetime_from_timestamp and datetime.fromtimestamp ) while the second inlined form imply only one function call to datetime.fromtimestamp . In Python, function calls are heavy, so it's good to know. On the other hand, serializer registration overwrite the previous registration if any. That's how the default serialization of builtin types like datetime can be modified (because it's just a serializer call in Apischema code). This is not possible to overwrite this way deserializers (because they stack), but reset_deserializers can be used to reset them before adding new ones. Also, self_deserializer can be used to add a class itself as a deserializer (when it's a supported type like a dataclass). Inheritance \u00b6 All serializers are naturally inherited. In fact, with a conversion function (Source) -> Target , you can always pass a subtype of Source and get a Target in return. Moreover, when serializer is a method (and no param is passed to serializer , overriding this method in a subclass will override the inherited serializer. from apischema import serialize , serializer class Foo : pass @serializer def serialize_foo ( foo : Foo ) -> int : return 0 class Foo2 ( Foo ): pass # Deserializer is inherited assert serialize ( Foo ()) == serialize ( Foo2 ()) == 0 class Bar : @serializer def serialize ( self ) -> int : return 0 class Bar2 ( Bar ): def serialize ( self ) -> int : return 1 # Deserializer is inherited and overridden assert serialize ( Bar ()) == 0 != serialize ( Bar2 ()) == 1 On the other hand, deserializers cannot be inherited, because the same Source passed to a conversion function (Source) -> Target will always give the same Target (not ensured to be the desired subtype). However, there is one way to do it by using a classmethod and the special decorator inherited_deserializer ; the class parameter of the method is then assumed to be used to instantiate the return. from apischema import deserialize from apischema.conversion import inherited_deserializer class Foo : def __init__ ( self , n : int ): self . n = int def __eq__ ( self , other ): return type ( self ) == type ( other ) and self . n == other . n @inherited_deserializer @classmethod def from_int ( cls , n : int ): return cls ( n ) class Bar ( Foo ): pass assert deserialize ( Foo , 0 ) == Foo ( 0 ) assert deserialize ( Bar , 0 ) == Bar ( 0 ) != Foo ( 0 ) Note An \"other\" way to achieve that would be to use __init_subclass__ method in order to add a deserializer to each subclass. In fact, that's what inherited_deserializer is doing behind the scene. Extra conversions - choose the conversion you want \u00b6 Conversion is a powerful feature, but, registering only one (de)serialization by type may not be enough. Some types may have different representations, or you may have different serialization for a given entity with more or less data (for example a \"simple\" and a \"detailed\" view). Hopefully, Apischema let you register as many conversion as you want for your classes and gives you the possibility to select the one you want. Conversions registered with deserializer / serializer are the default ones, they are selected when no conversion is precised. Other conversions are registered extra_deserializer / extra_serializer (they have the same signature than the previous ones). Conversions can then be selected using the conversions parameter of Apischema functions deserialize / serialize / deserialization_schema / serialization_schema . This parameter must be mapping of types: for deserialization, target as key and source(s) as value for serialization, source as key and target as value (Actually, the type for which is registered the conversion is in key) For deserialization, if there is several possible source , conversions values can also be a collection of types. It will again result in a Union deserialization. Note For definitions_schema , conversions can be added with types by using a tuple instead, for example definitions_schema(serializations=[(List[Foo], {Foo: Bar})]) . import os import time from dataclasses import dataclass from datetime import datetime from typing import NewType from apischema import deserialize , serialize from apischema.conversion import extra_serializer from apischema.conversion.converters import extra_deserializer # Set UTC timezone for example os . environ [ \"TZ\" ] = \"UTC\" time . tzset () @extra_deserializer def datetime_from_timestamp ( timestamp : int ) -> datetime : return datetime . fromtimestamp ( timestamp ) date = datetime ( 2017 , 9 , 2 ) assert deserialize ( datetime , 1504310400 , conversions = { datetime : int }) == date Diff = NewType ( \"Diff\" , int ) @dataclass class Foo : bar : int baz : int @extra_serializer def summary ( self ) -> int : return self . bar + self . baz # You can use NewType to disambiguate conversion to int @extra_serializer def diff ( self ) -> Diff : return Diff ( self . bar - self . baz ) assert serialize ( Foo ( 0 , 1 )) == { \"bar\" : 0 , \"baz\" : 1 } assert serialize ( Foo ( 0 , 1 ), conversions = { Foo : Foo }) == { \"bar\" : 0 , \"baz\" : 1 } assert serialize ( Foo ( 0 , 1 ), conversions = { Foo : int }) == 1 assert serialize ( Foo ( 0 , 1 ), conversions = { Foo : Diff }) == - 1 Chain conversions \u00b6 Conversions mapping put in conversions parameter is not used in all the deserialization/serialization. In fact it is \"reset\" as soon as a non-builtin type (so, not int / List[int] / NewType instances/etc.) is encountered. Not having this reset would completely break the possibility to have $ref in generated schema, because a conversions could then change the serialization of a field of a dataclass in one particular schema but not in another (and bye-bye OpenAPI components schema). But everything is not lost. Let's illustrate with an example. As previously mentioned, Apischema uses its own feature internally at several places. One of them is schema generation. JSON schema is generated using an internal JsonSchema type, and is then serialized; the JSON schema version selection result in fact in a conversion that is selected according to the version (by JsonSchemaVersion.conversions property). However, JSON schema is recursive and the serialization of JsonSchema returns a dictionary which can contain other JsonSchema ... but it has been written above that conversions is reset. That's why deserializer and others conversion registers have a conversions parameter that will be taken as the new conversions after the conversion application. In JSON schema example, it allows sub- JsonSchema to be serialized with the correct conversions . The following example is extracted from Apischema code : from typing import Any , Dict , Mapping , NewType from apischema.conversion import extra_serializer class JsonSchema ( Dict [ str , Any ]): pass JsonSchema7 = NewType ( \"JsonSchema7\" , Mapping [ str , Any ]) def isolate_ref ( schema : Dict [ str , Any ]): if \"$ref\" in schema and len ( schema ) > 1 : schema . setdefault ( \"allOf\" , []) . append ({ \"$ref\" : schema . pop ( \"$ref\" )}) @extra_serializer ( conversions = { JsonSchema : JsonSchema7 }) def to_json_schema_7 ( schema : JsonSchema ) -> JsonSchema7 : result = schema . copy () isolate_ref ( result ) if \"$defs\" in result : result [ \"definitions\" ] = { ** result . pop ( \"$defs\" ), ** result . get ( \"definitions\" , {})} if \"dependentRequired\" in result : result [ \"dependencies\" ] = { ** result . pop ( \"dependentRequired\" ), ** result . get ( \"dependencies\" , {}), } return JsonSchema7 ( result ) Field conversions \u00b6 Dataclass fields conversions can also be customized using conversions metadata. import os import time from dataclasses import dataclass , field from datetime import datetime from apischema import deserialize , serialize from apischema.conversion import extra_serializer from apischema.conversion.converters import extra_deserializer from apischema.metadata import conversions # Set UTC timezone for example os . environ [ \"TZ\" ] = \"UTC\" time . tzset () extra_deserializer ( datetime . fromtimestamp , int , datetime ) @extra_serializer def to_timestamp ( d : datetime ) -> int : return int ( d . timestamp ()) @dataclass class Foo : some_date : datetime = field ( metadata = conversions ( int )) # `conversions(int)` is equivalent to # `conversions(deserialization={datetime: int}, serialization={datetime: int})` other_date : datetime assert deserialize ( Foo , { \"some_date\" : 0 , \"other_date\" : \"2019-10-13\" }) == Foo ( datetime ( 1970 , 1 , 1 ), datetime ( 2019 , 10 , 13 ) ) assert serialize ( Foo ( datetime ( 1970 , 1 , 1 ), datetime ( 2019 , 10 , 13 ))) == { \"some_date\" : 0 , \"other_date\" : \"2019-10-13T00:00:00\" , } conversions metadata can also be used to add directly a (de)serializer to a field \u2014 deserialization and serialization are then applied after the (de)serializer as chained conversions import os import time from dataclasses import dataclass , field from datetime import datetime from apischema import deserialize , serialize from apischema.metadata import conversions # Set UTC timezone for example os . environ [ \"TZ\" ] = \"UTC\" time . tzset () def from_timestamp ( t : int ) -> datetime : return datetime . fromtimestamp ( t ) def to_timestamp ( d : datetime ) -> int : return int ( d . timestamp ()) @dataclass class Foo : some_date : datetime = field ( metadata = conversions ( deserializer = from_timestamp , serializer = to_timestamp ) ) other_date : datetime assert deserialize ( Foo , { \"some_date\" : 0 , \"other_date\" : \"2019-10-13\" }) == Foo ( datetime ( 1970 , 1 , 1 ), datetime ( 2019 , 10 , 13 ) ) assert serialize ( Foo ( datetime ( 1970 , 1 , 1 ), datetime ( 2019 , 10 , 13 ))) == { \"some_date\" : 0 , \"other_date\" : \"2019-10-13T00:00:00\" , } Generic conversions \u00b6 Generic conversions are supported out of the box. However, keep in mind that serialization doesn't use type model, so they will not be used in serialization, but will be used on the other hand in serialization schema generation. import sys from typing import Generic , List , TypeVar from pytest import raises from apischema import ValidationError , deserialize , deserializer , serialize , serializer from apischema.json_schema import deserialization_schema , serialization_schema T = TypeVar ( \"T\" ) class Wrapper ( Generic [ T ]): def __init__ ( self , wrapped : T ): self . wrapped = wrapped if sys . version_info >= ( 3 , 7 ): # Methods of generic classes are not handled before 3.7 @serializer def _wrapped ( self ) -> T : return self . wrapped else : def _wrapped ( self ) -> T : return self . wrapped if sys . version_info <= ( 3 , 7 ): serializer ( Wrapper . _wrapped , Wrapper [ T ]) U = TypeVar ( \"U\" ) deserializer ( Wrapper , U , Wrapper [ U ]) assert deserialize ( Wrapper [ List [ int ]], [ 0 , 1 ]) . wrapped == [ 0 , 1 ] with raises ( ValidationError ): deserialize ( Wrapper [ int ], \"wrapped\" ) assert serialize ( Wrapper ( \"wrapped\" )) == \"wrapped\" assert ( deserialization_schema ( Wrapper [ int ]) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"type\" : \"integer\" } == serialization_schema ( Wrapper [ int ]) ) Warning As shown in example, methods of Generic classes are not handled before 3.7 Note Apischema doesn't support specialization of Generic conversion like Foo[bool] -> int . That's not all \u00b6 Also not (yet) presented in this section : raw deserializers , dataclass serializers and global default (de)serialization . FAQ \u00b6 Why conversions parameter is a mapping and not just a tuple? Are there any cases where it can be several conversions at the same time? \u00b6 Tuples (and unions in case of deserialization)","title":"(De)serialization: Customize with conversions"},{"location":"conversions/#deserialization-customize-with-conversions","text":"Apischema covers majority of standard data types, but it's of course not enough, that's why it gives you the way to add support for all your classes and the libraries you use. Actually, Apischema uses internally its own feature to support standard library data types like UUID/datetime/etc. (see std_types.py ) ORM support can easily be achieved with this feature (see SQLAlchemy example ).","title":"(De)serialization: Customize with conversions"},{"location":"conversions/#principle-apischema-conversions","text":"An Apischema conversion is composed of a source type, let's call it Source , a target type Target and a function of signature (Source) -> Target . When a type (actually, a non-builtin type, so not int / List[str] /etc.) is deserialized, Apischema will look if there is a conversion where this type is the target. If found, the source type of conversion will be deserialized, then conversion function will be applied to get an object of the expected type. Serialization works the same (inverted) way: look for a conversion with type as source, apply conversion (normally get the target type). Conversions are also handled in schema generation: for a deserialization schema, source schema is used (after merging target schema annotations) while target schema is used (after merging source schema annotations) for a serialization schema.","title":"Principle - Apischema conversions"},{"location":"conversions/#register-a-conversion","text":"Conversion is registered using deserializer / serializer for deserialization/serialization respectively. When used as a decorator, the Source / Target types are directly extracted from conversion function signature. They can also be passed as argument when the function has no type annotations (builtins like datetime.isoformat or foreign library functions). Methods can be used for serializer , as well as classmethod / staticmethod for both (especially deserializer ) from dataclasses import dataclass from typing import NewType from apischema import deserialize , deserializer , schema , serialize , serializer from apischema.json_schema import deserialization_schema , serialization_schema HexaRGB = NewType ( \"HexaRGB\" , str ) schema ( pattern = r \"^#[0-9a-fA-F] {6} $\" )( HexaRGB ) @dataclass class RGB : red : int green : int blue : int @serializer def hexa ( self ) -> HexaRGB : return HexaRGB ( f \"# { self . red : 02x }{ self . green : 02x }{ self . blue : 02x } \" ) @deserializer def from_hexa ( hexa : HexaRGB ) -> \"RGB\" : return RGB ( int ( hexa [ 1 : 3 ], 16 ), int ( hexa [ 3 : 5 ], 16 ), int ( hexa [ 5 : 7 ], 16 )) assert deserialize ( RGB , \"#000000\" ) == RGB ( 0 , 0 , 0 ) assert serialize ( RGB ( 0 , 0 , 42 )) == \"#00002a\" assert ( deserialization_schema ( RGB ) == serialization_schema ( RGB ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"type\" : \"string\" , \"pattern\" : \"^#[0-9a-fA-F] {6} $\" , } )","title":"Register a conversion"},{"location":"conversions/#multiple-deserializers","text":"Sometimes, you want to have several possibilities to deserialize a type. If it's possible to register a deserializer with an Union param, it's not very practical. That's why Apischema make it possible to register several deserializers for the same type. They will be handled with an Union source type (ordered by deserializers registration), with the right serializer selected according to the matching alternative. import os import time from datetime import datetime from apischema import deserialize , deserializer from apischema.json_schema import deserialization_schema # Set UTC timezone for example os . environ [ \"TZ\" ] = \"UTC\" time . tzset () # There is already `deserializer(datetime.fromisoformat, str, datetime) in apischema # Let's add an other deserializer for datetime from a timestamp @deserializer def datetime_from_timestamp ( timestamp : int ) -> datetime : return datetime . fromtimestamp ( timestamp ) assert deserialization_schema ( datetime ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"anyOf\" : [{ \"type\" : \"string\" , \"format\" : \"date-time\" }, { \"type\" : \"integer\" }], } assert ( deserialize ( datetime , \"2019-10-13\" ) == datetime ( 2019 , 10 , 13 ) == deserialize ( datetime , 1570924800 ) ) Note If it seems that the deserializer declared is equivalent to deserializer(datetime.fromtimestamp, int, datetime) , there is actually a slight difference: in the example, the deserializer makes 2 function calls ( datetime_from_timestamp and datetime.fromtimestamp ) while the second inlined form imply only one function call to datetime.fromtimestamp . In Python, function calls are heavy, so it's good to know. On the other hand, serializer registration overwrite the previous registration if any. That's how the default serialization of builtin types like datetime can be modified (because it's just a serializer call in Apischema code). This is not possible to overwrite this way deserializers (because they stack), but reset_deserializers can be used to reset them before adding new ones. Also, self_deserializer can be used to add a class itself as a deserializer (when it's a supported type like a dataclass).","title":"Multiple deserializers"},{"location":"conversions/#inheritance","text":"All serializers are naturally inherited. In fact, with a conversion function (Source) -> Target , you can always pass a subtype of Source and get a Target in return. Moreover, when serializer is a method (and no param is passed to serializer , overriding this method in a subclass will override the inherited serializer. from apischema import serialize , serializer class Foo : pass @serializer def serialize_foo ( foo : Foo ) -> int : return 0 class Foo2 ( Foo ): pass # Deserializer is inherited assert serialize ( Foo ()) == serialize ( Foo2 ()) == 0 class Bar : @serializer def serialize ( self ) -> int : return 0 class Bar2 ( Bar ): def serialize ( self ) -> int : return 1 # Deserializer is inherited and overridden assert serialize ( Bar ()) == 0 != serialize ( Bar2 ()) == 1 On the other hand, deserializers cannot be inherited, because the same Source passed to a conversion function (Source) -> Target will always give the same Target (not ensured to be the desired subtype). However, there is one way to do it by using a classmethod and the special decorator inherited_deserializer ; the class parameter of the method is then assumed to be used to instantiate the return. from apischema import deserialize from apischema.conversion import inherited_deserializer class Foo : def __init__ ( self , n : int ): self . n = int def __eq__ ( self , other ): return type ( self ) == type ( other ) and self . n == other . n @inherited_deserializer @classmethod def from_int ( cls , n : int ): return cls ( n ) class Bar ( Foo ): pass assert deserialize ( Foo , 0 ) == Foo ( 0 ) assert deserialize ( Bar , 0 ) == Bar ( 0 ) != Foo ( 0 ) Note An \"other\" way to achieve that would be to use __init_subclass__ method in order to add a deserializer to each subclass. In fact, that's what inherited_deserializer is doing behind the scene.","title":"Inheritance"},{"location":"conversions/#extra-conversions-choose-the-conversion-you-want","text":"Conversion is a powerful feature, but, registering only one (de)serialization by type may not be enough. Some types may have different representations, or you may have different serialization for a given entity with more or less data (for example a \"simple\" and a \"detailed\" view). Hopefully, Apischema let you register as many conversion as you want for your classes and gives you the possibility to select the one you want. Conversions registered with deserializer / serializer are the default ones, they are selected when no conversion is precised. Other conversions are registered extra_deserializer / extra_serializer (they have the same signature than the previous ones). Conversions can then be selected using the conversions parameter of Apischema functions deserialize / serialize / deserialization_schema / serialization_schema . This parameter must be mapping of types: for deserialization, target as key and source(s) as value for serialization, source as key and target as value (Actually, the type for which is registered the conversion is in key) For deserialization, if there is several possible source , conversions values can also be a collection of types. It will again result in a Union deserialization. Note For definitions_schema , conversions can be added with types by using a tuple instead, for example definitions_schema(serializations=[(List[Foo], {Foo: Bar})]) . import os import time from dataclasses import dataclass from datetime import datetime from typing import NewType from apischema import deserialize , serialize from apischema.conversion import extra_serializer from apischema.conversion.converters import extra_deserializer # Set UTC timezone for example os . environ [ \"TZ\" ] = \"UTC\" time . tzset () @extra_deserializer def datetime_from_timestamp ( timestamp : int ) -> datetime : return datetime . fromtimestamp ( timestamp ) date = datetime ( 2017 , 9 , 2 ) assert deserialize ( datetime , 1504310400 , conversions = { datetime : int }) == date Diff = NewType ( \"Diff\" , int ) @dataclass class Foo : bar : int baz : int @extra_serializer def summary ( self ) -> int : return self . bar + self . baz # You can use NewType to disambiguate conversion to int @extra_serializer def diff ( self ) -> Diff : return Diff ( self . bar - self . baz ) assert serialize ( Foo ( 0 , 1 )) == { \"bar\" : 0 , \"baz\" : 1 } assert serialize ( Foo ( 0 , 1 ), conversions = { Foo : Foo }) == { \"bar\" : 0 , \"baz\" : 1 } assert serialize ( Foo ( 0 , 1 ), conversions = { Foo : int }) == 1 assert serialize ( Foo ( 0 , 1 ), conversions = { Foo : Diff }) == - 1","title":"Extra conversions - choose the conversion you want"},{"location":"conversions/#chain-conversions","text":"Conversions mapping put in conversions parameter is not used in all the deserialization/serialization. In fact it is \"reset\" as soon as a non-builtin type (so, not int / List[int] / NewType instances/etc.) is encountered. Not having this reset would completely break the possibility to have $ref in generated schema, because a conversions could then change the serialization of a field of a dataclass in one particular schema but not in another (and bye-bye OpenAPI components schema). But everything is not lost. Let's illustrate with an example. As previously mentioned, Apischema uses its own feature internally at several places. One of them is schema generation. JSON schema is generated using an internal JsonSchema type, and is then serialized; the JSON schema version selection result in fact in a conversion that is selected according to the version (by JsonSchemaVersion.conversions property). However, JSON schema is recursive and the serialization of JsonSchema returns a dictionary which can contain other JsonSchema ... but it has been written above that conversions is reset. That's why deserializer and others conversion registers have a conversions parameter that will be taken as the new conversions after the conversion application. In JSON schema example, it allows sub- JsonSchema to be serialized with the correct conversions . The following example is extracted from Apischema code : from typing import Any , Dict , Mapping , NewType from apischema.conversion import extra_serializer class JsonSchema ( Dict [ str , Any ]): pass JsonSchema7 = NewType ( \"JsonSchema7\" , Mapping [ str , Any ]) def isolate_ref ( schema : Dict [ str , Any ]): if \"$ref\" in schema and len ( schema ) > 1 : schema . setdefault ( \"allOf\" , []) . append ({ \"$ref\" : schema . pop ( \"$ref\" )}) @extra_serializer ( conversions = { JsonSchema : JsonSchema7 }) def to_json_schema_7 ( schema : JsonSchema ) -> JsonSchema7 : result = schema . copy () isolate_ref ( result ) if \"$defs\" in result : result [ \"definitions\" ] = { ** result . pop ( \"$defs\" ), ** result . get ( \"definitions\" , {})} if \"dependentRequired\" in result : result [ \"dependencies\" ] = { ** result . pop ( \"dependentRequired\" ), ** result . get ( \"dependencies\" , {}), } return JsonSchema7 ( result )","title":"Chain conversions"},{"location":"conversions/#field-conversions","text":"Dataclass fields conversions can also be customized using conversions metadata. import os import time from dataclasses import dataclass , field from datetime import datetime from apischema import deserialize , serialize from apischema.conversion import extra_serializer from apischema.conversion.converters import extra_deserializer from apischema.metadata import conversions # Set UTC timezone for example os . environ [ \"TZ\" ] = \"UTC\" time . tzset () extra_deserializer ( datetime . fromtimestamp , int , datetime ) @extra_serializer def to_timestamp ( d : datetime ) -> int : return int ( d . timestamp ()) @dataclass class Foo : some_date : datetime = field ( metadata = conversions ( int )) # `conversions(int)` is equivalent to # `conversions(deserialization={datetime: int}, serialization={datetime: int})` other_date : datetime assert deserialize ( Foo , { \"some_date\" : 0 , \"other_date\" : \"2019-10-13\" }) == Foo ( datetime ( 1970 , 1 , 1 ), datetime ( 2019 , 10 , 13 ) ) assert serialize ( Foo ( datetime ( 1970 , 1 , 1 ), datetime ( 2019 , 10 , 13 ))) == { \"some_date\" : 0 , \"other_date\" : \"2019-10-13T00:00:00\" , } conversions metadata can also be used to add directly a (de)serializer to a field \u2014 deserialization and serialization are then applied after the (de)serializer as chained conversions import os import time from dataclasses import dataclass , field from datetime import datetime from apischema import deserialize , serialize from apischema.metadata import conversions # Set UTC timezone for example os . environ [ \"TZ\" ] = \"UTC\" time . tzset () def from_timestamp ( t : int ) -> datetime : return datetime . fromtimestamp ( t ) def to_timestamp ( d : datetime ) -> int : return int ( d . timestamp ()) @dataclass class Foo : some_date : datetime = field ( metadata = conversions ( deserializer = from_timestamp , serializer = to_timestamp ) ) other_date : datetime assert deserialize ( Foo , { \"some_date\" : 0 , \"other_date\" : \"2019-10-13\" }) == Foo ( datetime ( 1970 , 1 , 1 ), datetime ( 2019 , 10 , 13 ) ) assert serialize ( Foo ( datetime ( 1970 , 1 , 1 ), datetime ( 2019 , 10 , 13 ))) == { \"some_date\" : 0 , \"other_date\" : \"2019-10-13T00:00:00\" , }","title":"Field conversions"},{"location":"conversions/#generic-conversions","text":"Generic conversions are supported out of the box. However, keep in mind that serialization doesn't use type model, so they will not be used in serialization, but will be used on the other hand in serialization schema generation. import sys from typing import Generic , List , TypeVar from pytest import raises from apischema import ValidationError , deserialize , deserializer , serialize , serializer from apischema.json_schema import deserialization_schema , serialization_schema T = TypeVar ( \"T\" ) class Wrapper ( Generic [ T ]): def __init__ ( self , wrapped : T ): self . wrapped = wrapped if sys . version_info >= ( 3 , 7 ): # Methods of generic classes are not handled before 3.7 @serializer def _wrapped ( self ) -> T : return self . wrapped else : def _wrapped ( self ) -> T : return self . wrapped if sys . version_info <= ( 3 , 7 ): serializer ( Wrapper . _wrapped , Wrapper [ T ]) U = TypeVar ( \"U\" ) deserializer ( Wrapper , U , Wrapper [ U ]) assert deserialize ( Wrapper [ List [ int ]], [ 0 , 1 ]) . wrapped == [ 0 , 1 ] with raises ( ValidationError ): deserialize ( Wrapper [ int ], \"wrapped\" ) assert serialize ( Wrapper ( \"wrapped\" )) == \"wrapped\" assert ( deserialization_schema ( Wrapper [ int ]) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"type\" : \"integer\" } == serialization_schema ( Wrapper [ int ]) ) Warning As shown in example, methods of Generic classes are not handled before 3.7 Note Apischema doesn't support specialization of Generic conversion like Foo[bool] -> int .","title":"Generic conversions"},{"location":"conversions/#thats-not-all","text":"Also not (yet) presented in this section : raw deserializers , dataclass serializers and global default (de)serialization .","title":"That's not all"},{"location":"conversions/#faq","text":"","title":"FAQ"},{"location":"conversions/#why-conversions-parameter-is-a-mapping-and-not-just-a-tuple-are-there-any-cases-where-it-can-be-several-conversions-at-the-same-time","text":"Tuples (and unions in case of deserialization)","title":"Why conversions parameter is a mapping and not just a tuple? Are there any cases where it can be several conversions at the same time?"},{"location":"data_model/","text":"Data model \u00b6 Apischema handle every classes/types you need. By the way, it's done in an additive way, meaning that it doesn't affect your types. PEP 585 \u00b6 With Python 3.9 and PEP 585 , typing is substantially shaken up. Because Apischema must be up-to-date with modern Python, support of Python 3.9 covers PEP 585. But because 3.9 is not yet released and far from being widely adopted, the following documentation will use old fashioned Python typing with typing module. Dataclasses \u00b6 Because the library aims to bring the minimum boilerplate, it's build on the top of standard library. Dataclasses are thus the core structure of the data model. Dataclasses bring the possibility of field customization, with more than just a default value. In addition to the common parameters of dataclasses.field , customization is done with metadata parameter. With some teasing of features presented later: from dataclasses import dataclass , field from apischema import alias , schema from apischema.metadata import required @dataclass class Foo : bar : int = field ( default = 0 , metadata = alias ( \"foo_bar\" ) | schema ( title = \"foo! bar!\" , min = 0 , max = 42 ) | required , ) Note Field's metadata are just an ordinary dict ; Apischema provides some functions to enrich these metadata with it's own keys ( alias(\"foo_bar) is roughly equivalent to `{\"_apischema_alias\": \"foo_bar\"}) and use them when the time comes, but metadata are not reserved to Apischema and other keys can be added. Because PEP 584 is painfully missing before Python 3.9, Apischema metadata use their own subclass of dict just to add | operator for convenience. Dataclasses __post_init__ and field(init=False) are fully supported. Implication of this feature usage is documented in the relative sections. Warning Before 3.8, InitVar is doing type erasure , that's why it's not possible for Apischema to retrieve type information of init variables. To fix this behavior, a field metadata init_var can be used to put back the type of the field ( init_var also accepts stringified type annotations). Standard library types \u00b6 Apischema handle natively most of the types provided by the standard library. They are sorted in the following categories: Primitive \u00b6 str , int , float , bool , None , subclasses of them They correspond to JSON primitive types. Collection \u00b6 typing.Collection typing.Sequence typing.Tuple typing.MutableSequence typing.List typing.AbstractSet typing.FrozenSet typing.Set They correspond to JSON array and are serialized to list . Some of them are abstract; deserialization will instantiate a concrete child class. For example typing.Sequence will be instantiated with tuple while typing.MutableSequence will be instantiated with list . Mapping \u00b6 typing.Mapping , typing.MutableMapping , typing.Dict . They correpond to JSON object and are serialized to dict . Enumeration \u00b6 enum.Enum subclasses, typing.Literal For Enum , this is the value and not the attribute name that is serialized Typing facilities \u00b6 typing.Optional / typing.Union ( Optional[T] is strictly equivalent to Union[T, None] ) Deserialization select the first matching alternative (see below how to skip some union member ) typing.Tuple Can be used as collection as well as true Tuple, like Tuple[str, int] typing.NewType Serialized according to its base type typing.TypedDict , typing.NamedTuple Kind of discount dataclass without field customization Any Untouched by deserialization Other standard library types \u00b6 bytes with str (de)serialization using base64 encoding datetime.datetime datetime.date datetime.time Supported only in 3.7+ with fromisoformat / isoformat Decimal With float (de)serialization ipaddress.IPv4Address ipaddress.IPv4Interface ipaddress.IPv4Network ipaddress.IPv6Address ipaddress.IPv6Interface ipaddress.IPv6Network pathlib.Path typing.Pattern / re.Pattern uuid.UUID With str (de)serialization Generic \u00b6 typing.Generic can be used out of the box like in the following example: from dataclasses import dataclass from typing import Generic , TypeVar from pytest import raises from apischema import ValidationError , deserialize T = TypeVar ( \"T\" ) @dataclass class Box ( Generic [ T ]): content : T shaken : bool = False assert deserialize ( Box [ str ], { \"content\" : \"void\" }) == Box ( \"void\" ) with raises ( ValidationError ): deserialize ( Box [ str ], { \"content\" : 42 }) Recursive types, string annotations and PEP 563 \u00b6 Recursive classes can be typed as they usually do, with or without PEP 563 . Here with string annotations: from dataclasses import dataclass from typing import Optional from apischema import deserialize @dataclass class Node : value : int child : Optional [ \"Node\" ] = None assert deserialize ( Node , { \"value\" : 0 , \"child\" : { \"value\" : 1 }}) == Node ( 0 , Node ( 1 )) Here with PEP 563 (requires 3.7+) from __future__ import annotations from dataclasses import dataclass from typing import Optional from apischema import deserialize @dataclass class Node : value : int child : Optional [ Node ] = None assert deserialize ( Node , { \"value\" : 0 , \"child\" : { \"value\" : 1 }}) == Node ( 0 , Node ( 1 )) Warning To resolve annotations, Apischema uses typing.get_type_hints ; this doesn't work really well when used on objects defined outside of global scope. Warning (minor) Currently, PEP 585 can have surprising behavior when used outside the box, see bpo-41370 Annotated - PEP 593 \u00b6 PEP 593 is fully supported; annotations stranger to Apischema are simlply ignored. Skip Union member \u00b6 Sometimes, one of the Union members has not to be taken in account during validation; it can just be here to match the type of the default value of the field. This member can be marked as to be skipped with PEP 593 Annotated from dataclasses import dataclass from typing import Union from pytest import raises from typing_extensions import Annotated from apischema import ValidationError , deserialize , serialize from apischema.types import Skip @dataclass class Foo : # by the way NotNull = Optional[T, Annotated[None, Skip]] bar : Union [ int , Annotated [ None , Skip ]] = None with raises ( ValidationError ) as err : deserialize ( Foo , { \"bar\" : None }) assert serialize ( err . value ) == [ { \"loc\" : [ \"bar\" ], \"err\" : [ \"expected type integer, found null\" ]} ] Note Skip(schema_only=True) can also be used to skip the member only for JSON schema generation Optional vs. NotNull \u00b6 Optional type is not always appropriate, because it allows deserialized value to be null , but sometimes, you just want None as a default value for unset fields, not an authorized one. To solve this issue, Apischema defines a NotNull type. from dataclasses import dataclass from pytest import raises from apischema import NotNull , ValidationError , deserialize , serialize @dataclass class Foo : # NotNull is exactly like Optional for type checkers, # it's only considered differently by Apischema bar : NotNull [ int ] = None with raises ( ValidationError ) as err : deserialize ( Foo , { \"bar\" : None }) assert serialize ( err . value ) == [ { \"loc\" : [ \"bar\" ], \"err\" : [ \"expected type integer, found null\" ]} ] Note In fact, NotNull = Union[T, Annotated[None, Skip]] . Composed dataclasses merging \u00b6 Dataclass fields which are themselves dataclass can be \"merged\" into the owning one by using merged metadata. from dataclasses import dataclass , field from typing import List , Optional from apischema import alias , deserialize , serialize from apischema.fields import with_fields_set from apischema.json_schema import deserialization_schema from apischema.metadata import merged @with_fields_set @dataclass class JsonSchema : title : Optional [ str ] = None description : Optional [ str ] = None format : Optional [ str ] = None ... @with_fields_set @dataclass class RootJsonSchema : schema : Optional [ str ] = field ( default = None , metadata = alias ( \"$schema\" )) defs : List [ JsonSchema ] = field ( default_factory = list , metadata = alias ( \"$defs\" )) json_schema : JsonSchema = field ( default = JsonSchema (), metadata = merged ) data = { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"title\" : \"merged example\" , } root_schema = RootJsonSchema ( schema = \"http://json-schema.org/draft/2019-09/schema#\" , json_schema = JsonSchema ( title = \"merged example\" ), ) assert deserialize ( RootJsonSchema , data ) == root_schema assert serialize ( root_schema ) == data assert deserialization_schema ( RootJsonSchema ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"$defs\" : { \"JsonSchema\" : { \"type\" : \"object\" , \"properties\" : { \"title\" : { \"type\" : [ \"string\" , \"null\" ]}, \"description\" : { \"type\" : [ \"string\" , \"null\" ]}, \"format\" : { \"type\" : [ \"string\" , \"null\" ]}, }, \"additionalProperties\" : False , } }, \"type\" : \"object\" , \"allOf\" : [ { \"type\" : \"object\" , \"properties\" : { \"$schema\" : { \"type\" : [ \"string\" , \"null\" ]}, \"$defs\" : { \"type\" : \"array\" , \"items\" : { \"$ref\" : \"#/$defs/JsonSchema\" }}, }, \"additionalProperties\" : False , }, { \"$ref\" : \"#/$defs/JsonSchema\" }, ], \"unevaluatedProperties\" : False , } Note This feature use JSON schema draft 2019-09 unevaluatedProperties keyword . Custom types / ORM \u00b6 See conversion in order to support every possible types in a few lines of code. Unsupported types \u00b6 When Apischema encounters a type that it doesn't support, Unsupported exception will be raised. from pytest import raises from apischema import Unsupported , deserialize , serialize class Foo : pass with raises ( Unsupported ): deserialize ( Foo , {}) with raises ( Unsupported ): serialize ( Foo ()) See conversion section to make Apischema support all your classes. FAQ \u00b6 Why Iterable is not handled with other collection type? \u00b6 Iterable could be handled (actually, it was at the beginning), however, this doesn't really make sense from a data point of view. Iterable are computation objects, they can be infinite, etc. They don't correspond to a serialized data; Collection is way more appropriate in this context. What happens if I override dataclass __init__ ? \u00b6 Apischema always assumes that dataclass __init__ can be called with with all its fields as kwargs parameters. If that's no more the case after a modification of __init__ (what means if an exception is thrown when the constructor is called because of bad parameters), Apischema treats then the class as not supported .","title":"Data model"},{"location":"data_model/#data-model","text":"Apischema handle every classes/types you need. By the way, it's done in an additive way, meaning that it doesn't affect your types.","title":"Data model"},{"location":"data_model/#pep-585","text":"With Python 3.9 and PEP 585 , typing is substantially shaken up. Because Apischema must be up-to-date with modern Python, support of Python 3.9 covers PEP 585. But because 3.9 is not yet released and far from being widely adopted, the following documentation will use old fashioned Python typing with typing module.","title":"PEP 585"},{"location":"data_model/#dataclasses","text":"Because the library aims to bring the minimum boilerplate, it's build on the top of standard library. Dataclasses are thus the core structure of the data model. Dataclasses bring the possibility of field customization, with more than just a default value. In addition to the common parameters of dataclasses.field , customization is done with metadata parameter. With some teasing of features presented later: from dataclasses import dataclass , field from apischema import alias , schema from apischema.metadata import required @dataclass class Foo : bar : int = field ( default = 0 , metadata = alias ( \"foo_bar\" ) | schema ( title = \"foo! bar!\" , min = 0 , max = 42 ) | required , ) Note Field's metadata are just an ordinary dict ; Apischema provides some functions to enrich these metadata with it's own keys ( alias(\"foo_bar) is roughly equivalent to `{\"_apischema_alias\": \"foo_bar\"}) and use them when the time comes, but metadata are not reserved to Apischema and other keys can be added. Because PEP 584 is painfully missing before Python 3.9, Apischema metadata use their own subclass of dict just to add | operator for convenience. Dataclasses __post_init__ and field(init=False) are fully supported. Implication of this feature usage is documented in the relative sections. Warning Before 3.8, InitVar is doing type erasure , that's why it's not possible for Apischema to retrieve type information of init variables. To fix this behavior, a field metadata init_var can be used to put back the type of the field ( init_var also accepts stringified type annotations).","title":"Dataclasses"},{"location":"data_model/#standard-library-types","text":"Apischema handle natively most of the types provided by the standard library. They are sorted in the following categories:","title":"Standard library types"},{"location":"data_model/#primitive","text":"str , int , float , bool , None , subclasses of them They correspond to JSON primitive types.","title":"Primitive"},{"location":"data_model/#collection","text":"typing.Collection typing.Sequence typing.Tuple typing.MutableSequence typing.List typing.AbstractSet typing.FrozenSet typing.Set They correspond to JSON array and are serialized to list . Some of them are abstract; deserialization will instantiate a concrete child class. For example typing.Sequence will be instantiated with tuple while typing.MutableSequence will be instantiated with list .","title":"Collection"},{"location":"data_model/#mapping","text":"typing.Mapping , typing.MutableMapping , typing.Dict . They correpond to JSON object and are serialized to dict .","title":"Mapping"},{"location":"data_model/#enumeration","text":"enum.Enum subclasses, typing.Literal For Enum , this is the value and not the attribute name that is serialized","title":"Enumeration"},{"location":"data_model/#typing-facilities","text":"typing.Optional / typing.Union ( Optional[T] is strictly equivalent to Union[T, None] ) Deserialization select the first matching alternative (see below how to skip some union member ) typing.Tuple Can be used as collection as well as true Tuple, like Tuple[str, int] typing.NewType Serialized according to its base type typing.TypedDict , typing.NamedTuple Kind of discount dataclass without field customization Any Untouched by deserialization","title":"Typing facilities"},{"location":"data_model/#other-standard-library-types","text":"bytes with str (de)serialization using base64 encoding datetime.datetime datetime.date datetime.time Supported only in 3.7+ with fromisoformat / isoformat Decimal With float (de)serialization ipaddress.IPv4Address ipaddress.IPv4Interface ipaddress.IPv4Network ipaddress.IPv6Address ipaddress.IPv6Interface ipaddress.IPv6Network pathlib.Path typing.Pattern / re.Pattern uuid.UUID With str (de)serialization","title":"Other standard library types"},{"location":"data_model/#generic","text":"typing.Generic can be used out of the box like in the following example: from dataclasses import dataclass from typing import Generic , TypeVar from pytest import raises from apischema import ValidationError , deserialize T = TypeVar ( \"T\" ) @dataclass class Box ( Generic [ T ]): content : T shaken : bool = False assert deserialize ( Box [ str ], { \"content\" : \"void\" }) == Box ( \"void\" ) with raises ( ValidationError ): deserialize ( Box [ str ], { \"content\" : 42 })","title":"Generic"},{"location":"data_model/#recursive-types-string-annotations-and-pep-563","text":"Recursive classes can be typed as they usually do, with or without PEP 563 . Here with string annotations: from dataclasses import dataclass from typing import Optional from apischema import deserialize @dataclass class Node : value : int child : Optional [ \"Node\" ] = None assert deserialize ( Node , { \"value\" : 0 , \"child\" : { \"value\" : 1 }}) == Node ( 0 , Node ( 1 )) Here with PEP 563 (requires 3.7+) from __future__ import annotations from dataclasses import dataclass from typing import Optional from apischema import deserialize @dataclass class Node : value : int child : Optional [ Node ] = None assert deserialize ( Node , { \"value\" : 0 , \"child\" : { \"value\" : 1 }}) == Node ( 0 , Node ( 1 )) Warning To resolve annotations, Apischema uses typing.get_type_hints ; this doesn't work really well when used on objects defined outside of global scope. Warning (minor) Currently, PEP 585 can have surprising behavior when used outside the box, see bpo-41370","title":"Recursive types, string annotations and PEP 563"},{"location":"data_model/#annotated-pep-593","text":"PEP 593 is fully supported; annotations stranger to Apischema are simlply ignored.","title":"Annotated - PEP 593"},{"location":"data_model/#skip-union-member","text":"Sometimes, one of the Union members has not to be taken in account during validation; it can just be here to match the type of the default value of the field. This member can be marked as to be skipped with PEP 593 Annotated from dataclasses import dataclass from typing import Union from pytest import raises from typing_extensions import Annotated from apischema import ValidationError , deserialize , serialize from apischema.types import Skip @dataclass class Foo : # by the way NotNull = Optional[T, Annotated[None, Skip]] bar : Union [ int , Annotated [ None , Skip ]] = None with raises ( ValidationError ) as err : deserialize ( Foo , { \"bar\" : None }) assert serialize ( err . value ) == [ { \"loc\" : [ \"bar\" ], \"err\" : [ \"expected type integer, found null\" ]} ] Note Skip(schema_only=True) can also be used to skip the member only for JSON schema generation","title":"Skip Union member"},{"location":"data_model/#optional-vs-notnull","text":"Optional type is not always appropriate, because it allows deserialized value to be null , but sometimes, you just want None as a default value for unset fields, not an authorized one. To solve this issue, Apischema defines a NotNull type. from dataclasses import dataclass from pytest import raises from apischema import NotNull , ValidationError , deserialize , serialize @dataclass class Foo : # NotNull is exactly like Optional for type checkers, # it's only considered differently by Apischema bar : NotNull [ int ] = None with raises ( ValidationError ) as err : deserialize ( Foo , { \"bar\" : None }) assert serialize ( err . value ) == [ { \"loc\" : [ \"bar\" ], \"err\" : [ \"expected type integer, found null\" ]} ] Note In fact, NotNull = Union[T, Annotated[None, Skip]] .","title":"Optional vs. NotNull"},{"location":"data_model/#composed-dataclasses-merging","text":"Dataclass fields which are themselves dataclass can be \"merged\" into the owning one by using merged metadata. from dataclasses import dataclass , field from typing import List , Optional from apischema import alias , deserialize , serialize from apischema.fields import with_fields_set from apischema.json_schema import deserialization_schema from apischema.metadata import merged @with_fields_set @dataclass class JsonSchema : title : Optional [ str ] = None description : Optional [ str ] = None format : Optional [ str ] = None ... @with_fields_set @dataclass class RootJsonSchema : schema : Optional [ str ] = field ( default = None , metadata = alias ( \"$schema\" )) defs : List [ JsonSchema ] = field ( default_factory = list , metadata = alias ( \"$defs\" )) json_schema : JsonSchema = field ( default = JsonSchema (), metadata = merged ) data = { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"title\" : \"merged example\" , } root_schema = RootJsonSchema ( schema = \"http://json-schema.org/draft/2019-09/schema#\" , json_schema = JsonSchema ( title = \"merged example\" ), ) assert deserialize ( RootJsonSchema , data ) == root_schema assert serialize ( root_schema ) == data assert deserialization_schema ( RootJsonSchema ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"$defs\" : { \"JsonSchema\" : { \"type\" : \"object\" , \"properties\" : { \"title\" : { \"type\" : [ \"string\" , \"null\" ]}, \"description\" : { \"type\" : [ \"string\" , \"null\" ]}, \"format\" : { \"type\" : [ \"string\" , \"null\" ]}, }, \"additionalProperties\" : False , } }, \"type\" : \"object\" , \"allOf\" : [ { \"type\" : \"object\" , \"properties\" : { \"$schema\" : { \"type\" : [ \"string\" , \"null\" ]}, \"$defs\" : { \"type\" : \"array\" , \"items\" : { \"$ref\" : \"#/$defs/JsonSchema\" }}, }, \"additionalProperties\" : False , }, { \"$ref\" : \"#/$defs/JsonSchema\" }, ], \"unevaluatedProperties\" : False , } Note This feature use JSON schema draft 2019-09 unevaluatedProperties keyword .","title":"Composed dataclasses merging"},{"location":"data_model/#custom-types-orm","text":"See conversion in order to support every possible types in a few lines of code.","title":"Custom types / ORM"},{"location":"data_model/#unsupported-types","text":"When Apischema encounters a type that it doesn't support, Unsupported exception will be raised. from pytest import raises from apischema import Unsupported , deserialize , serialize class Foo : pass with raises ( Unsupported ): deserialize ( Foo , {}) with raises ( Unsupported ): serialize ( Foo ()) See conversion section to make Apischema support all your classes.","title":"Unsupported types"},{"location":"data_model/#faq","text":"","title":"FAQ"},{"location":"data_model/#why-iterable-is-not-handled-with-other-collection-type","text":"Iterable could be handled (actually, it was at the beginning), however, this doesn't really make sense from a data point of view. Iterable are computation objects, they can be infinite, etc. They don't correspond to a serialized data; Collection is way more appropriate in this context.","title":"Why Iterable is not handled with other collection type?"},{"location":"data_model/#what-happens-if-i-override-dataclass-__init__","text":"Apischema always assumes that dataclass __init__ can be called with with all its fields as kwargs parameters. If that's no more the case after a modification of __init__ (what means if an exception is thrown when the constructor is called because of bad parameters), Apischema treats then the class as not supported .","title":"What happens if I override dataclass __init__?"},{"location":"de_serialization/","text":"(De)serialization: the basics \u00b6 Apischema aims to help API with deserialization/serialization of data, mostly JSON. Let start again with the overview example from dataclasses import dataclass , field from typing import Set from uuid import UUID , uuid4 from pytest import raises from apischema import ValidationError , deserialize , serialize # Define a schema with standard dataclasses from apischema.json_schema import deserialization_schema @dataclass class Resource : id : UUID name : str tags : Set [ str ] = field ( default_factory = set ) # Get some data uuid = uuid4 () data = { \"id\" : str ( uuid ), \"name\" : \"wyfo\" , \"tags\" : [ \"some_tag\" ]} # Deserialize data resource = deserialize ( Resource , data ) assert resource == Resource ( uuid , \"wyfo\" , { \"some_tag\" }) # Serialize objects assert serialize ( resource ) == data # Validate during deserialization with raises ( ValidationError ) as err : # pytest check exception is raised deserialize ( Resource , { \"id\" : \"42\" , \"name\" : \"wyfo\" }) assert serialize ( err . value ) == [ # ValidationError is serializable { \"loc\" : [ \"id\" ], \"err\" : [ \"badly formed hexadecimal UUID string\" ]} ] # Generate JSON Schema assert deserialization_schema ( Resource ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"type\" : \"object\" , \"properties\" : { \"id\" : { \"type\" : \"string\" , \"format\" : \"uuid\" }, \"name\" : { \"type\" : \"string\" }, \"tags\" : { \"type\" : \"array\" , \"items\" : { \"type\" : \"string\" }, \"uniqueItems\" : True }, }, \"required\" : [ \"id\" , \"name\" ], \"additionalProperties\" : False , } Deserialization \u00b6 Deserialization is done through the function apischema.deserialize with the following simplified signature: def deserialize ( cls : Type [ T ], data : Any ) -> T : ... cls can be a dataclass as well as a list[int] a NewType , or whatever you want (see conversions to extend deserialization support to every type you want). data must be a JSON-like serialized data: dict / list / str / int / float / bool / None , in short, what you get when you execute json.loads . Deserialization performs a validation of data, based on typing annotations and other information (see schema and validation ). from dataclasses import dataclass from typing import Collection, Mapping, NewType from apischema import deserialize @dataclass class Foo: bar: str MyInt = NewType(\"MyInt\", int) assert deserialize(Foo, {\"bar\": \"bar\"}) == Foo(\"bar\") assert deserialize(MyInt, 0) == MyInt(0) == 0 assert deserialize(Mapping[str, Collection[Foo]], {\"key\": [{\"bar\": \"42\"}]}) == { \"key\": (Foo(\"42\"),) } Strictness \u00b6 Coercion \u00b6 Apischema is strict by default. You ask for an integer, you have to receive an integer. However, in some cases, particularly concerning stringified configuration (see Configuration management ), data must be coerced. That can be done using coerce parameter; when set to True , all primitive types will be coerce to the expected type of the data model like the following: from pytest import raises from apischema import ValidationError , deserialize with raises ( ValidationError ): deserialize ( bool , \"ok\" ) assert deserialize ( bool , \"ok\" , coercion = True ) bool can be coerced from str with the following case-insensitive mapping: False True 0 1 f t n y no yes false true off on ko ok Note bool coercion from str is just a global dict[str, bool] named apischema.data.coercion.STR_TO_BOOL and it can be customized according to your need (but keys have to be lower cased). There is also a global set[str] named apischema.data.coercion.STR_NONE_VALUES for None coercion. coercion parameter can also receive a coercion function which will then be used instead of default one. from typing import Type , TypeVar , cast from pytest import raises from apischema import ValidationError , deserialize T = TypeVar ( \"T\" ) def coerce ( cls : Type [ T ], data ) -> T : \"\"\"Only coerce int to bool\"\"\" if cls is bool and isinstance ( data , int ): return cast ( T , bool ( data )) else : return data with raises ( ValidationError ): deserialize ( bool , 0 ) with raises ( ValidationError ): assert deserialize ( bool , \"ok\" , coercion = coerce ) assert deserialize ( bool , 1 , coercion = coerce ) Note If coercer result is not an instance of class passed in argument, a ValidationError will be raised with an appropriate error message Warning Coercer first argument can is a primitive json type str / bool / int / float / list / dict / type(None) ; it can be type(None) , so returning cls(data) will fail in this case. Additional properties \u00b6 Apischema is strict too about number of fields received for an object . In JSON schema terms, Apischema put \"additionalProperties\": false by default (this can be configured by class with properties field ). This behavior can be controlled by additional_properties parameter. When set to True , it prevents the reject of unexpected properties. from dataclasses import dataclass from pytest import raises from apischema import ValidationError , deserialize @dataclass class Foo : bar : str data = { \"bar\" : \"bar\" , \"other\" : 42 } with raises ( ValidationError ): deserialize ( Foo , data ) assert deserialize ( Foo , data , additional_properties = True ) == Foo ( \"bar\" ) Default fallback \u00b6 Validation error can happen when deserializing an ill-formed field. However, if this field has a default value/factory, deserialization can fallback to this default; this is enabled by default_fallback parameter. This behavior can also be configured for each field using metadata. from dataclasses import dataclass , field from pytest import raises from apischema import ValidationError , deserialize from apischema.metadata import default_fallback @dataclass class Foo : bar : str = \"bar\" baz : str = field ( default = \"baz\" , metadata = default_fallback ) with raises ( ValidationError ): deserialize ( Foo , { \"bar\" : 0 }) assert deserialize ( Foo , { \"bar\" : 0 }, default_fallback = True ) == Foo () assert deserialize ( Foo , { \"baz\" : 0 }) == Foo () Strictness configuration \u00b6 Apischema global configuration is managed through apischema.settings module. This module has, among other, three global variables settings.additional_properties , settings.coercion and settings.default_fallback whose values are used as default parameter values for the deserialize function. Global coercion function can be set with settings.coercer following this example: import json from apischema import ValidationError , settings prev_coercer = settings . coercer () @settings . coercer def coercer ( cls , data ): \"\"\"In case of coercion failures, try to deserialize json data\"\"\" try : return prev_coercer ( cls , data ) except ValidationError as err : if not isinstance ( data , str ): raise try : return json . loads ( data ) except json . JSONDecodeError : raise err Note Like all settings function, coercer has an overloaded signature. Without argument, it returns the current settings function, and with an argument, it set the settings function. Fields set \u00b6 Sometimes, it can be useful to know which field has been set by the deserialization, for example in the case of a PATCH requests, to know which field has been updated. Moreover, it is also used in serialization to limit the fields serialized (see next section ) Because Apischema use vanilla dataclasses, this feature is not enabled by default and must be set explicitly on a per-class basis. Apischema provides a simple API to get/set this metadata. from dataclasses import dataclass from typing import Optional from apischema import deserialize from apischema.fields import ( fields_set , is_set , set_fields , unset_fields , with_fields_set , ) # This decorator enable the feature @with_fields_set @dataclass class Foo : bar : int baz : Optional [ str ] = None # Retrieve fields set foo1 = Foo ( 0 , None ) assert fields_set ( foo1 ) == { \"bar\" , \"baz\" } foo2 = Foo ( 0 ) assert fields_set ( foo2 ) == { \"bar\" } # Test fields individually (with autocompletion and refactoring) assert is_set ( foo1 ) . baz assert not is_set ( foo2 ) . baz # Mark fields as set/unset set_fields ( foo2 , \"baz\" ) assert fields_set ( foo2 ) == { \"bar\" , \"baz\" } unset_fields ( foo2 , \"baz\" ) assert fields_set ( foo2 ) == { \"bar\" } set_fields ( foo2 , \"baz\" , overwrite = True ) assert fields_set ( foo2 ) == { \"baz\" } # Fields modification are taken in account foo2 . bar = 0 assert fields_set ( foo2 ) == { \"bar\" , \"baz\" } # Because deserialization use normal constructor, it works with the feature foo3 = deserialize ( Foo , { \"bar\" : 0 }) assert fields_set ( foo3 ) == { \"bar\" } Warning with_fields_set decorator MUST be put above dataclass one. This is because both of them modify __init__ method, but only the first is built to take the second in account. Warning dataclasses.replace works by setting all the fields of the replaced object. Because of this issue, Apischema provides a little wrapper apischema.dataclasses.replace . Serialization \u00b6 Serialization is simpler than deserialization; serialize(obj) will generate a JSON-like serialized obj . There is no validation, objects provided are trusted \u2014 they are supposed to be statically type-checked. When there from dataclasses import dataclass from apischema import serialize @dataclass class Foo : bar : str assert serialize ( Foo ( \"bar\" )) == { \"bar\" : \"bar\" } assert serialize (( 0 , 1 )) == [ 0 , 1 ] assert serialize ({ \"key\" : ( \"value\" , 42 )}) == { \"key\" : [ \"value\" , 42 ]} Exclude unset fields \u00b6 When a class has a lot of optional fields, it can be convenient to not include all of them, to avoid a bunch of useless fields in your serialized data. Using the previous feature of fields set tracking , serialize can exclude unset fields using its exclude_unset parameter; this parameter is defaulted to True . from dataclasses import dataclass from typing import Optional from apischema import serialize from apischema.fields import with_fields_set # Decorator needed to benefit from the feature @with_fields_set @dataclass class Foo : bar : int baz : Optional [ str ] = None assert serialize ( Foo ( 0 )) == { \"bar\" : 0 } assert serialize ( Foo ( 0 ), exclude_unset = False ) == { \"bar\" : 0 , \"baz\" : None } Note As written in comment in the example, with_fields_set is necessary to benefit from the feature. If the dataclass don't use it, the feature will have no effect. Sometimes, some fields must be serialized, even with their default value; this behavior can be enforced using field metadata. With it, field will be marked as set even if its default value is used at initialization. from dataclasses import dataclass , field from typing import Optional from apischema import serialize from apischema.fields import with_fields_set from apischema.metadata import default_as_set # Decorator needed to benefit from the feature @with_fields_set @dataclass class Foo : bar : Optional [ int ] = field ( default = None , metadata = default_as_set ) assert serialize ( Foo ()) == { \"bar\" : None } assert serialize ( Foo ( 0 )) == { \"bar\" : 0 } Note This metadata has effect only in combination with with_fields_set decorator. FAQ \u00b6 Why coercion is not default behavior? \u00b6 Because ill-formed data can be symptomatic of problems, and it has been decided that highlighting them would be better than hiding them. By the way, this is easily globally configurable. Why with_fields_set feature is not enable by default? \u00b6 It's true that this feature has the little cost of adding a decorator everywhere. However, keeping dataclass decorator allows IDEs/linters/type checkers/etc. to handle the class as such, so there is no need to develop a plugin for them. Standard compliance can be worth the additional decorator. (And little overhead can be avoided when not useful)","title":"(De)serialization: The basics"},{"location":"de_serialization/#deserialization-the-basics","text":"Apischema aims to help API with deserialization/serialization of data, mostly JSON. Let start again with the overview example from dataclasses import dataclass , field from typing import Set from uuid import UUID , uuid4 from pytest import raises from apischema import ValidationError , deserialize , serialize # Define a schema with standard dataclasses from apischema.json_schema import deserialization_schema @dataclass class Resource : id : UUID name : str tags : Set [ str ] = field ( default_factory = set ) # Get some data uuid = uuid4 () data = { \"id\" : str ( uuid ), \"name\" : \"wyfo\" , \"tags\" : [ \"some_tag\" ]} # Deserialize data resource = deserialize ( Resource , data ) assert resource == Resource ( uuid , \"wyfo\" , { \"some_tag\" }) # Serialize objects assert serialize ( resource ) == data # Validate during deserialization with raises ( ValidationError ) as err : # pytest check exception is raised deserialize ( Resource , { \"id\" : \"42\" , \"name\" : \"wyfo\" }) assert serialize ( err . value ) == [ # ValidationError is serializable { \"loc\" : [ \"id\" ], \"err\" : [ \"badly formed hexadecimal UUID string\" ]} ] # Generate JSON Schema assert deserialization_schema ( Resource ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"type\" : \"object\" , \"properties\" : { \"id\" : { \"type\" : \"string\" , \"format\" : \"uuid\" }, \"name\" : { \"type\" : \"string\" }, \"tags\" : { \"type\" : \"array\" , \"items\" : { \"type\" : \"string\" }, \"uniqueItems\" : True }, }, \"required\" : [ \"id\" , \"name\" ], \"additionalProperties\" : False , }","title":"(De)serialization: the basics"},{"location":"de_serialization/#deserialization","text":"Deserialization is done through the function apischema.deserialize with the following simplified signature: def deserialize ( cls : Type [ T ], data : Any ) -> T : ... cls can be a dataclass as well as a list[int] a NewType , or whatever you want (see conversions to extend deserialization support to every type you want). data must be a JSON-like serialized data: dict / list / str / int / float / bool / None , in short, what you get when you execute json.loads . Deserialization performs a validation of data, based on typing annotations and other information (see schema and validation ). from dataclasses import dataclass from typing import Collection, Mapping, NewType from apischema import deserialize @dataclass class Foo: bar: str MyInt = NewType(\"MyInt\", int) assert deserialize(Foo, {\"bar\": \"bar\"}) == Foo(\"bar\") assert deserialize(MyInt, 0) == MyInt(0) == 0 assert deserialize(Mapping[str, Collection[Foo]], {\"key\": [{\"bar\": \"42\"}]}) == { \"key\": (Foo(\"42\"),) }","title":"Deserialization"},{"location":"de_serialization/#strictness","text":"","title":"Strictness"},{"location":"de_serialization/#coercion","text":"Apischema is strict by default. You ask for an integer, you have to receive an integer. However, in some cases, particularly concerning stringified configuration (see Configuration management ), data must be coerced. That can be done using coerce parameter; when set to True , all primitive types will be coerce to the expected type of the data model like the following: from pytest import raises from apischema import ValidationError , deserialize with raises ( ValidationError ): deserialize ( bool , \"ok\" ) assert deserialize ( bool , \"ok\" , coercion = True ) bool can be coerced from str with the following case-insensitive mapping: False True 0 1 f t n y no yes false true off on ko ok Note bool coercion from str is just a global dict[str, bool] named apischema.data.coercion.STR_TO_BOOL and it can be customized according to your need (but keys have to be lower cased). There is also a global set[str] named apischema.data.coercion.STR_NONE_VALUES for None coercion. coercion parameter can also receive a coercion function which will then be used instead of default one. from typing import Type , TypeVar , cast from pytest import raises from apischema import ValidationError , deserialize T = TypeVar ( \"T\" ) def coerce ( cls : Type [ T ], data ) -> T : \"\"\"Only coerce int to bool\"\"\" if cls is bool and isinstance ( data , int ): return cast ( T , bool ( data )) else : return data with raises ( ValidationError ): deserialize ( bool , 0 ) with raises ( ValidationError ): assert deserialize ( bool , \"ok\" , coercion = coerce ) assert deserialize ( bool , 1 , coercion = coerce ) Note If coercer result is not an instance of class passed in argument, a ValidationError will be raised with an appropriate error message Warning Coercer first argument can is a primitive json type str / bool / int / float / list / dict / type(None) ; it can be type(None) , so returning cls(data) will fail in this case.","title":"Coercion"},{"location":"de_serialization/#additional-properties","text":"Apischema is strict too about number of fields received for an object . In JSON schema terms, Apischema put \"additionalProperties\": false by default (this can be configured by class with properties field ). This behavior can be controlled by additional_properties parameter. When set to True , it prevents the reject of unexpected properties. from dataclasses import dataclass from pytest import raises from apischema import ValidationError , deserialize @dataclass class Foo : bar : str data = { \"bar\" : \"bar\" , \"other\" : 42 } with raises ( ValidationError ): deserialize ( Foo , data ) assert deserialize ( Foo , data , additional_properties = True ) == Foo ( \"bar\" )","title":"Additional properties"},{"location":"de_serialization/#default-fallback","text":"Validation error can happen when deserializing an ill-formed field. However, if this field has a default value/factory, deserialization can fallback to this default; this is enabled by default_fallback parameter. This behavior can also be configured for each field using metadata. from dataclasses import dataclass , field from pytest import raises from apischema import ValidationError , deserialize from apischema.metadata import default_fallback @dataclass class Foo : bar : str = \"bar\" baz : str = field ( default = \"baz\" , metadata = default_fallback ) with raises ( ValidationError ): deserialize ( Foo , { \"bar\" : 0 }) assert deserialize ( Foo , { \"bar\" : 0 }, default_fallback = True ) == Foo () assert deserialize ( Foo , { \"baz\" : 0 }) == Foo ()","title":"Default fallback"},{"location":"de_serialization/#strictness-configuration","text":"Apischema global configuration is managed through apischema.settings module. This module has, among other, three global variables settings.additional_properties , settings.coercion and settings.default_fallback whose values are used as default parameter values for the deserialize function. Global coercion function can be set with settings.coercer following this example: import json from apischema import ValidationError , settings prev_coercer = settings . coercer () @settings . coercer def coercer ( cls , data ): \"\"\"In case of coercion failures, try to deserialize json data\"\"\" try : return prev_coercer ( cls , data ) except ValidationError as err : if not isinstance ( data , str ): raise try : return json . loads ( data ) except json . JSONDecodeError : raise err Note Like all settings function, coercer has an overloaded signature. Without argument, it returns the current settings function, and with an argument, it set the settings function.","title":"Strictness configuration"},{"location":"de_serialization/#fields-set","text":"Sometimes, it can be useful to know which field has been set by the deserialization, for example in the case of a PATCH requests, to know which field has been updated. Moreover, it is also used in serialization to limit the fields serialized (see next section ) Because Apischema use vanilla dataclasses, this feature is not enabled by default and must be set explicitly on a per-class basis. Apischema provides a simple API to get/set this metadata. from dataclasses import dataclass from typing import Optional from apischema import deserialize from apischema.fields import ( fields_set , is_set , set_fields , unset_fields , with_fields_set , ) # This decorator enable the feature @with_fields_set @dataclass class Foo : bar : int baz : Optional [ str ] = None # Retrieve fields set foo1 = Foo ( 0 , None ) assert fields_set ( foo1 ) == { \"bar\" , \"baz\" } foo2 = Foo ( 0 ) assert fields_set ( foo2 ) == { \"bar\" } # Test fields individually (with autocompletion and refactoring) assert is_set ( foo1 ) . baz assert not is_set ( foo2 ) . baz # Mark fields as set/unset set_fields ( foo2 , \"baz\" ) assert fields_set ( foo2 ) == { \"bar\" , \"baz\" } unset_fields ( foo2 , \"baz\" ) assert fields_set ( foo2 ) == { \"bar\" } set_fields ( foo2 , \"baz\" , overwrite = True ) assert fields_set ( foo2 ) == { \"baz\" } # Fields modification are taken in account foo2 . bar = 0 assert fields_set ( foo2 ) == { \"bar\" , \"baz\" } # Because deserialization use normal constructor, it works with the feature foo3 = deserialize ( Foo , { \"bar\" : 0 }) assert fields_set ( foo3 ) == { \"bar\" } Warning with_fields_set decorator MUST be put above dataclass one. This is because both of them modify __init__ method, but only the first is built to take the second in account. Warning dataclasses.replace works by setting all the fields of the replaced object. Because of this issue, Apischema provides a little wrapper apischema.dataclasses.replace .","title":"Fields set"},{"location":"de_serialization/#serialization","text":"Serialization is simpler than deserialization; serialize(obj) will generate a JSON-like serialized obj . There is no validation, objects provided are trusted \u2014 they are supposed to be statically type-checked. When there from dataclasses import dataclass from apischema import serialize @dataclass class Foo : bar : str assert serialize ( Foo ( \"bar\" )) == { \"bar\" : \"bar\" } assert serialize (( 0 , 1 )) == [ 0 , 1 ] assert serialize ({ \"key\" : ( \"value\" , 42 )}) == { \"key\" : [ \"value\" , 42 ]}","title":"Serialization"},{"location":"de_serialization/#exclude-unset-fields","text":"When a class has a lot of optional fields, it can be convenient to not include all of them, to avoid a bunch of useless fields in your serialized data. Using the previous feature of fields set tracking , serialize can exclude unset fields using its exclude_unset parameter; this parameter is defaulted to True . from dataclasses import dataclass from typing import Optional from apischema import serialize from apischema.fields import with_fields_set # Decorator needed to benefit from the feature @with_fields_set @dataclass class Foo : bar : int baz : Optional [ str ] = None assert serialize ( Foo ( 0 )) == { \"bar\" : 0 } assert serialize ( Foo ( 0 ), exclude_unset = False ) == { \"bar\" : 0 , \"baz\" : None } Note As written in comment in the example, with_fields_set is necessary to benefit from the feature. If the dataclass don't use it, the feature will have no effect. Sometimes, some fields must be serialized, even with their default value; this behavior can be enforced using field metadata. With it, field will be marked as set even if its default value is used at initialization. from dataclasses import dataclass , field from typing import Optional from apischema import serialize from apischema.fields import with_fields_set from apischema.metadata import default_as_set # Decorator needed to benefit from the feature @with_fields_set @dataclass class Foo : bar : Optional [ int ] = field ( default = None , metadata = default_as_set ) assert serialize ( Foo ()) == { \"bar\" : None } assert serialize ( Foo ( 0 )) == { \"bar\" : 0 } Note This metadata has effect only in combination with with_fields_set decorator.","title":"Exclude unset fields"},{"location":"de_serialization/#faq","text":"","title":"FAQ"},{"location":"de_serialization/#why-coercion-is-not-default-behavior","text":"Because ill-formed data can be symptomatic of problems, and it has been decided that highlighting them would be better than hiding them. By the way, this is easily globally configurable.","title":"Why coercion is not default behavior?"},{"location":"de_serialization/#why-with_fields_set-feature-is-not-enable-by-default","text":"It's true that this feature has the little cost of adding a decorator everywhere. However, keeping dataclass decorator allows IDEs/linters/type checkers/etc. to handle the class as such, so there is no need to develop a plugin for them. Standard compliance can be worth the additional decorator. (And little overhead can be avoided when not useful)","title":"Why with_fields_set feature is not enable by default?"},{"location":"json_schema/","text":"JSON schema \u00b6 JSON schema generation \u00b6 JSON schema can be generated from data model. However, because of all possible customizations , schema can be differ between deserilialization and serialization. In common cases, deserialization_schema and serialization_schema will give the same result. from dataclasses import dataclass from apischema.json_schema import deserialization_schema , serialization_schema @dataclass class Foo : bar : str assert deserialization_schema ( Foo ) == serialization_schema ( Foo ) assert deserialization_schema ( Foo ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"additionalProperties\" : False , \"properties\" : { \"bar\" : { \"type\" : \"string\" }}, \"required\" : [ \"bar\" ], \"type\" : \"object\" , } Field alias \u00b6 Sometimes dataclass field names can clash with language keyword, sometimes the property name is not convenient. Hopefully, field can define an alias which will be used in schema and deserialization/serialization. from dataclasses import dataclass , field from apischema import alias , deserialize , serialize from apischema.json_schema import deserialization_schema @dataclass class Foo : class_ : str = field ( metadata = alias ( \"class\" )) assert deserialization_schema ( Foo ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"additionalProperties\" : False , \"properties\" : { \"class\" : { \"type\" : \"string\" }}, \"required\" : [ \"class\" ], \"type\" : \"object\" , } assert deserialize ( Foo , { \"class\" : \"bar\" }) == Foo ( \"bar\" ) assert serialize ( Foo ( \"bar\" )) == { \"class\" : \"bar\" } Alias all fields \u00b6 Field aliasing can also be done at class level by specifying an aliasing function. This aliaser is applied to field alias if defined or field name, or not applied if override=False is specified. from dataclasses import dataclass , field from typing import Any from apischema import alias from apischema.json_schema import deserialization_schema @alias ( lambda s : f \"foo_ { s } \" ) @dataclass class Foo : field1 : Any field2 : Any = field ( metadata = alias ( override = False )) field3 : Any = field ( metadata = alias ( \"field03\" )) field4 : Any = field ( metadata = alias ( \"field04\" , override = False )) assert deserialization_schema ( Foo ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"additionalProperties\" : False , \"properties\" : { \"foo_field1\" : {}, \"field2\" : {}, \"foo_field03\" : {}, \"field04\" : {}}, \"required\" : [ \"foo_field1\" , \"field2\" , \"foo_field03\" , \"field04\" ], \"type\" : \"object\" , } Class-level aliasing can be used to define a camelCase API. Global aliaser \u00b6 apischema.settings.aliaser can be used to set a global aliaser function which will be applied to every field ( override=False is ignored by the global aliaser). It can be used for example to make all an application use camelCase . Actually, there is a shortcut for that: from apischema import settings settings . aliaser ( camel_case = True ) Otherwise, it's used the same way than settings.coercer . Schema annotations \u00b6 Type annotations are not enough to express a complete schema, but Apischema has a function for that; schema can be used both as type decorator or field metadata. from dataclasses import dataclass , field from typing import List , NewType from apischema import schema from apischema.json_schema import deserialization_schema Tag = NewType ( \"Tag\" , str ) schema ( min_len = 3 , pattern = r \"^\\w*$\" , examples = [ \"available\" , \"EMEA\" ])( Tag ) @dataclass class Resource : id : int tags : List [ Tag ] = field ( default_factory = list , metadata = schema ( description = \"regroup multiple resources\" , max_items = 3 , unique = True ), ) assert deserialization_schema ( Resource ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"additionalProperties\" : False , \"properties\" : { \"id\" : { \"type\" : \"integer\" }, \"tags\" : { \"description\" : \"regroup multiple resources\" , \"items\" : { \"examples\" : [ \"available\" , \"EMEA\" ], \"minLength\" : 3 , \"pattern\" : \"^ \\\\ w*$\" , \"type\" : \"string\" , }, \"maxItems\" : 3 , \"type\" : \"array\" , \"uniqueItems\" : True , }, }, \"required\" : [ \"id\" ], \"type\" : \"object\" , } Note Schema are particularly useful with NewType . For example, if you use prefixed ids, you can use a NewType with a pattern schema to validate them, and benefit of more precise type checking. The following keys are available (they are sometimes shorten compared to JSON schema original for code concision and snake_case): Key JSON schema keyword type restriction title / / description / / default / / examples / / min minimum int max maximum int exc_min exclusiveMinimum int exc_max exclusiveMaximum int mult_of multipleOf int format / str min_len minLength str max_len maxLength str pattern / str min_items minItems list max_items maxItems list unique / list min_props minProperties dict max_props maxProperties dict schema function has an overloaded signature which prevents to mix incompatible keywords. By the way, one other extra keyword allows to add arbitrary keys to JSON schema generated; setting extra_only to True force Apischema to use only extra keys. from typing import NewType from apischema import schema from apischema.json_schema import deserialization_schema Foo = NewType ( \"Foo\" , str ) schema ( min_len = 20 , extra = { \"typeName\" : \"Foo\" })( Foo ) Bar = NewType ( \"Bar\" , int ) schema ( extra = { \"$ref\" : \"http://some-domain.org/path/tp/schema.json#/$defs/Bar\" }, extra_only = True , )( Bar ) assert deserialization_schema ( Foo ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"type\" : \"string\" , \"minLength\" : 20 , \"typeName\" : \"Foo\" , } assert deserialization_schema ( Bar ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"$ref\" : \"http://some-domain.org/path/tp/schema.json#/$defs/Bar\" , } default annotation \u00b6 default annotation is not added automatically when a field has a default value (see FAQ ); schema default parameter must be used in order to make it appear in the schema. However ... can be used as a placeholder to make Apischema use field default value; this one will be serialized \u2014 if serialization fails, error will be ignored as well as default annotation. Constraints validation \u00b6 JSON schema constrains the data deserialized; this constraints are naturally used for validation. from dataclasses import dataclass , field from typing import List , NewType from pytest import raises from apischema import ValidationError , deserialize , schema , serialize Tag = NewType ( \"Tag\" , str ) schema ( min_len = 3 , pattern = r \"^\\w*$\" , examples = [ \"available\" , \"EMEA\" ])( Tag ) @dataclass class Resource : id : int tags : List [ Tag ] = field ( default_factory = list , metadata = schema ( description = \"regroup multiple resources\" , max_items = 3 , unique = True ), ) with raises ( ValidationError ) as err : # pytest check exception is raised deserialize ( Resource , { \"id\" : 42 , \"tags\" : [ \"tag\" , \"duplicate\" , \"duplicate\" , \"bad&\" , \"_\" ]} ) assert serialize ( err . value ) == [ { \"loc\" : [ \"tags\" ], \"err\" : [ \"size greater than 3 (maxItems)\" , \"duplicate items (uniqueItems)\" ], }, { \"loc\" : [ \"tags\" , 3 ], \"err\" : [ \"unmatched pattern '^ \\\\ w*$'\" ]}, { \"loc\" : [ \"tags\" , 4 ], \"err\" : [ \"length less than 3 (minLength)\" ]}, ] Required field with default value \u00b6 By default, a dataclass/namedtuple field will be tagged required if it doesn't have a default value. However, you may want to have a default value for a field in order to be more convenient in your code, but still make the field required. One could think about some schema model where version is fixed but is required, for example JSON-RPC with \"jsonrpc\": \"2.0\" . That's done with field metadata required . from dataclasses import dataclass , field from typing import Optional from pytest import raises from apischema import ValidationError , deserialize , serialize from apischema.metadata import required @dataclass class Foo : bar : Optional [ int ] = field ( default = None , metadata = required ) with raises ( ValidationError ) as err : deserialize ( Foo , {}) assert serialize ( err . value ) == [{ \"loc\" : [ \"bar\" ], \"err\" : [ \"missing property\" ]}] Additional properties / pattern properties \u00b6 With Mapping \u00b6 Schema of a Mapping / Dict type is naturally translated to \"additionalProperties\": <schema of the value type> . However when the schema of the key has a pattern , it will give a \"patternProperties\": {<key pattern>: <schema of the value type>} With dataclass \u00b6 additionalProperties / patternProperties can be added to dataclasses by using fields annotated with properties metadata. Properties not mapped on regular fields will be deserialized into this fields; they must have a Mapping type (or be convertible from Mapping ) because they are instanciated with a mapping. from dataclasses import dataclass , field from typing import Mapping from typing_extensions import Annotated from apischema import ( deserialize , properties , schema , schema_ref , ) from apischema.json_schema import deserialization_schema @schema_ref ( None ) @dataclass class Config : active : bool = True server_options : Mapping [ str , bool ] = field ( default_factory = dict , metadata = properties ( pattern = r \"^server_\" ) ) client_options : Mapping [ Annotated [ str , schema ( pattern = r \"^client_\" )], bool ] = field ( default_factory = dict , metadata = properties ( ... ) ) options : Mapping [ str , bool ] = field ( default_factory = dict , metadata = properties ) assert deserialize ( Config , { \"use_lightsaber\" : True , \"server_auto_restart\" : False , \"client_timeout\" : False }, ) == Config ( True , { \"server_auto_restart\" : False }, { \"client_timeout\" : False }, { \"use_lightsaber\" : True }, ) assert deserialization_schema ( Config ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"type\" : \"object\" , \"properties\" : { \"active\" : { \"type\" : \"boolean\" }}, \"additionalProperties\" : { \"type\" : \"boolean\" }, \"patternProperties\" : { \"^server_\" : { \"type\" : \"boolean\" }, \"^client_\" : { \"type\" : \"boolean\" }, }, } Note Of course, a dataclass can only have a single properties field without pattern, because it makes no sens to have several additionalProperties . Property dependencies \u00b6 Apischema support property dependencies for dataclass through a class member. Dependencies are also used in validation. Note JSON schema draft 2019-09 renames properties dependencies dependentRequired to disambiguate with schema dependencies from dataclasses import dataclass , field from pytest import raises from apischema import ( NotNull , ValidationError , deserialize , serialize , ) from apischema.dependencies import DependentRequired from apischema.json_schema import deserialization_schema @dataclass class Billing : name : str # Fields used in dependencies MUST be declared with `field` credit_card : NotNull [ int ] = field ( default = None ) billing_address : NotNull [ str ] = field ( default = None ) dependencies = DependentRequired ({ credit_card : [ billing_address ]}) assert deserialization_schema ( Billing ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"additionalProperties\" : False , \"dependentRequired\" : { \"credit_card\" : [ \"billing_address\" ]}, \"properties\" : { \"name\" : { \"type\" : \"string\" }, \"credit_card\" : { \"type\" : \"integer\" }, \"billing_address\" : { \"type\" : \"string\" }, }, \"required\" : [ \"name\" ], \"type\" : \"object\" , } with raises ( ValidationError ) as err : deserialize ( Billing , { \"name\" : \"Anonymous\" , \"credit_card\" : 1234_5678_9012_3456 }) assert serialize ( err . value ) == [ { \"loc\" : [ \"billing_address\" ], \"err\" : [ \"missing property (required by ['credit_card'])\" ], } ] Because bidirectional dependencies are a common idiom, Apischema provides a shortcut notation. Its indeed possible to write DependentRequired([credit_card, billing_adress]) . Complex/recursive types - JSON schema definitions/OpenAPI components \u00b6 For complex schema with type reuse, it's convenient to extract definitions of schema components in order to reuse them; it's even mandatory for recursive types. Then, schema use JSON pointers \"$ref\" to refer to the definitions. Apischema handles this feature natively. from dataclasses import dataclass from typing import Optional from apischema.json_schema import deserialization_schema @dataclass class Node : value : int child : Optional [ \"Node\" ] = None assert deserialization_schema ( Node ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"$ref\" : \"#/$defs/Node\" , \"$defs\" : { \"Node\" : { \"type\" : \"object\" , \"properties\" : { \"value\" : { \"type\" : \"integer\" }, \"child\" : { \"anyOf\" : [{ \"$ref\" : \"#/$defs/Node\" }, { \"type\" : \"null\" }]}, }, \"required\" : [ \"value\" ], \"additionalProperties\" : False , } }, } Use ref only for reused types \u00b6 If some types appear only once in the schema, you maybe don't want to use a $ref and a definition but inline the type definition directly. It is possible by setting schema_ref(None) (see next section ) on the the concerned type, but it could affect others schema where this types is reused several time. However, Apischema provides a parameter all_ref for this reason: - all_refs=True -> all types with a reference will be put in the definitions and referenced with $ref ; - all_refs=False -> only types which are reused in the schema are put in definitions all_refs default value depends on the JSON schema version : it's False for JSON schema drafts but True for OpenAPI . from dataclasses import dataclass from apischema.json_schema import deserialization_schema @dataclass class Bar : baz : str @dataclass class Foo : bar1 : Bar bar2 : Bar assert deserialization_schema ( Foo , all_refs = False ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"$defs\" : { \"Bar\" : { \"additionalProperties\" : False , \"properties\" : { \"baz\" : { \"type\" : \"string\" }}, \"required\" : [ \"baz\" ], \"type\" : \"object\" , } }, \"additionalProperties\" : False , \"properties\" : { \"bar1\" : { \"$ref\" : \"#/$defs/Bar\" }, \"bar2\" : { \"$ref\" : \"#/$defs/Bar\" }}, \"required\" : [ \"bar1\" , \"bar2\" ], \"type\" : \"object\" , } assert deserialization_schema ( Foo , all_refs = True ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"$defs\" : { \"Bar\" : { \"additionalProperties\" : False , \"properties\" : { \"baz\" : { \"type\" : \"string\" }}, \"required\" : [ \"baz\" ], \"type\" : \"object\" , }, \"Foo\" : { \"additionalProperties\" : False , \"properties\" : { \"bar1\" : { \"$ref\" : \"#/$defs/Bar\" }, \"bar2\" : { \"$ref\" : \"#/$defs/Bar\" }, }, \"required\" : [ \"bar1\" , \"bar2\" ], \"type\" : \"object\" , }, }, \"$ref\" : \"#/$defs/Foo\" , } Definitions schema only \u00b6 Sometimes you need to extract only the definitions in a separate schema (especially OpenAPI components). That's done with the definitions_schema function. It takes two lists deserializations and serializations of schema (or schema + conversions ) and combines the definitions of all the schema that would have been generated with types given in the list of schema. from dataclasses import dataclass from typing import List from apischema.json_schema import definitions_schema @dataclass class Bar : baz : int = 0 @dataclass class Foo : bar : Bar assert definitions_schema ( deserialization = [ List [ Foo ]], all_refs = True ) == { \"Foo\" : { \"type\" : \"object\" , \"properties\" : { \"bar\" : { \"$ref\" : \"#/$defs/Bar\" }}, \"required\" : [ \"bar\" ], \"additionalProperties\" : False , }, \"Bar\" : { \"type\" : \"object\" , \"properties\" : { \"baz\" : { \"type\" : \"integer\" }}, \"additionalProperties\" : False , }, } Customize $ref \u00b6 Adds $ref for every types \u00b6 In the previous example, only dataclasses has a $ref , but it can be fully customized. You can use schema_ref on any defined types ( NewType / class / Annotated /etc.). schema_ref argument can be: - str -> this string will be used directly in schema generation - ... -> schema generation will use the name of the - None -> this type will have no $ref in schema from dataclasses import dataclass from typing import AbstractSet , NewType from apischema import schema_ref from apischema.json_schema import deserialization_schema Tags = NewType ( \"Tags\" , AbstractSet [ str ]) schema_ref ( ... )( Tags ) @dataclass class Resource : id : int tags : Tags assert deserialization_schema ( Resource , all_refs = True ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"$defs\" : { \"Resource\" : { \"type\" : \"object\" , \"properties\" : { \"id\" : { \"type\" : \"integer\" }, \"tags\" : { \"$ref\" : \"#/$defs/Tags\" }}, \"required\" : [ \"id\" , \"tags\" ], \"additionalProperties\" : False , }, \"Tags\" : { \"type\" : \"array\" , \"items\" : { \"type\" : \"string\" }, \"uniqueItems\" : True }, }, \"$ref\" : \"#/$defs/Resource\" , } Moreover, there is a default schema_ref for each type when the function is not used directly; by default, it's implemented like this ... if is_dataclass(cls) else None , what means that dataclasses have a default ref with their name but other types don't. This default behavior is customizable by setting settings.default_ref function like this from apischema import settings @settings . default_ref def default_ref ( cls ): return None # This example remove default ref for every types Note Actually, there is a small restriction with NewType : you cannot put a schema_ref if the super type is not a builtin type ( List[...] / int /etc.). Ref factory \u00b6 schema_ref is used to set a short ref, like the name of a class, but in schema, $ref looks like #/$defs/Foo . In fact, schema generation use the ref given by schema_ref and pass it to a ref_factory function (a parameter of schema generation functions) which will convert it to its final form. JSON schema version comes with its default ref_factory , for draft 2019-09, it prefixes the ref with #/$defs/ , while it prefixes with #/components/schema in case of OpenAPI . from dataclasses import dataclass from apischema.json_schema import deserialization_schema @dataclass class Foo : bar : int def ref_factory ( ref : str ) -> str : return f \"http://some-domain.org/path/to/ { ref } .json#\" assert deserialization_schema ( Foo , ref_factory = ref_factory ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"$ref\" : \"http://some-domain.org/path/to/Foo.json#\" , } Note When ref_factory is passed in arguments, definitions are not added to the generated schema. That's because ref_factory would surely change definitions location, so there would be no interest to add them with a wrong location. This definitions can of course be generated separately with definitions_schema . Passing ref_factory also give a default value of True for all_refs parameters. JSON schema / OpenAPI version \u00b6 JSON schema has several versions \u2014 OpenAPI is treated as a JSON schema version. If Apischema natively use the last one: draft 2019-09, it is possible to specify a schema version which will be used for the generation. from dataclasses import dataclass from typing import Optional from apischema.json_schema import ( JsonSchemaVersion , definitions_schema , deserialization_schema , ) @dataclass class Bar : baz : Optional [ int ] @dataclass class Foo : bar : Bar assert deserialization_schema ( Foo , all_refs = True ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"$ref\" : \"#/$defs/Foo\" , \"$defs\" : { \"Foo\" : { \"type\" : \"object\" , \"properties\" : { \"bar\" : { \"$ref\" : \"#/$defs/Bar\" }}, \"required\" : [ \"bar\" ], \"additionalProperties\" : False , }, \"Bar\" : { \"type\" : \"object\" , \"properties\" : { \"baz\" : { \"type\" : [ \"integer\" , \"null\" ]}}, \"required\" : [ \"baz\" ], \"additionalProperties\" : False , }, }, } assert deserialization_schema ( Foo , all_refs = True , version = JsonSchemaVersion . DRAFT_7 ) == { \"$schema\" : \"http://json-schema.org/draft-07/schema#\" , # $ref is isolated in allOf + draft 7 prefix \"allOf\" : [{ \"$ref\" : \"#/definitions/Foo\" }], \"definitions\" : { # not \"$defs\" \"Foo\" : { \"type\" : \"object\" , \"properties\" : { \"bar\" : { \"$ref\" : \"#/definitions/Bar\" }}, \"required\" : [ \"bar\" ], \"additionalProperties\" : False , }, \"Bar\" : { \"type\" : \"object\" , \"properties\" : { \"baz\" : { \"type\" : [ \"integer\" , \"null\" ]}}, \"required\" : [ \"baz\" ], \"additionalProperties\" : False , }, }, } assert deserialization_schema ( Foo , version = JsonSchemaVersion . OPEN_API_3_0 ) == { # No definitions for OpenAPI, use refs_schema for it \"$ref\" : \"#/components/schema/Foo\" , # OpenAPI prefix } assert definitions_schema ( deserialization = [ Foo ], version = JsonSchemaVersion . OPEN_API_3_0 ) == { \"Foo\" : { \"type\" : \"object\" , \"properties\" : { \"bar\" : { \"$ref\" : \"#/components/schema/Bar\" }}, \"required\" : [ \"bar\" ], \"additionalProperties\" : False , }, \"Bar\" : { \"type\" : \"object\" , # \"nullable\" instead of \"type\": \"null\" \"properties\" : { \"baz\" : { \"type\" : \"integer\" , \"nullable\" : True }}, \"required\" : [ \"baz\" ], \"additionalProperties\" : False , }, } readOnly / writeOnly \u00b6 Dataclasses InitVar and field(init=False) fields will be flagged respectively with \"writeOnly\": true and \"readOnly\": true in the generated schema. If deserialization and serialization schemas both appears in definition_schema , properties are merged and the resulting schema contains then readOnly and writeOnly properties. By the way, the required is not merged because it can't (it would mess up validation if some not-init field was required), so deserialization required is kept because it's more important as it can be used in validation ( OpenAPI 3.0 semantic which allows the merge has been dropped in 3.1, so it has not been judged useful to be supported) FAQ \u00b6 Why field default value is not used by default to to generate JSON schema? \u00b6 Actually, default value is not always the usable in the schema, for example NotNull fields with a None default value. Because default keyword is kind of facultative in the schema, it has been decided to not put it by default in order to not put wrong default by accident.","title":"JSON schema"},{"location":"json_schema/#json-schema","text":"","title":"JSON schema"},{"location":"json_schema/#json-schema-generation","text":"JSON schema can be generated from data model. However, because of all possible customizations , schema can be differ between deserilialization and serialization. In common cases, deserialization_schema and serialization_schema will give the same result. from dataclasses import dataclass from apischema.json_schema import deserialization_schema , serialization_schema @dataclass class Foo : bar : str assert deserialization_schema ( Foo ) == serialization_schema ( Foo ) assert deserialization_schema ( Foo ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"additionalProperties\" : False , \"properties\" : { \"bar\" : { \"type\" : \"string\" }}, \"required\" : [ \"bar\" ], \"type\" : \"object\" , }","title":"JSON schema generation"},{"location":"json_schema/#field-alias","text":"Sometimes dataclass field names can clash with language keyword, sometimes the property name is not convenient. Hopefully, field can define an alias which will be used in schema and deserialization/serialization. from dataclasses import dataclass , field from apischema import alias , deserialize , serialize from apischema.json_schema import deserialization_schema @dataclass class Foo : class_ : str = field ( metadata = alias ( \"class\" )) assert deserialization_schema ( Foo ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"additionalProperties\" : False , \"properties\" : { \"class\" : { \"type\" : \"string\" }}, \"required\" : [ \"class\" ], \"type\" : \"object\" , } assert deserialize ( Foo , { \"class\" : \"bar\" }) == Foo ( \"bar\" ) assert serialize ( Foo ( \"bar\" )) == { \"class\" : \"bar\" }","title":"Field alias"},{"location":"json_schema/#alias-all-fields","text":"Field aliasing can also be done at class level by specifying an aliasing function. This aliaser is applied to field alias if defined or field name, or not applied if override=False is specified. from dataclasses import dataclass , field from typing import Any from apischema import alias from apischema.json_schema import deserialization_schema @alias ( lambda s : f \"foo_ { s } \" ) @dataclass class Foo : field1 : Any field2 : Any = field ( metadata = alias ( override = False )) field3 : Any = field ( metadata = alias ( \"field03\" )) field4 : Any = field ( metadata = alias ( \"field04\" , override = False )) assert deserialization_schema ( Foo ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"additionalProperties\" : False , \"properties\" : { \"foo_field1\" : {}, \"field2\" : {}, \"foo_field03\" : {}, \"field04\" : {}}, \"required\" : [ \"foo_field1\" , \"field2\" , \"foo_field03\" , \"field04\" ], \"type\" : \"object\" , } Class-level aliasing can be used to define a camelCase API.","title":"Alias all fields"},{"location":"json_schema/#global-aliaser","text":"apischema.settings.aliaser can be used to set a global aliaser function which will be applied to every field ( override=False is ignored by the global aliaser). It can be used for example to make all an application use camelCase . Actually, there is a shortcut for that: from apischema import settings settings . aliaser ( camel_case = True ) Otherwise, it's used the same way than settings.coercer .","title":"Global aliaser"},{"location":"json_schema/#schema-annotations","text":"Type annotations are not enough to express a complete schema, but Apischema has a function for that; schema can be used both as type decorator or field metadata. from dataclasses import dataclass , field from typing import List , NewType from apischema import schema from apischema.json_schema import deserialization_schema Tag = NewType ( \"Tag\" , str ) schema ( min_len = 3 , pattern = r \"^\\w*$\" , examples = [ \"available\" , \"EMEA\" ])( Tag ) @dataclass class Resource : id : int tags : List [ Tag ] = field ( default_factory = list , metadata = schema ( description = \"regroup multiple resources\" , max_items = 3 , unique = True ), ) assert deserialization_schema ( Resource ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"additionalProperties\" : False , \"properties\" : { \"id\" : { \"type\" : \"integer\" }, \"tags\" : { \"description\" : \"regroup multiple resources\" , \"items\" : { \"examples\" : [ \"available\" , \"EMEA\" ], \"minLength\" : 3 , \"pattern\" : \"^ \\\\ w*$\" , \"type\" : \"string\" , }, \"maxItems\" : 3 , \"type\" : \"array\" , \"uniqueItems\" : True , }, }, \"required\" : [ \"id\" ], \"type\" : \"object\" , } Note Schema are particularly useful with NewType . For example, if you use prefixed ids, you can use a NewType with a pattern schema to validate them, and benefit of more precise type checking. The following keys are available (they are sometimes shorten compared to JSON schema original for code concision and snake_case): Key JSON schema keyword type restriction title / / description / / default / / examples / / min minimum int max maximum int exc_min exclusiveMinimum int exc_max exclusiveMaximum int mult_of multipleOf int format / str min_len minLength str max_len maxLength str pattern / str min_items minItems list max_items maxItems list unique / list min_props minProperties dict max_props maxProperties dict schema function has an overloaded signature which prevents to mix incompatible keywords. By the way, one other extra keyword allows to add arbitrary keys to JSON schema generated; setting extra_only to True force Apischema to use only extra keys. from typing import NewType from apischema import schema from apischema.json_schema import deserialization_schema Foo = NewType ( \"Foo\" , str ) schema ( min_len = 20 , extra = { \"typeName\" : \"Foo\" })( Foo ) Bar = NewType ( \"Bar\" , int ) schema ( extra = { \"$ref\" : \"http://some-domain.org/path/tp/schema.json#/$defs/Bar\" }, extra_only = True , )( Bar ) assert deserialization_schema ( Foo ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"type\" : \"string\" , \"minLength\" : 20 , \"typeName\" : \"Foo\" , } assert deserialization_schema ( Bar ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"$ref\" : \"http://some-domain.org/path/tp/schema.json#/$defs/Bar\" , }","title":"Schema annotations"},{"location":"json_schema/#default-annotation","text":"default annotation is not added automatically when a field has a default value (see FAQ ); schema default parameter must be used in order to make it appear in the schema. However ... can be used as a placeholder to make Apischema use field default value; this one will be serialized \u2014 if serialization fails, error will be ignored as well as default annotation.","title":"default annotation"},{"location":"json_schema/#constraints-validation","text":"JSON schema constrains the data deserialized; this constraints are naturally used for validation. from dataclasses import dataclass , field from typing import List , NewType from pytest import raises from apischema import ValidationError , deserialize , schema , serialize Tag = NewType ( \"Tag\" , str ) schema ( min_len = 3 , pattern = r \"^\\w*$\" , examples = [ \"available\" , \"EMEA\" ])( Tag ) @dataclass class Resource : id : int tags : List [ Tag ] = field ( default_factory = list , metadata = schema ( description = \"regroup multiple resources\" , max_items = 3 , unique = True ), ) with raises ( ValidationError ) as err : # pytest check exception is raised deserialize ( Resource , { \"id\" : 42 , \"tags\" : [ \"tag\" , \"duplicate\" , \"duplicate\" , \"bad&\" , \"_\" ]} ) assert serialize ( err . value ) == [ { \"loc\" : [ \"tags\" ], \"err\" : [ \"size greater than 3 (maxItems)\" , \"duplicate items (uniqueItems)\" ], }, { \"loc\" : [ \"tags\" , 3 ], \"err\" : [ \"unmatched pattern '^ \\\\ w*$'\" ]}, { \"loc\" : [ \"tags\" , 4 ], \"err\" : [ \"length less than 3 (minLength)\" ]}, ]","title":"Constraints validation"},{"location":"json_schema/#required-field-with-default-value","text":"By default, a dataclass/namedtuple field will be tagged required if it doesn't have a default value. However, you may want to have a default value for a field in order to be more convenient in your code, but still make the field required. One could think about some schema model where version is fixed but is required, for example JSON-RPC with \"jsonrpc\": \"2.0\" . That's done with field metadata required . from dataclasses import dataclass , field from typing import Optional from pytest import raises from apischema import ValidationError , deserialize , serialize from apischema.metadata import required @dataclass class Foo : bar : Optional [ int ] = field ( default = None , metadata = required ) with raises ( ValidationError ) as err : deserialize ( Foo , {}) assert serialize ( err . value ) == [{ \"loc\" : [ \"bar\" ], \"err\" : [ \"missing property\" ]}]","title":"Required field with default value"},{"location":"json_schema/#additional-properties-pattern-properties","text":"","title":"Additional properties / pattern properties"},{"location":"json_schema/#with-mapping","text":"Schema of a Mapping / Dict type is naturally translated to \"additionalProperties\": <schema of the value type> . However when the schema of the key has a pattern , it will give a \"patternProperties\": {<key pattern>: <schema of the value type>}","title":"With Mapping"},{"location":"json_schema/#with-dataclass","text":"additionalProperties / patternProperties can be added to dataclasses by using fields annotated with properties metadata. Properties not mapped on regular fields will be deserialized into this fields; they must have a Mapping type (or be convertible from Mapping ) because they are instanciated with a mapping. from dataclasses import dataclass , field from typing import Mapping from typing_extensions import Annotated from apischema import ( deserialize , properties , schema , schema_ref , ) from apischema.json_schema import deserialization_schema @schema_ref ( None ) @dataclass class Config : active : bool = True server_options : Mapping [ str , bool ] = field ( default_factory = dict , metadata = properties ( pattern = r \"^server_\" ) ) client_options : Mapping [ Annotated [ str , schema ( pattern = r \"^client_\" )], bool ] = field ( default_factory = dict , metadata = properties ( ... ) ) options : Mapping [ str , bool ] = field ( default_factory = dict , metadata = properties ) assert deserialize ( Config , { \"use_lightsaber\" : True , \"server_auto_restart\" : False , \"client_timeout\" : False }, ) == Config ( True , { \"server_auto_restart\" : False }, { \"client_timeout\" : False }, { \"use_lightsaber\" : True }, ) assert deserialization_schema ( Config ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"type\" : \"object\" , \"properties\" : { \"active\" : { \"type\" : \"boolean\" }}, \"additionalProperties\" : { \"type\" : \"boolean\" }, \"patternProperties\" : { \"^server_\" : { \"type\" : \"boolean\" }, \"^client_\" : { \"type\" : \"boolean\" }, }, } Note Of course, a dataclass can only have a single properties field without pattern, because it makes no sens to have several additionalProperties .","title":"With dataclass"},{"location":"json_schema/#property-dependencies","text":"Apischema support property dependencies for dataclass through a class member. Dependencies are also used in validation. Note JSON schema draft 2019-09 renames properties dependencies dependentRequired to disambiguate with schema dependencies from dataclasses import dataclass , field from pytest import raises from apischema import ( NotNull , ValidationError , deserialize , serialize , ) from apischema.dependencies import DependentRequired from apischema.json_schema import deserialization_schema @dataclass class Billing : name : str # Fields used in dependencies MUST be declared with `field` credit_card : NotNull [ int ] = field ( default = None ) billing_address : NotNull [ str ] = field ( default = None ) dependencies = DependentRequired ({ credit_card : [ billing_address ]}) assert deserialization_schema ( Billing ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"additionalProperties\" : False , \"dependentRequired\" : { \"credit_card\" : [ \"billing_address\" ]}, \"properties\" : { \"name\" : { \"type\" : \"string\" }, \"credit_card\" : { \"type\" : \"integer\" }, \"billing_address\" : { \"type\" : \"string\" }, }, \"required\" : [ \"name\" ], \"type\" : \"object\" , } with raises ( ValidationError ) as err : deserialize ( Billing , { \"name\" : \"Anonymous\" , \"credit_card\" : 1234_5678_9012_3456 }) assert serialize ( err . value ) == [ { \"loc\" : [ \"billing_address\" ], \"err\" : [ \"missing property (required by ['credit_card'])\" ], } ] Because bidirectional dependencies are a common idiom, Apischema provides a shortcut notation. Its indeed possible to write DependentRequired([credit_card, billing_adress]) .","title":"Property dependencies"},{"location":"json_schema/#complexrecursive-types-json-schema-definitionsopenapi-components","text":"For complex schema with type reuse, it's convenient to extract definitions of schema components in order to reuse them; it's even mandatory for recursive types. Then, schema use JSON pointers \"$ref\" to refer to the definitions. Apischema handles this feature natively. from dataclasses import dataclass from typing import Optional from apischema.json_schema import deserialization_schema @dataclass class Node : value : int child : Optional [ \"Node\" ] = None assert deserialization_schema ( Node ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"$ref\" : \"#/$defs/Node\" , \"$defs\" : { \"Node\" : { \"type\" : \"object\" , \"properties\" : { \"value\" : { \"type\" : \"integer\" }, \"child\" : { \"anyOf\" : [{ \"$ref\" : \"#/$defs/Node\" }, { \"type\" : \"null\" }]}, }, \"required\" : [ \"value\" ], \"additionalProperties\" : False , } }, }","title":"Complex/recursive types - JSON schema definitions/OpenAPI components"},{"location":"json_schema/#use-ref-only-for-reused-types","text":"If some types appear only once in the schema, you maybe don't want to use a $ref and a definition but inline the type definition directly. It is possible by setting schema_ref(None) (see next section ) on the the concerned type, but it could affect others schema where this types is reused several time. However, Apischema provides a parameter all_ref for this reason: - all_refs=True -> all types with a reference will be put in the definitions and referenced with $ref ; - all_refs=False -> only types which are reused in the schema are put in definitions all_refs default value depends on the JSON schema version : it's False for JSON schema drafts but True for OpenAPI . from dataclasses import dataclass from apischema.json_schema import deserialization_schema @dataclass class Bar : baz : str @dataclass class Foo : bar1 : Bar bar2 : Bar assert deserialization_schema ( Foo , all_refs = False ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"$defs\" : { \"Bar\" : { \"additionalProperties\" : False , \"properties\" : { \"baz\" : { \"type\" : \"string\" }}, \"required\" : [ \"baz\" ], \"type\" : \"object\" , } }, \"additionalProperties\" : False , \"properties\" : { \"bar1\" : { \"$ref\" : \"#/$defs/Bar\" }, \"bar2\" : { \"$ref\" : \"#/$defs/Bar\" }}, \"required\" : [ \"bar1\" , \"bar2\" ], \"type\" : \"object\" , } assert deserialization_schema ( Foo , all_refs = True ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"$defs\" : { \"Bar\" : { \"additionalProperties\" : False , \"properties\" : { \"baz\" : { \"type\" : \"string\" }}, \"required\" : [ \"baz\" ], \"type\" : \"object\" , }, \"Foo\" : { \"additionalProperties\" : False , \"properties\" : { \"bar1\" : { \"$ref\" : \"#/$defs/Bar\" }, \"bar2\" : { \"$ref\" : \"#/$defs/Bar\" }, }, \"required\" : [ \"bar1\" , \"bar2\" ], \"type\" : \"object\" , }, }, \"$ref\" : \"#/$defs/Foo\" , }","title":"Use ref only for reused types"},{"location":"json_schema/#definitions-schema-only","text":"Sometimes you need to extract only the definitions in a separate schema (especially OpenAPI components). That's done with the definitions_schema function. It takes two lists deserializations and serializations of schema (or schema + conversions ) and combines the definitions of all the schema that would have been generated with types given in the list of schema. from dataclasses import dataclass from typing import List from apischema.json_schema import definitions_schema @dataclass class Bar : baz : int = 0 @dataclass class Foo : bar : Bar assert definitions_schema ( deserialization = [ List [ Foo ]], all_refs = True ) == { \"Foo\" : { \"type\" : \"object\" , \"properties\" : { \"bar\" : { \"$ref\" : \"#/$defs/Bar\" }}, \"required\" : [ \"bar\" ], \"additionalProperties\" : False , }, \"Bar\" : { \"type\" : \"object\" , \"properties\" : { \"baz\" : { \"type\" : \"integer\" }}, \"additionalProperties\" : False , }, }","title":"Definitions schema only"},{"location":"json_schema/#customize-ref","text":"","title":"Customize $ref"},{"location":"json_schema/#adds-ref-for-every-types","text":"In the previous example, only dataclasses has a $ref , but it can be fully customized. You can use schema_ref on any defined types ( NewType / class / Annotated /etc.). schema_ref argument can be: - str -> this string will be used directly in schema generation - ... -> schema generation will use the name of the - None -> this type will have no $ref in schema from dataclasses import dataclass from typing import AbstractSet , NewType from apischema import schema_ref from apischema.json_schema import deserialization_schema Tags = NewType ( \"Tags\" , AbstractSet [ str ]) schema_ref ( ... )( Tags ) @dataclass class Resource : id : int tags : Tags assert deserialization_schema ( Resource , all_refs = True ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"$defs\" : { \"Resource\" : { \"type\" : \"object\" , \"properties\" : { \"id\" : { \"type\" : \"integer\" }, \"tags\" : { \"$ref\" : \"#/$defs/Tags\" }}, \"required\" : [ \"id\" , \"tags\" ], \"additionalProperties\" : False , }, \"Tags\" : { \"type\" : \"array\" , \"items\" : { \"type\" : \"string\" }, \"uniqueItems\" : True }, }, \"$ref\" : \"#/$defs/Resource\" , } Moreover, there is a default schema_ref for each type when the function is not used directly; by default, it's implemented like this ... if is_dataclass(cls) else None , what means that dataclasses have a default ref with their name but other types don't. This default behavior is customizable by setting settings.default_ref function like this from apischema import settings @settings . default_ref def default_ref ( cls ): return None # This example remove default ref for every types Note Actually, there is a small restriction with NewType : you cannot put a schema_ref if the super type is not a builtin type ( List[...] / int /etc.).","title":"Adds $ref for every types"},{"location":"json_schema/#ref-factory","text":"schema_ref is used to set a short ref, like the name of a class, but in schema, $ref looks like #/$defs/Foo . In fact, schema generation use the ref given by schema_ref and pass it to a ref_factory function (a parameter of schema generation functions) which will convert it to its final form. JSON schema version comes with its default ref_factory , for draft 2019-09, it prefixes the ref with #/$defs/ , while it prefixes with #/components/schema in case of OpenAPI . from dataclasses import dataclass from apischema.json_schema import deserialization_schema @dataclass class Foo : bar : int def ref_factory ( ref : str ) -> str : return f \"http://some-domain.org/path/to/ { ref } .json#\" assert deserialization_schema ( Foo , ref_factory = ref_factory ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"$ref\" : \"http://some-domain.org/path/to/Foo.json#\" , } Note When ref_factory is passed in arguments, definitions are not added to the generated schema. That's because ref_factory would surely change definitions location, so there would be no interest to add them with a wrong location. This definitions can of course be generated separately with definitions_schema . Passing ref_factory also give a default value of True for all_refs parameters.","title":"Ref factory"},{"location":"json_schema/#json-schema-openapi-version","text":"JSON schema has several versions \u2014 OpenAPI is treated as a JSON schema version. If Apischema natively use the last one: draft 2019-09, it is possible to specify a schema version which will be used for the generation. from dataclasses import dataclass from typing import Optional from apischema.json_schema import ( JsonSchemaVersion , definitions_schema , deserialization_schema , ) @dataclass class Bar : baz : Optional [ int ] @dataclass class Foo : bar : Bar assert deserialization_schema ( Foo , all_refs = True ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"$ref\" : \"#/$defs/Foo\" , \"$defs\" : { \"Foo\" : { \"type\" : \"object\" , \"properties\" : { \"bar\" : { \"$ref\" : \"#/$defs/Bar\" }}, \"required\" : [ \"bar\" ], \"additionalProperties\" : False , }, \"Bar\" : { \"type\" : \"object\" , \"properties\" : { \"baz\" : { \"type\" : [ \"integer\" , \"null\" ]}}, \"required\" : [ \"baz\" ], \"additionalProperties\" : False , }, }, } assert deserialization_schema ( Foo , all_refs = True , version = JsonSchemaVersion . DRAFT_7 ) == { \"$schema\" : \"http://json-schema.org/draft-07/schema#\" , # $ref is isolated in allOf + draft 7 prefix \"allOf\" : [{ \"$ref\" : \"#/definitions/Foo\" }], \"definitions\" : { # not \"$defs\" \"Foo\" : { \"type\" : \"object\" , \"properties\" : { \"bar\" : { \"$ref\" : \"#/definitions/Bar\" }}, \"required\" : [ \"bar\" ], \"additionalProperties\" : False , }, \"Bar\" : { \"type\" : \"object\" , \"properties\" : { \"baz\" : { \"type\" : [ \"integer\" , \"null\" ]}}, \"required\" : [ \"baz\" ], \"additionalProperties\" : False , }, }, } assert deserialization_schema ( Foo , version = JsonSchemaVersion . OPEN_API_3_0 ) == { # No definitions for OpenAPI, use refs_schema for it \"$ref\" : \"#/components/schema/Foo\" , # OpenAPI prefix } assert definitions_schema ( deserialization = [ Foo ], version = JsonSchemaVersion . OPEN_API_3_0 ) == { \"Foo\" : { \"type\" : \"object\" , \"properties\" : { \"bar\" : { \"$ref\" : \"#/components/schema/Bar\" }}, \"required\" : [ \"bar\" ], \"additionalProperties\" : False , }, \"Bar\" : { \"type\" : \"object\" , # \"nullable\" instead of \"type\": \"null\" \"properties\" : { \"baz\" : { \"type\" : \"integer\" , \"nullable\" : True }}, \"required\" : [ \"baz\" ], \"additionalProperties\" : False , }, }","title":"JSON schema / OpenAPI version"},{"location":"json_schema/#readonly-writeonly","text":"Dataclasses InitVar and field(init=False) fields will be flagged respectively with \"writeOnly\": true and \"readOnly\": true in the generated schema. If deserialization and serialization schemas both appears in definition_schema , properties are merged and the resulting schema contains then readOnly and writeOnly properties. By the way, the required is not merged because it can't (it would mess up validation if some not-init field was required), so deserialization required is kept because it's more important as it can be used in validation ( OpenAPI 3.0 semantic which allows the merge has been dropped in 3.1, so it has not been judged useful to be supported)","title":"readOnly / writeOnly"},{"location":"json_schema/#faq","text":"","title":"FAQ"},{"location":"json_schema/#why-field-default-value-is-not-used-by-default-to-to-generate-json-schema","text":"Actually, default value is not always the usable in the schema, for example NotNull fields with a None default value. Because default keyword is kind of facultative in the schema, it has been decided to not put it by default in order to not put wrong default by accident.","title":"Why field default value is not used by default to to generate JSON schema?"},{"location":"validation/","text":"Validation \u00b6 Validation is an important part of deserialization. By default, Apischema validate types of data according to typing annotations, and schema constraints. But custom validators can also be add for a more precise validation. Deserialization and validation error \u00b6 ValidationError is raised when validation fails. This exception will contains all the information about the ill-formed part of the data. By the way this exception is also serializable and can be sent back directly to client. from dataclasses import dataclass , field from typing import List , NewType from pytest import raises from apischema import ValidationError , deserialize , schema , serialize Tag = NewType ( \"Tag\" , str ) schema ( min_len = 3 , pattern = r \"^\\w*$\" , examples = [ \"available\" , \"EMEA\" ])( Tag ) @dataclass class Resource : id : int tags : List [ Tag ] = field ( default_factory = list , metadata = schema ( description = \"regroup multiple resources\" , max_items = 3 , unique = True ), ) with raises ( ValidationError ) as err : # pytest check exception is raised deserialize ( Resource , { \"id\" : 42 , \"tags\" : [ \"tag\" , \"duplicate\" , \"duplicate\" , \"bad&\" , \"_\" ]} ) assert serialize ( err . value ) == [ { \"loc\" : [ \"tags\" ], \"err\" : [ \"size greater than 3 (maxItems)\" , \"duplicate items (uniqueItems)\" ], }, { \"loc\" : [ \"tags\" , 3 ], \"err\" : [ \"unmatched pattern '^ \\\\ w*$'\" ]}, { \"loc\" : [ \"tags\" , 4 ], \"err\" : [ \"length less than 3 (minLength)\" ]}, ] As shown in the example, Apischema will not stop at the first error met but tries to validate all parts of the data. Dataclass validators \u00b6 Dataclass validation can be completed by custom validators. These are simple decorated methods which will be executed during validation, after all fields having been deserialized. from dataclasses import dataclass from pytest import raises from apischema import ValidationError , deserialize , serialize , validator @dataclass class PasswordForm : password : str confirmation : str @validator def password_match ( self ): # DO NOT use assert if self . password != self . confirmation : raise ValueError ( \"password doesn't match its confirmation\" ) with raises ( ValidationError ) as err : deserialize ( PasswordForm , { \"password\" : \"p455w0rd\" , \"confirmation\" : \"...\" }) assert serialize ( err . value ) == [ { \"loc\" : [], \"err\" : [ \"password doesn't match its confirmation\" ]} ] Warning DO NOT use assert statement to validate external data, never. In fact, this statement is made to be disabled when executed in optimized mode (see documentation ), so validation would be disabled too. This warning doesn't concern only Apischema ; assert is only for internal assertion in debug/development environment. That's why Apischema will not catch AssertionError as a validation error but reraises it, making deserialize fail. Note Validators are alawys executed in order of declaration. Computed dependencies \u00b6 It makes no sense to execute a validator using a field that is ill-formed. Hopefully, Apischema is able to compute validator dependencies \u2014 the fields used in validator; validator is executed only if the all its dependencies are ok. from dataclasses import dataclass from pytest import raises from apischema import ValidationError , deserialize , serialize , validator @dataclass class PasswordForm : password : str confirmation : str @validator def password_match ( self ): if self . password != self . confirmation : raise ValueError ( \"password doesn't match its confirmation\" ) with raises ( ValidationError ) as err : deserialize ( PasswordForm , { \"password\" : \"p455w0rd\" }) assert serialize ( err . value ) == [ # validator is not executed because confirmation is missing { \"loc\" : [ \"confirmation\" ], \"err\" : [ \"missing property\" ]} ] Note Despite the fact that validator use self argument, it can be called during validation even if all the fields of the class are not ok and the class not really instantiated. In fact, instance is kind of mocked for validation with only the needed field. Raise more than one error with yield \u00b6 Validation of list field can require to raise several exception, one for each bad elements. With raise , this is not possible, because you can raise only once. However, Apischema provides a way or raising as many errors as needed by using yield . Moreover, with this syntax, it is possible to add a \"path\" to the error to precise its location in the validated data. This path will be added to the loc key of the error. from dataclasses import dataclass from ipaddress import IPv4Address , IPv4Network from typing import List from pytest import raises from apischema import ValidationError , deserialize , serialize , validator from apischema.fields import fields @dataclass class SubnetIps : subnet : IPv4Network ips : List [ IPv4Address ] @validator def check_ips_in_subnet ( self ): for index , ip in enumerate ( self . ips ): if ip not in self . subnet : # yield <error path>, <error message> yield ( fields ( self ) . ips , index ), \"ip not in subnet\" with raises ( ValidationError ) as err : deserialize ( SubnetIps , { \"subnet\" : \"126.42.18.0/24\" , \"ips\" : [ \"126.42.18.1\" , \"126.42.19.0\" , \"0.0.0.0\" ]}, ) assert serialize ( err . value ) == [ { \"loc\" : [ \"ips\" , 1 ], \"err\" : [ \"ip not in subnet\" ]}, { \"loc\" : [ \"ips\" , 2 ], \"err\" : [ \"ip not in subnet\" ]}, ] Error path \u00b6 In the example, validator yield a tuple of an \"error path\" and the error message. Error path can be: a string an integer (for list indices) a dataclass field (obtained with get_fields ) a tuple of this 3 components. yield can also be used with only an error message. Note For dataclass field error path, it's advised to use get_fields instead of raw string, because it will take in account potential aliasing and it will be easier to rename field with IDE refactoring. Discard \u00b6 If one of your validators fails because a field is corrupted, maybe you don't want following validators to be executed. validator decorator provides a discard parameter to discard fields of the remaining validation. All the remaining validators having discarded fields in dependencies will not be executed. from dataclasses import dataclass , field from typing import List , Tuple from pytest import raises from apischema import ValidationError , deserialize , serialize , validator from apischema.fields import fields @dataclass class BoundedValues : bounds : Tuple [ int , int ] = field () values : List [ int ] @validator ( discard = bounds ) def bounds_are_sorted ( self ): min_bound , max_bound = self . bounds if min_bound > max_bound : yield fields ( self ) . bounds , \"bounds are not sorted\" @validator def values_dont_exceed_bounds ( self ): min_bound , max_bound = self . bounds for index , value in enumerate ( self . values ): if not min_bound <= value <= max_bound : yield ( fields ( self ) . values , index ), \"value exceeds bounds\" with raises ( ValidationError ) as err : deserialize ( BoundedValues , { \"bounds\" : [ 10 , 0 ], \"values\" : [ - 1 , 2 , 4 ]}) assert serialize ( err . value ) == [ { \"loc\" : [ \"bounds\" ], \"err\" : [ \"bounds are not sorted\" ]} # Without discard, there would have been an other error # {\"loc\": [\"values\", 1], \"err\": [\"value exceeds bounds\"]} ] Field validators \u00b6 At field level \u00b6 Fields are validated according to their types and schema. But it's also possible to add validators to fields from dataclasses import dataclass , field from pytest import raises from apischema import ValidationError , deserialize , serialize from apischema.metadata import validators def check_no_duplicate_digits ( n : int ): if len ( str ( n )) != len ( set ( str ( n ))): raise ValueError ( \"number has duplicate digits\" ) @dataclass class Foo : bar : str = field ( metadata = validators ( check_no_duplicate_digits )) with raises ( ValidationError ) as err : deserialize ( Foo , { \"bar\" : \"11\" }) assert serialize ( err . value ) == [ { \"loc\" : [ \"bar\" ], \"err\" : [ \"number has duplicate digits\" ]} ] When validation fails for a field, it is discarded and cannot be used in class validators, as it is the case when field schema validation fails. Note field_validator allow to reuse the the same validator for several fields. However in this case, using a custom type (for example a NewType ) with validators (see next section ) could often be a better solution. Using other fields \u00b6 A common pattern can be to validate a field using other fields values. This is achieved with dataclass validators seen above. However there is a shortcut for this use case: from dataclasses import dataclass , field from enum import Enum from pytest import raises from apischema import ValidationError , deserialize , serialize , validator from apischema.fields import fields class Parity ( Enum ): EVEN = \"even\" ODD = \"odd\" @dataclass class NumberWithParity : parity : Parity number : int = field () # field must be assign, even with empty `field()` @validator ( number ) def check_parity ( self ): if ( self . parity == Parity . EVEN ) != ( self . number % 2 == 0 ): yield \"number doesn't respect parity\" # using field argument adds automatically discard argument # and prefix all error paths with the field @validator ( discard = number ) def check_parity_equivalent ( self ): if ( self . parity == Parity . EVEN ) != ( self . number % 2 == 0 ): yield fields ( self ) . number , \"number doesn't respect parity\" with raises ( ValidationError ) as err : deserialize ( NumberWithParity , { \"parity\" : \"even\" , \"number\" : 1 }) assert serialize ( err . value ) == [ { \"loc\" : [ \"number\" ], \"err\" : [ \"number doesn't respect parity\" ]} ] Validators inheritance \u00b6 Validators are inherited just like other class fields. from dataclasses import dataclass from pytest import raises from apischema import ValidationError , deserialize , serialize , validator @dataclass class PasswordForm : password : str confirmation : str @validator def password_match ( self ): if self . password != self . confirmation : raise ValueError ( \"password doesn't match its confirmation\" ) @dataclass class CompleteForm ( PasswordForm ): username : str with raises ( ValidationError ) as err : deserialize ( CompleteForm , { \"username\" : \"wyfo\" , \"password\" : \"p455w0rd\" , \"confirmation\" : \"...\" }, ) assert serialize ( err . value ) == [ { \"loc\" : [], \"err\" : [ \"password doesn't match its confirmation\" ]} ] Validator with InitVar \u00b6 Dataclasses InitVar are accessible in validators by using parameters the same way __post_init__ does. Only the needed fields has to be put in parameters, they are then added to validator dependencies. from dataclasses import InitVar , dataclass , field from pytest import raises from apischema import ValidationError , deserialize , serialize , validator from apischema.metadata import init_var @dataclass class Foo : bar : InitVar [ int ] = field ( metadata = init_var ( int )) @validator ( bar ) def validate ( self , bar : int ): if bar < 0 : yield \"negative\" with raises ( ValidationError ) as err : deserialize ( Foo , { \"bar\" : - 1 }) assert serialize ( err . value ) == [{ \"loc\" : [ \"bar\" ], \"err\" : [ \"negative\" ]}] Validators are not run on default values \u00b6 If all validator dependencies are initialized with their defau lt values, they are not run; make sure your default values make sens. from dataclasses import dataclass , field from apischema import deserialize , validator validator_run = False @dataclass class Foo : bar : int = field ( default = 0 ) @validator ( bar ) def password_match ( self ): global validator_run validator_run = True if self . bar < 0 : raise ValueError ( \"negative\" ) deserialize ( Foo , {}) assert not validator_run Validators for every type \u00b6 Validators can be added to other user-defined types. When a user type is deseriarialized (even in case of conversion ), its validators are played. from typing import NewType from pytest import raises from apischema import ValidationError , deserialize , serialize from apischema.validation import add_validator Palindrome = NewType ( \"Palindrome\" , str ) @add_validator ( Palindrome ) def check_palindrome ( s : str ): for i in range ( len ( s ) // 2 ): if s [ i ] != s [ - 1 - i ]: raise ValueError ( \"Not a palindrome\" ) assert deserialize ( Palindrome , \"tacocat\" ) == \"tacocat\" with raises ( ValidationError ) as err : deserialize ( Palindrome , \"not a palindrome\" ) assert serialize ( err . value ) == [{ \"loc\" : [], \"err\" : [ \"Not a palindrome\" ]}] Type checker \u00b6 Validation is done only at deserialization (see FAQ ). However, it is possible at runtime to check the type of an object, and to do it descending all the type model of the given types (dataclass fields, list elements, etc.) with the function check_types . It can also \"validate\" the object (running all its validator and validate its constraints) if needed. from dataclasses import dataclass , field from typing import AbstractSet from uuid import UUID , uuid4 from pytest import raises from apischema import ValidationError , check_types , schema , serialize @dataclass class Resource : id : UUID tags : AbstractSet [ str ] = field ( metadata = schema ( max_items = 3 )) check_types ( Resource , Resource ( uuid4 (), { \"tag\" })) # no error with raises ( ValidationError ) as err : check_types ( Resource , Resource ( \"id\" , { 0 })) # type: ignore assert serialize ( err . value ) == [ { \"loc\" : [ \"id\" ], \"err\" : [ \"expected UUID, found type str\" ]}, { \"loc\" : [ \"tags\" , 0 ], \"err\" : [ \"expected str, found type int\" ]}, ] too_many_tags = Resource ( uuid4 (), { \"0\" , \"1\" , \"2\" , \"3\" }) assert check_types ( Resource , too_many_tags ) # No error because no validation with raises ( ValidationError ) as err : check_types ( Resource , too_many_tags , validate = True ) # type: ignore assert serialize ( err . value ) == [ { \"loc\" : [ \"tags\" ], \"err\" : [ \"size greater than 3 (maxItems)\" ]} ] FAQ \u00b6 How are computed validator depedencies? \u00b6 ast.NodeVisitor and the Python black magic begins... Why only validate at deserialization and not at instantiation? \u00b6 Dataclass are typed-checked, so data put in the constructor are supposed to be typed-checked too, so validation would be useless most of the time. That's why it doesn't seem justified to add this high overhead everywhere for everyone. By the way, if it was implemented, in order to be consistent, validation should be performed each time the instance is modified (if you don't trust __init__ argument, why would it be different for __setattr__ ) and it would add even more overhead. Other libraries exist to ensure preservation of class invariants. Why use validators for dataclasses instead of doing validation in __post_init__ ? \u00b6 Actually, validation can completly be done in __post_init__ , there is not problem with that. However, validators offers one thing that cannot be achieved with __post_init__ : they are run before __init__ , so they can validate incomplete data. Moreover, they are only run during deserialization, so they don't add overhead to normal class instantiation (yes, that's a minor argument). Of course, there is also features like dependencies computing, discard ... (which could be achieved in __post_init__ but require some lines of code).","title":"Validation"},{"location":"validation/#validation","text":"Validation is an important part of deserialization. By default, Apischema validate types of data according to typing annotations, and schema constraints. But custom validators can also be add for a more precise validation.","title":"Validation"},{"location":"validation/#deserialization-and-validation-error","text":"ValidationError is raised when validation fails. This exception will contains all the information about the ill-formed part of the data. By the way this exception is also serializable and can be sent back directly to client. from dataclasses import dataclass , field from typing import List , NewType from pytest import raises from apischema import ValidationError , deserialize , schema , serialize Tag = NewType ( \"Tag\" , str ) schema ( min_len = 3 , pattern = r \"^\\w*$\" , examples = [ \"available\" , \"EMEA\" ])( Tag ) @dataclass class Resource : id : int tags : List [ Tag ] = field ( default_factory = list , metadata = schema ( description = \"regroup multiple resources\" , max_items = 3 , unique = True ), ) with raises ( ValidationError ) as err : # pytest check exception is raised deserialize ( Resource , { \"id\" : 42 , \"tags\" : [ \"tag\" , \"duplicate\" , \"duplicate\" , \"bad&\" , \"_\" ]} ) assert serialize ( err . value ) == [ { \"loc\" : [ \"tags\" ], \"err\" : [ \"size greater than 3 (maxItems)\" , \"duplicate items (uniqueItems)\" ], }, { \"loc\" : [ \"tags\" , 3 ], \"err\" : [ \"unmatched pattern '^ \\\\ w*$'\" ]}, { \"loc\" : [ \"tags\" , 4 ], \"err\" : [ \"length less than 3 (minLength)\" ]}, ] As shown in the example, Apischema will not stop at the first error met but tries to validate all parts of the data.","title":"Deserialization and validation error"},{"location":"validation/#dataclass-validators","text":"Dataclass validation can be completed by custom validators. These are simple decorated methods which will be executed during validation, after all fields having been deserialized. from dataclasses import dataclass from pytest import raises from apischema import ValidationError , deserialize , serialize , validator @dataclass class PasswordForm : password : str confirmation : str @validator def password_match ( self ): # DO NOT use assert if self . password != self . confirmation : raise ValueError ( \"password doesn't match its confirmation\" ) with raises ( ValidationError ) as err : deserialize ( PasswordForm , { \"password\" : \"p455w0rd\" , \"confirmation\" : \"...\" }) assert serialize ( err . value ) == [ { \"loc\" : [], \"err\" : [ \"password doesn't match its confirmation\" ]} ] Warning DO NOT use assert statement to validate external data, never. In fact, this statement is made to be disabled when executed in optimized mode (see documentation ), so validation would be disabled too. This warning doesn't concern only Apischema ; assert is only for internal assertion in debug/development environment. That's why Apischema will not catch AssertionError as a validation error but reraises it, making deserialize fail. Note Validators are alawys executed in order of declaration.","title":"Dataclass validators"},{"location":"validation/#computed-dependencies","text":"It makes no sense to execute a validator using a field that is ill-formed. Hopefully, Apischema is able to compute validator dependencies \u2014 the fields used in validator; validator is executed only if the all its dependencies are ok. from dataclasses import dataclass from pytest import raises from apischema import ValidationError , deserialize , serialize , validator @dataclass class PasswordForm : password : str confirmation : str @validator def password_match ( self ): if self . password != self . confirmation : raise ValueError ( \"password doesn't match its confirmation\" ) with raises ( ValidationError ) as err : deserialize ( PasswordForm , { \"password\" : \"p455w0rd\" }) assert serialize ( err . value ) == [ # validator is not executed because confirmation is missing { \"loc\" : [ \"confirmation\" ], \"err\" : [ \"missing property\" ]} ] Note Despite the fact that validator use self argument, it can be called during validation even if all the fields of the class are not ok and the class not really instantiated. In fact, instance is kind of mocked for validation with only the needed field.","title":"Computed dependencies"},{"location":"validation/#raise-more-than-one-error-with-yield","text":"Validation of list field can require to raise several exception, one for each bad elements. With raise , this is not possible, because you can raise only once. However, Apischema provides a way or raising as many errors as needed by using yield . Moreover, with this syntax, it is possible to add a \"path\" to the error to precise its location in the validated data. This path will be added to the loc key of the error. from dataclasses import dataclass from ipaddress import IPv4Address , IPv4Network from typing import List from pytest import raises from apischema import ValidationError , deserialize , serialize , validator from apischema.fields import fields @dataclass class SubnetIps : subnet : IPv4Network ips : List [ IPv4Address ] @validator def check_ips_in_subnet ( self ): for index , ip in enumerate ( self . ips ): if ip not in self . subnet : # yield <error path>, <error message> yield ( fields ( self ) . ips , index ), \"ip not in subnet\" with raises ( ValidationError ) as err : deserialize ( SubnetIps , { \"subnet\" : \"126.42.18.0/24\" , \"ips\" : [ \"126.42.18.1\" , \"126.42.19.0\" , \"0.0.0.0\" ]}, ) assert serialize ( err . value ) == [ { \"loc\" : [ \"ips\" , 1 ], \"err\" : [ \"ip not in subnet\" ]}, { \"loc\" : [ \"ips\" , 2 ], \"err\" : [ \"ip not in subnet\" ]}, ]","title":"Raise more than one error with yield"},{"location":"validation/#error-path","text":"In the example, validator yield a tuple of an \"error path\" and the error message. Error path can be: a string an integer (for list indices) a dataclass field (obtained with get_fields ) a tuple of this 3 components. yield can also be used with only an error message. Note For dataclass field error path, it's advised to use get_fields instead of raw string, because it will take in account potential aliasing and it will be easier to rename field with IDE refactoring.","title":"Error path"},{"location":"validation/#discard","text":"If one of your validators fails because a field is corrupted, maybe you don't want following validators to be executed. validator decorator provides a discard parameter to discard fields of the remaining validation. All the remaining validators having discarded fields in dependencies will not be executed. from dataclasses import dataclass , field from typing import List , Tuple from pytest import raises from apischema import ValidationError , deserialize , serialize , validator from apischema.fields import fields @dataclass class BoundedValues : bounds : Tuple [ int , int ] = field () values : List [ int ] @validator ( discard = bounds ) def bounds_are_sorted ( self ): min_bound , max_bound = self . bounds if min_bound > max_bound : yield fields ( self ) . bounds , \"bounds are not sorted\" @validator def values_dont_exceed_bounds ( self ): min_bound , max_bound = self . bounds for index , value in enumerate ( self . values ): if not min_bound <= value <= max_bound : yield ( fields ( self ) . values , index ), \"value exceeds bounds\" with raises ( ValidationError ) as err : deserialize ( BoundedValues , { \"bounds\" : [ 10 , 0 ], \"values\" : [ - 1 , 2 , 4 ]}) assert serialize ( err . value ) == [ { \"loc\" : [ \"bounds\" ], \"err\" : [ \"bounds are not sorted\" ]} # Without discard, there would have been an other error # {\"loc\": [\"values\", 1], \"err\": [\"value exceeds bounds\"]} ]","title":"Discard"},{"location":"validation/#field-validators","text":"","title":"Field validators"},{"location":"validation/#at-field-level","text":"Fields are validated according to their types and schema. But it's also possible to add validators to fields from dataclasses import dataclass , field from pytest import raises from apischema import ValidationError , deserialize , serialize from apischema.metadata import validators def check_no_duplicate_digits ( n : int ): if len ( str ( n )) != len ( set ( str ( n ))): raise ValueError ( \"number has duplicate digits\" ) @dataclass class Foo : bar : str = field ( metadata = validators ( check_no_duplicate_digits )) with raises ( ValidationError ) as err : deserialize ( Foo , { \"bar\" : \"11\" }) assert serialize ( err . value ) == [ { \"loc\" : [ \"bar\" ], \"err\" : [ \"number has duplicate digits\" ]} ] When validation fails for a field, it is discarded and cannot be used in class validators, as it is the case when field schema validation fails. Note field_validator allow to reuse the the same validator for several fields. However in this case, using a custom type (for example a NewType ) with validators (see next section ) could often be a better solution.","title":"At field level"},{"location":"validation/#using-other-fields","text":"A common pattern can be to validate a field using other fields values. This is achieved with dataclass validators seen above. However there is a shortcut for this use case: from dataclasses import dataclass , field from enum import Enum from pytest import raises from apischema import ValidationError , deserialize , serialize , validator from apischema.fields import fields class Parity ( Enum ): EVEN = \"even\" ODD = \"odd\" @dataclass class NumberWithParity : parity : Parity number : int = field () # field must be assign, even with empty `field()` @validator ( number ) def check_parity ( self ): if ( self . parity == Parity . EVEN ) != ( self . number % 2 == 0 ): yield \"number doesn't respect parity\" # using field argument adds automatically discard argument # and prefix all error paths with the field @validator ( discard = number ) def check_parity_equivalent ( self ): if ( self . parity == Parity . EVEN ) != ( self . number % 2 == 0 ): yield fields ( self ) . number , \"number doesn't respect parity\" with raises ( ValidationError ) as err : deserialize ( NumberWithParity , { \"parity\" : \"even\" , \"number\" : 1 }) assert serialize ( err . value ) == [ { \"loc\" : [ \"number\" ], \"err\" : [ \"number doesn't respect parity\" ]} ]","title":"Using other fields"},{"location":"validation/#validators-inheritance","text":"Validators are inherited just like other class fields. from dataclasses import dataclass from pytest import raises from apischema import ValidationError , deserialize , serialize , validator @dataclass class PasswordForm : password : str confirmation : str @validator def password_match ( self ): if self . password != self . confirmation : raise ValueError ( \"password doesn't match its confirmation\" ) @dataclass class CompleteForm ( PasswordForm ): username : str with raises ( ValidationError ) as err : deserialize ( CompleteForm , { \"username\" : \"wyfo\" , \"password\" : \"p455w0rd\" , \"confirmation\" : \"...\" }, ) assert serialize ( err . value ) == [ { \"loc\" : [], \"err\" : [ \"password doesn't match its confirmation\" ]} ]","title":"Validators inheritance"},{"location":"validation/#validator-with-initvar","text":"Dataclasses InitVar are accessible in validators by using parameters the same way __post_init__ does. Only the needed fields has to be put in parameters, they are then added to validator dependencies. from dataclasses import InitVar , dataclass , field from pytest import raises from apischema import ValidationError , deserialize , serialize , validator from apischema.metadata import init_var @dataclass class Foo : bar : InitVar [ int ] = field ( metadata = init_var ( int )) @validator ( bar ) def validate ( self , bar : int ): if bar < 0 : yield \"negative\" with raises ( ValidationError ) as err : deserialize ( Foo , { \"bar\" : - 1 }) assert serialize ( err . value ) == [{ \"loc\" : [ \"bar\" ], \"err\" : [ \"negative\" ]}]","title":"Validator with InitVar"},{"location":"validation/#validators-are-not-run-on-default-values","text":"If all validator dependencies are initialized with their defau lt values, they are not run; make sure your default values make sens. from dataclasses import dataclass , field from apischema import deserialize , validator validator_run = False @dataclass class Foo : bar : int = field ( default = 0 ) @validator ( bar ) def password_match ( self ): global validator_run validator_run = True if self . bar < 0 : raise ValueError ( \"negative\" ) deserialize ( Foo , {}) assert not validator_run","title":"Validators are not run on default values"},{"location":"validation/#validators-for-every-type","text":"Validators can be added to other user-defined types. When a user type is deseriarialized (even in case of conversion ), its validators are played. from typing import NewType from pytest import raises from apischema import ValidationError , deserialize , serialize from apischema.validation import add_validator Palindrome = NewType ( \"Palindrome\" , str ) @add_validator ( Palindrome ) def check_palindrome ( s : str ): for i in range ( len ( s ) // 2 ): if s [ i ] != s [ - 1 - i ]: raise ValueError ( \"Not a palindrome\" ) assert deserialize ( Palindrome , \"tacocat\" ) == \"tacocat\" with raises ( ValidationError ) as err : deserialize ( Palindrome , \"not a palindrome\" ) assert serialize ( err . value ) == [{ \"loc\" : [], \"err\" : [ \"Not a palindrome\" ]}]","title":"Validators for every type"},{"location":"validation/#type-checker","text":"Validation is done only at deserialization (see FAQ ). However, it is possible at runtime to check the type of an object, and to do it descending all the type model of the given types (dataclass fields, list elements, etc.) with the function check_types . It can also \"validate\" the object (running all its validator and validate its constraints) if needed. from dataclasses import dataclass , field from typing import AbstractSet from uuid import UUID , uuid4 from pytest import raises from apischema import ValidationError , check_types , schema , serialize @dataclass class Resource : id : UUID tags : AbstractSet [ str ] = field ( metadata = schema ( max_items = 3 )) check_types ( Resource , Resource ( uuid4 (), { \"tag\" })) # no error with raises ( ValidationError ) as err : check_types ( Resource , Resource ( \"id\" , { 0 })) # type: ignore assert serialize ( err . value ) == [ { \"loc\" : [ \"id\" ], \"err\" : [ \"expected UUID, found type str\" ]}, { \"loc\" : [ \"tags\" , 0 ], \"err\" : [ \"expected str, found type int\" ]}, ] too_many_tags = Resource ( uuid4 (), { \"0\" , \"1\" , \"2\" , \"3\" }) assert check_types ( Resource , too_many_tags ) # No error because no validation with raises ( ValidationError ) as err : check_types ( Resource , too_many_tags , validate = True ) # type: ignore assert serialize ( err . value ) == [ { \"loc\" : [ \"tags\" ], \"err\" : [ \"size greater than 3 (maxItems)\" ]} ]","title":"Type checker"},{"location":"validation/#faq","text":"","title":"FAQ"},{"location":"validation/#how-are-computed-validator-depedencies","text":"ast.NodeVisitor and the Python black magic begins...","title":"How are computed validator depedencies?"},{"location":"validation/#why-only-validate-at-deserialization-and-not-at-instantiation","text":"Dataclass are typed-checked, so data put in the constructor are supposed to be typed-checked too, so validation would be useless most of the time. That's why it doesn't seem justified to add this high overhead everywhere for everyone. By the way, if it was implemented, in order to be consistent, validation should be performed each time the instance is modified (if you don't trust __init__ argument, why would it be different for __setattr__ ) and it would add even more overhead. Other libraries exist to ensure preservation of class invariants.","title":"Why only validate at deserialization and not at instantiation?"},{"location":"validation/#why-use-validators-for-dataclasses-instead-of-doing-validation-in-__post_init__","text":"Actually, validation can completly be done in __post_init__ , there is not problem with that. However, validators offers one thing that cannot be achieved with __post_init__ : they are run before __init__ , so they can validate incomplete data. Moreover, they are only run during deserialization, so they don't add overhead to normal class instantiation (yes, that's a minor argument). Of course, there is also features like dependencies computing, discard ... (which could be achieved in __post_init__ but require some lines of code).","title":"Why use validators for dataclasses instead of doing validation in __post_init__?"},{"location":"examples/open_rpc/","text":"OpenRPC \u00b6 from dataclasses import dataclass from typing import Any , Generic , List , TypeVar , Union from pytest import raises from typing_extensions import Annotated from apischema import NotNull , Skip , ValidationError , deserialize , schema , serialize from apischema.fields import with_fields_set from apischema.json_schema import deserialization_schema class NoResult : pass NO_RESULT = NoResult () NO_DATA = object () T = TypeVar ( \"T\" ) @with_fields_set @dataclass class Error ( Exception , Generic [ T ]): code : int description : str data : Any = NO_DATA @schema ( min_props = 1 , max_props = 1 ) @with_fields_set @dataclass class Result ( Generic [ T ]): result : Union [ T , Annotated [ NoResult , Skip ]] = NO_RESULT error : NotNull [ Error ] = None def get ( self ) -> T : if self . error is not None : raise self . error assert not isinstance ( self . result , NoResult ) return self . result assert deserialization_schema ( Result [ List [ int ]]) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"additionalProperties\" : False , \"maxProperties\" : 1 , \"minProperties\" : 1 , \"properties\" : { \"result\" : { \"type\" : \"array\" , \"items\" : { \"type\" : \"integer\" }}, \"error\" : { \"additionalProperties\" : False , \"properties\" : { \"code\" : { \"type\" : \"integer\" }, \"description\" : { \"type\" : \"string\" }, \"data\" : {}, }, \"required\" : [ \"code\" , \"description\" ], \"type\" : \"object\" , }, }, \"type\" : \"object\" , } data = { \"result\" : 0 } with raises ( ValidationError ): deserialize ( Result [ str ], data ) result = deserialize ( Result [ int ], data ) assert result == Result ( 0 ) assert result . get () == 0 assert serialize ( result ) == { \"result\" : 0 } error = deserialize ( Result , { \"error\" : { \"code\" : 42 , \"description\" : \"...\" }}) with raises ( Error ) as err : error . get () assert err . value == Error ( 42 , \"...\" )","title":"OpenRPC"},{"location":"examples/open_rpc/#openrpc","text":"from dataclasses import dataclass from typing import Any , Generic , List , TypeVar , Union from pytest import raises from typing_extensions import Annotated from apischema import NotNull , Skip , ValidationError , deserialize , schema , serialize from apischema.fields import with_fields_set from apischema.json_schema import deserialization_schema class NoResult : pass NO_RESULT = NoResult () NO_DATA = object () T = TypeVar ( \"T\" ) @with_fields_set @dataclass class Error ( Exception , Generic [ T ]): code : int description : str data : Any = NO_DATA @schema ( min_props = 1 , max_props = 1 ) @with_fields_set @dataclass class Result ( Generic [ T ]): result : Union [ T , Annotated [ NoResult , Skip ]] = NO_RESULT error : NotNull [ Error ] = None def get ( self ) -> T : if self . error is not None : raise self . error assert not isinstance ( self . result , NoResult ) return self . result assert deserialization_schema ( Result [ List [ int ]]) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"additionalProperties\" : False , \"maxProperties\" : 1 , \"minProperties\" : 1 , \"properties\" : { \"result\" : { \"type\" : \"array\" , \"items\" : { \"type\" : \"integer\" }}, \"error\" : { \"additionalProperties\" : False , \"properties\" : { \"code\" : { \"type\" : \"integer\" }, \"description\" : { \"type\" : \"string\" }, \"data\" : {}, }, \"required\" : [ \"code\" , \"description\" ], \"type\" : \"object\" , }, }, \"type\" : \"object\" , } data = { \"result\" : 0 } with raises ( ValidationError ): deserialize ( Result [ str ], data ) result = deserialize ( Result [ int ], data ) assert result == Result ( 0 ) assert result . get () == 0 assert serialize ( result ) == { \"result\" : 0 } error = deserialize ( Result , { \"error\" : { \"code\" : 42 , \"description\" : \"...\" }}) with raises ( Error ) as err : error . get () assert err . value == Error ( 42 , \"...\" )","title":"OpenRPC"},{"location":"examples/recoverable_fields/","text":"Recoverable fields \u00b6 Inspired by https://github.com/samuelcolvin/pydantic/issues/800 from typing import Any , Generic , TypeVar , Union from typing_extensions import Annotated from pytest import raises from apischema import Skip , deserialize , deserializer , serialize , serializer from apischema.json_schema import deserialization_schema , serialization_schema class RecoverableRaw ( Exception ): def __init__ ( self , raw ): self . raw = raw deserializer ( RecoverableRaw , Any , RecoverableRaw ) T = TypeVar ( \"T\" ) class Recoverable ( Generic [ T ]): def __init__ ( self , value : T ): self . _value = value @property def value ( self ) -> T : if isinstance ( self . _value , RecoverableRaw ): raise self . _value return self . _value @value . setter def value ( self , value : T ): self . _value = value deserializer ( Recoverable , Union [ T , Annotated [ RecoverableRaw , Skip ( schema_only = True )]], Recoverable [ T ], ) @serializer def serialize_recoverable ( recoverable : Recoverable [ T ]) -> T : return recoverable . value assert deserialize ( Recoverable [ int ], 0 ) . value == 0 with raises ( RecoverableRaw ) as err : assert deserialize ( Recoverable [ int ], \"bad\" ) . value assert err . value . raw == \"bad\" assert serialize ( Recoverable ( 0 )) == 0 with raises ( RecoverableRaw ) as err : assert serialize ( Recoverable ( RecoverableRaw ( \"bad\" ))) assert err . value . raw == \"bad\" assert ( deserialization_schema ( Recoverable [ int ]) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"type\" : \"integer\" } == serialization_schema ( Recoverable [ int ]) )","title":"Recoverable fields"},{"location":"examples/recoverable_fields/#recoverable-fields","text":"Inspired by https://github.com/samuelcolvin/pydantic/issues/800 from typing import Any , Generic , TypeVar , Union from typing_extensions import Annotated from pytest import raises from apischema import Skip , deserialize , deserializer , serialize , serializer from apischema.json_schema import deserialization_schema , serialization_schema class RecoverableRaw ( Exception ): def __init__ ( self , raw ): self . raw = raw deserializer ( RecoverableRaw , Any , RecoverableRaw ) T = TypeVar ( \"T\" ) class Recoverable ( Generic [ T ]): def __init__ ( self , value : T ): self . _value = value @property def value ( self ) -> T : if isinstance ( self . _value , RecoverableRaw ): raise self . _value return self . _value @value . setter def value ( self , value : T ): self . _value = value deserializer ( Recoverable , Union [ T , Annotated [ RecoverableRaw , Skip ( schema_only = True )]], Recoverable [ T ], ) @serializer def serialize_recoverable ( recoverable : Recoverable [ T ]) -> T : return recoverable . value assert deserialize ( Recoverable [ int ], 0 ) . value == 0 with raises ( RecoverableRaw ) as err : assert deserialize ( Recoverable [ int ], \"bad\" ) . value assert err . value . raw == \"bad\" assert serialize ( Recoverable ( 0 )) == 0 with raises ( RecoverableRaw ) as err : assert serialize ( Recoverable ( RecoverableRaw ( \"bad\" ))) assert err . value . raw == \"bad\" assert ( deserialization_schema ( Recoverable [ int ]) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"type\" : \"integer\" } == serialization_schema ( Recoverable [ int ]) )","title":"Recoverable fields"},{"location":"examples/sqlalchemy/","text":"SQLAlchemy quick support \u00b6 from dataclasses import field , make_dataclass from inspect import getmembers from sqlalchemy import Column , Integer from sqlalchemy.ext.declarative import as_declarative from apischema import deserialize , deserializer , serialize , serializer from apischema.fields import fields_set , with_fields_set @as_declarative () class Base : def __init_subclass__ ( cls ): columns = getmembers ( cls , lambda m : isinstance ( m , Column )) if not columns : return fields = [ ( col . name or f , col . type . python_type , field ( default = None )) for f , col in columns ] dataclass = make_dataclass ( cls . __name__ , fields ) with_fields_set ( dataclass ) def from_data ( data ): return cls ( ** { f : getattr ( data , f ) for f in fields_set ( data )}) def to_data ( obj ): return dataclass ( ** { f : getattr ( obj , f ) for f , _ in columns }) deserializer ( from_data , dataclass , cls ) serializer ( to_data , cls , dataclass ) class A ( Base ): __tablename__ = \"a\" key = Column ( Integer , primary_key = True ) a = deserialize ( A , { \"key\" : 0 }) assert isinstance ( a , A ) assert a . key == 0 assert serialize ( a ) == { \"key\" : 0 }","title":"SQLAlchemy"},{"location":"examples/sqlalchemy/#sqlalchemy-quick-support","text":"from dataclasses import field , make_dataclass from inspect import getmembers from sqlalchemy import Column , Integer from sqlalchemy.ext.declarative import as_declarative from apischema import deserialize , deserializer , serialize , serializer from apischema.fields import fields_set , with_fields_set @as_declarative () class Base : def __init_subclass__ ( cls ): columns = getmembers ( cls , lambda m : isinstance ( m , Column )) if not columns : return fields = [ ( col . name or f , col . type . python_type , field ( default = None )) for f , col in columns ] dataclass = make_dataclass ( cls . __name__ , fields ) with_fields_set ( dataclass ) def from_data ( data ): return cls ( ** { f : getattr ( data , f ) for f in fields_set ( data )}) def to_data ( obj ): return dataclass ( ** { f : getattr ( obj , f ) for f , _ in columns }) deserializer ( from_data , dataclass , cls ) serializer ( to_data , cls , dataclass ) class A ( Base ): __tablename__ = \"a\" key = Column ( Integer , primary_key = True ) a = deserialize ( A , { \"key\" : 0 }) assert isinstance ( a , A ) assert a . key == 0 assert serialize ( a ) == { \"key\" : 0 }","title":"SQLAlchemy quick support"}]}