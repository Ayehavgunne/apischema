{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Overview \u00b6 Apischema \u00b6 Makes your life easier when it comes to python API. JSON (de)serialization + schema generation through python typing, with some sugar and magic. Install \u00b6 As simple as pip install apischema It requires only Python 3.6+ (and dataclasses [official backport] for version 3.6 only) Why another library? \u00b6 This library fulfill the following goals: stay as close as possible to the standard library (dataclasses, typing, etc.) to be as accessible as possible \u2014 as a consequency do not need plugins for editor/linter/etc.; be additive and tunable, be able to work with user own types as well as foreign libraries ones; do not need a PR for handling new types like bson.ObjectId , avoid subclassing; avoid dynamic things like using string for attribute name. No known alternative achieves that. Example \u00b6 from dataclasses import dataclass , field from typing import List from uuid import UUID , uuid4 from pytest import raises from apischema import ValidationError , deserialize , serialize # Define a schema with standard dataclasses @dataclass class Resource : id : UUID name : str tags : List [ str ] = field ( default_factory = list ) # Get some data uuid = uuid4 () data = { \"id\" : str ( uuid ), \"name\" : \"wyfo\" , \"tags\" : [ \"some_tag\" ]} # Deserialize with `from_data` resource = deserialize ( Resource , data ) assert resource == Resource ( uuid , \"wyfo\" , [ \"some_tag\" ]) # Serialize with `to_data` assert serialize ( resource ) == data # Validate during deserialization with raises ( ValidationError ) as err : # pytest check exception is raised deserialize ( Resource , { \"id\" : \"42\" , \"name\" : \"wyfo\" }) assert serialize ( err . value ) == [ { \"loc\" : [ \"id\" ], \"err\" : [ \"badly formed hexadecimal UUID string\" ]} ] Apischema works out of the box with you data model. (This example and further ones are using pytest stuff because they are in fact run as tests in the library CI; that's very convenient) FAQ \u00b6 Which Python version does I need? \u00b6 Apischema needs generalized typing annotation syntax introduced in Python 3.6. Even if dataclasses were brought by 3.7 version, there is an official backport for 3.6 (and Apischema will install it). So Apischema supports all python versions from 3.6. I already have my data model with my SQLAlchemy /ORM tables, will I have to duplicate my code, making one dataclass by table? \u00b6 Why would you have to duplicate them? Apischema can \"work with user own types as well as foreign libraries ones\". Some teasing of conversion feature: you can add default serialization for all your tables, or register different serializer that you can select according to your API endpoint, or both. So SQLAlchemy is supported? Does it support others library? \u00b6 No, in fact, no library are supported, even SQLAlchemy ; it was a choice made to be as small and generic as possible, and to support only the standard library (with types like datetime , UUID ). However, the library is flexible enough to code yourself the support you need with, I hope, the minimal effort. It's of course not excluded to add support in additional small plugin libraries. Feedbacks are welcome about the best way to do things. I need more accurate validation than \"ensure this is an integer and no a string \", can I do that? \u00b6 See the validation section. You can use standard JSON-schema validation ( maxItems , pattern , etc.) that will be embedded in your schema or add custom Python validators for each class/fields/ NewType you want. Can it generate JSON-schema/ OpenAPI from code? \u00b6 This feature has not been put in the example to keep it light, but of course it can. By the way, only JSON-schema is supported (but OpenAPI will at last converge with JSON-schema in version 3.1). However the opposite, generate code from JSON-Schema is not yet supported. Let's start the Apischema tour.","title":"Overview"},{"location":"#overview","text":"","title":"Overview"},{"location":"#apischema","text":"Makes your life easier when it comes to python API. JSON (de)serialization + schema generation through python typing, with some sugar and magic.","title":"Apischema"},{"location":"#install","text":"As simple as pip install apischema It requires only Python 3.6+ (and dataclasses [official backport] for version 3.6 only)","title":"Install"},{"location":"#why-another-library","text":"This library fulfill the following goals: stay as close as possible to the standard library (dataclasses, typing, etc.) to be as accessible as possible \u2014 as a consequency do not need plugins for editor/linter/etc.; be additive and tunable, be able to work with user own types as well as foreign libraries ones; do not need a PR for handling new types like bson.ObjectId , avoid subclassing; avoid dynamic things like using string for attribute name. No known alternative achieves that.","title":"Why another library?"},{"location":"#example","text":"from dataclasses import dataclass , field from typing import List from uuid import UUID , uuid4 from pytest import raises from apischema import ValidationError , deserialize , serialize # Define a schema with standard dataclasses @dataclass class Resource : id : UUID name : str tags : List [ str ] = field ( default_factory = list ) # Get some data uuid = uuid4 () data = { \"id\" : str ( uuid ), \"name\" : \"wyfo\" , \"tags\" : [ \"some_tag\" ]} # Deserialize with `from_data` resource = deserialize ( Resource , data ) assert resource == Resource ( uuid , \"wyfo\" , [ \"some_tag\" ]) # Serialize with `to_data` assert serialize ( resource ) == data # Validate during deserialization with raises ( ValidationError ) as err : # pytest check exception is raised deserialize ( Resource , { \"id\" : \"42\" , \"name\" : \"wyfo\" }) assert serialize ( err . value ) == [ { \"loc\" : [ \"id\" ], \"err\" : [ \"badly formed hexadecimal UUID string\" ]} ] Apischema works out of the box with you data model. (This example and further ones are using pytest stuff because they are in fact run as tests in the library CI; that's very convenient)","title":"Example"},{"location":"#faq","text":"","title":"FAQ"},{"location":"#which-python-version-does-i-need","text":"Apischema needs generalized typing annotation syntax introduced in Python 3.6. Even if dataclasses were brought by 3.7 version, there is an official backport for 3.6 (and Apischema will install it). So Apischema supports all python versions from 3.6.","title":"Which Python version does I need?"},{"location":"#i-already-have-my-data-model-with-my-sqlalchemyorm-tables-will-i-have-to-duplicate-my-code-making-one-dataclass-by-table","text":"Why would you have to duplicate them? Apischema can \"work with user own types as well as foreign libraries ones\". Some teasing of conversion feature: you can add default serialization for all your tables, or register different serializer that you can select according to your API endpoint, or both.","title":"I already have my data model with my SQLAlchemy/ORM tables, will I have to duplicate my code, making one dataclass by table?"},{"location":"#so-sqlalchemy-is-supported-does-it-support-others-library","text":"No, in fact, no library are supported, even SQLAlchemy ; it was a choice made to be as small and generic as possible, and to support only the standard library (with types like datetime , UUID ). However, the library is flexible enough to code yourself the support you need with, I hope, the minimal effort. It's of course not excluded to add support in additional small plugin libraries. Feedbacks are welcome about the best way to do things.","title":"So SQLAlchemy is supported? Does it support others library?"},{"location":"#i-need-more-accurate-validation-than-ensure-this-is-an-integer-and-no-a-string-can-i-do-that","text":"See the validation section. You can use standard JSON-schema validation ( maxItems , pattern , etc.) that will be embedded in your schema or add custom Python validators for each class/fields/ NewType you want.","title":"I need more accurate validation than \"ensure this is an integer and no a string \", can I do that?"},{"location":"#can-it-generate-json-schemaopenapi-from-code","text":"This feature has not been put in the example to keep it light, but of course it can. By the way, only JSON-schema is supported (but OpenAPI will at last converge with JSON-schema in version 3.1). However the opposite, generate code from JSON-Schema is not yet supported. Let's start the Apischema tour.","title":"Can it generate JSON-schema/OpenAPI from code?"},{"location":"benchmark/","text":"Benchmark \u00b6 Benchmark presented is just Pydantic benchmark where Apischema has been inserted","title":"Benchmark"},{"location":"benchmark/#benchmark","text":"Benchmark presented is just Pydantic benchmark where Apischema has been inserted","title":"Benchmark"},{"location":"configuration_management/","text":"Configuration management \u00b6 Apischema is a multi-purpose deserialization library. If it's main use should be in API endpoint, that doesn't prevent to use it somewhere else, to parse application configuration for instance. For JSON/YAML configurations, there is no need to develop further the use of library. But there is many more ways of storing configurations, with environment variables or key-value store for example. When configurations are simple, there could be no need for a library like Apischema . However when it comes to being larger with some structure, it can play its cards right. Dot-seraparated key-value \u00b6 A common way of storing complex nested structures into flat key-value stores is to use dot-separated keys. Apischema provides an utility function to unflat this kind of data into nested JSON-like data, which can then be deserialized. from dataclasses import dataclass from ipaddress import IPv4Address , IPv4Network from typing import Collection from apischema import deserialize , unflat_key_value # Could be extracted from ini file, or environment variables with `os.environ` configuration = [ ( \"database.host\" , \"191.132.67.45\" ), ( \"database.user\" , \"guest\" ), ( \"filtered_subnets.0\" , \"247.252.191.00/24\" ), ( \"filtered_subnets.1\" , \"207.34.172.00/24\" ), ] @dataclass class Database : host : IPv4Address user : str @dataclass class Config : database : Database filtered_subnets : Collection [ IPv4Network ] # separator parameter is defaulted to \".\" but shown for example assert unflat_key_value ( configuration , separator = \".\" ) == { \"database\" : { \"host\" : \"191.132.67.45\" , \"user\" : \"guest\" }, \"filtered_subnets\" : [ \"247.252.191.00/24\" , \"207.34.172.00/24\" ], } assert deserialize ( Config , unflat_key_value ( configuration ), coerce = True , additional_properties = True ) == Config ( database = Database ( IPv4Address ( \"191.132.67.45\" ), \"guest\" ), filtered_subnets = ( IPv4Network ( \"247.252.191.00/24\" ), IPv4Network ( \"207.34.172.00/24\" ), ), )","title":"Configuration management"},{"location":"configuration_management/#configuration-management","text":"Apischema is a multi-purpose deserialization library. If it's main use should be in API endpoint, that doesn't prevent to use it somewhere else, to parse application configuration for instance. For JSON/YAML configurations, there is no need to develop further the use of library. But there is many more ways of storing configurations, with environment variables or key-value store for example. When configurations are simple, there could be no need for a library like Apischema . However when it comes to being larger with some structure, it can play its cards right.","title":"Configuration management"},{"location":"configuration_management/#dot-seraparated-key-value","text":"A common way of storing complex nested structures into flat key-value stores is to use dot-separated keys. Apischema provides an utility function to unflat this kind of data into nested JSON-like data, which can then be deserialized. from dataclasses import dataclass from ipaddress import IPv4Address , IPv4Network from typing import Collection from apischema import deserialize , unflat_key_value # Could be extracted from ini file, or environment variables with `os.environ` configuration = [ ( \"database.host\" , \"191.132.67.45\" ), ( \"database.user\" , \"guest\" ), ( \"filtered_subnets.0\" , \"247.252.191.00/24\" ), ( \"filtered_subnets.1\" , \"207.34.172.00/24\" ), ] @dataclass class Database : host : IPv4Address user : str @dataclass class Config : database : Database filtered_subnets : Collection [ IPv4Network ] # separator parameter is defaulted to \".\" but shown for example assert unflat_key_value ( configuration , separator = \".\" ) == { \"database\" : { \"host\" : \"191.132.67.45\" , \"user\" : \"guest\" }, \"filtered_subnets\" : [ \"247.252.191.00/24\" , \"207.34.172.00/24\" ], } assert deserialize ( Config , unflat_key_value ( configuration ), coerce = True , additional_properties = True ) == Config ( database = Database ( IPv4Address ( \"191.132.67.45\" ), \"guest\" ), filtered_subnets = ( IPv4Network ( \"247.252.191.00/24\" ), IPv4Network ( \"207.34.172.00/24\" ), ), )","title":"Dot-seraparated key-value"},{"location":"conversions/","text":"(De)serialization: Customize with conversions \u00b6","title":"(De)serialization: Customize with conversions"},{"location":"conversions/#deserialization-customize-with-conversions","text":"","title":"(De)serialization: Customize with conversions"},{"location":"data_model/","text":"Data model \u00b6 Apischema handle every classes/types you need. PEP 585 \u00b6 With Python 3.9 and PEP 585 , typing is substantially shaken up. Because Apischema must be up-to-date with modern Python, support of Python 3.9 covers PEP 585. But because 3.9 is not yet released and far from being widely adopted, the following documentation will use old fashioned Python typing with typing module. Dataclasses \u00b6 Because the library aims to bring the minimum boilerplate, it's build on the top of standard library. Dataclasses are thus the core structure of the data model. Dataclasses bring the possibility of field customization, with more than just a default value. In addition to the common parameters of dataclasses.field , customization is done with metadata parameter. With some teasing of features presented later: from dataclasses import dataclass , field from apischema import alias , schema from apischema.metadata import ignore_default @dataclass class Foo : bar : int = field ( default = 0 , metadata = alias ( \"foo_bar\" ) | schema ( title = \"foo! bar!\" , min = 0 , max = 42 ) | ignore_default () ) Note Field's metadata are just an ordinary dict ; Apischema provides some functions to enrich these metadata with it's own keys ( alias(\"foo_bar) is roughly equivalent to `{\"_apischema_alias\": \"foo_bar\"}) and use them when the time comes, but metadata are not reserved to Apischema and other keys can be added. Because PEP 584 is painfully missing before Python 3.9, Apischema metadata use their own subclass of dict just to add | operator for convenience. Standard library types \u00b6 Apischema handle natively most of the types provided by the standard library. They are sorted in the following categories: Primitive \u00b6 str , int , float , bool , None , subclasses of them They correspond to JSON primitive types. Collection \u00b6 typing.Collection typing.Sequence typing.Tuple typing.MutableSequence typing.List typing.AbstractSet typing.FrozenSet typing.Set They correspond to JSON array and are serialized to list . Some of them are abstract; deserialization will instantiate a concrete child class. For example typing.Sequence will be instantiated with tuple while typing.MutableSequence will be instantiated with list . Mapping \u00b6 typing.Mapping , typing.MutableMapping , typing.Dict . They correpond to JSON object and are serialized to dict . Enumeration \u00b6 enum.Enum subclasses, typing.Literal For Enum , this is the value and not the attribute name that is serialized Typing facilities \u00b6 typing.Optional / typing.Union ( Optional[T] is strictly equivalent to Union[T, None] ) Deserialization select the first matching alternative typing.Tuple Can be used as collection as well as true Tuple, like Tuple[str, int] typing.NewType Serialized according to its base type typing.TypedDict , typing.NamedTuple Kind of discount dataclass without field customization Any Untouched by deserialization Other standard library types \u00b6 datetime.datetime datetime.date datetime.time Supported only in 3.7+ with (from) isoformat Decimal With float (de)serialization ipaddress.IPv4Address ipaddress.IPv4Interface ipaddress.IPv4Network ipaddress.IPv6Address ipaddress.IPv6Interface ipaddress.IPv6Network pathlib.Path typing.Pattern / re.Pattern uuid.UUID With str (de)serialization Generic \u00b6 typing.Generic can be used out of the box like in the following example: from dataclasses import dataclass from typing import Generic , TypeVar from pytest import raises from apischema import ValidationError , deserialize T = TypeVar ( \"T\" ) @dataclass class Box ( Generic [ T ]): content : T shaken : bool = False assert deserialize ( Box [ str ], { \"content\" : \"void\" }) == Box ( \"void\" ) with raises ( ValidationError ): deserialize ( Box [ str ], { \"content\" : 42 }) Recursive types, string annotations and PEP 563 \u00b6 Recursive classes can be typed as they usually do, with or without PEP 563 . Here with string annotations: from dataclasses import dataclass from typing import Optional from apischema import deserialize @dataclass class Node : value : int child : Optional [ \"Node\" ] = None assert deserialize ( Node , { \"value\" : 0 , \"child\" : { \"value\" : 1 }}) == Node ( 0 , Node ( 1 )) Here with PEP 563 (requires 3.7+) from __future__ import annotations from dataclasses import dataclass from typing import Optional from apischema import deserialize @dataclass class Node : value : int child : Optional [ Node ] = None assert deserialize ( Node , { \"value\" : 0 , \"child\" : { \"value\" : 1 }}) == Node ( 0 , Node ( 1 )) Warning To resolve annotations, Apischema uses typing.get_type_hints ; this doesn't work really well when used on objects defined outside of global scope. Warning (minor) Currently, PEP 585 can have surprising behavior when used outside the box, see bpo-41370 Custom types / ORM \u00b6 See conversion in order to support every possible types in a few lines of code. FAQ \u00b6 Why bytes is not handled with other primitives? \u00b6 Because it's not primitive found in JSON data. It's use can differ according to user needs; some will use base64 data to represent binary, others could use another string representation. To not force anyone, handling is let to the user, but it can be done in a few lines of code (needs only 6loc for base64 handling, see conversions ) Why Iterable is not handled with other primitives? \u00b6 Iterable could be handled (actually, it was at the beginning), however, this doesn't really make sense from a data point of view. Iterable are computation objects, they can be infinite, etc. They don't correspond to a serialized data; Collection is way more appropriate in this context. Why collections are only deserialized from list (and not other iterables)? \u00b6 When you parse a JSON array, do you get something else than a list ? Handling of other types would make no sense. By the way, all the data model is pure Python types and code, so, the same way than a dataclass can be instantiated with its constructor in the code, an arbitrary collection can be instantiated from arbitrary thing in the code. Deserialization is meant to be applied concreteate (JSON-)serialized data.","title":"Data model"},{"location":"data_model/#data-model","text":"Apischema handle every classes/types you need.","title":"Data model"},{"location":"data_model/#pep-585","text":"With Python 3.9 and PEP 585 , typing is substantially shaken up. Because Apischema must be up-to-date with modern Python, support of Python 3.9 covers PEP 585. But because 3.9 is not yet released and far from being widely adopted, the following documentation will use old fashioned Python typing with typing module.","title":"PEP 585"},{"location":"data_model/#dataclasses","text":"Because the library aims to bring the minimum boilerplate, it's build on the top of standard library. Dataclasses are thus the core structure of the data model. Dataclasses bring the possibility of field customization, with more than just a default value. In addition to the common parameters of dataclasses.field , customization is done with metadata parameter. With some teasing of features presented later: from dataclasses import dataclass , field from apischema import alias , schema from apischema.metadata import ignore_default @dataclass class Foo : bar : int = field ( default = 0 , metadata = alias ( \"foo_bar\" ) | schema ( title = \"foo! bar!\" , min = 0 , max = 42 ) | ignore_default () ) Note Field's metadata are just an ordinary dict ; Apischema provides some functions to enrich these metadata with it's own keys ( alias(\"foo_bar) is roughly equivalent to `{\"_apischema_alias\": \"foo_bar\"}) and use them when the time comes, but metadata are not reserved to Apischema and other keys can be added. Because PEP 584 is painfully missing before Python 3.9, Apischema metadata use their own subclass of dict just to add | operator for convenience.","title":"Dataclasses"},{"location":"data_model/#standard-library-types","text":"Apischema handle natively most of the types provided by the standard library. They are sorted in the following categories:","title":"Standard library types"},{"location":"data_model/#primitive","text":"str , int , float , bool , None , subclasses of them They correspond to JSON primitive types.","title":"Primitive"},{"location":"data_model/#collection","text":"typing.Collection typing.Sequence typing.Tuple typing.MutableSequence typing.List typing.AbstractSet typing.FrozenSet typing.Set They correspond to JSON array and are serialized to list . Some of them are abstract; deserialization will instantiate a concrete child class. For example typing.Sequence will be instantiated with tuple while typing.MutableSequence will be instantiated with list .","title":"Collection"},{"location":"data_model/#mapping","text":"typing.Mapping , typing.MutableMapping , typing.Dict . They correpond to JSON object and are serialized to dict .","title":"Mapping"},{"location":"data_model/#enumeration","text":"enum.Enum subclasses, typing.Literal For Enum , this is the value and not the attribute name that is serialized","title":"Enumeration"},{"location":"data_model/#typing-facilities","text":"typing.Optional / typing.Union ( Optional[T] is strictly equivalent to Union[T, None] ) Deserialization select the first matching alternative typing.Tuple Can be used as collection as well as true Tuple, like Tuple[str, int] typing.NewType Serialized according to its base type typing.TypedDict , typing.NamedTuple Kind of discount dataclass without field customization Any Untouched by deserialization","title":"Typing facilities"},{"location":"data_model/#other-standard-library-types","text":"datetime.datetime datetime.date datetime.time Supported only in 3.7+ with (from) isoformat Decimal With float (de)serialization ipaddress.IPv4Address ipaddress.IPv4Interface ipaddress.IPv4Network ipaddress.IPv6Address ipaddress.IPv6Interface ipaddress.IPv6Network pathlib.Path typing.Pattern / re.Pattern uuid.UUID With str (de)serialization","title":"Other standard library types"},{"location":"data_model/#generic","text":"typing.Generic can be used out of the box like in the following example: from dataclasses import dataclass from typing import Generic , TypeVar from pytest import raises from apischema import ValidationError , deserialize T = TypeVar ( \"T\" ) @dataclass class Box ( Generic [ T ]): content : T shaken : bool = False assert deserialize ( Box [ str ], { \"content\" : \"void\" }) == Box ( \"void\" ) with raises ( ValidationError ): deserialize ( Box [ str ], { \"content\" : 42 })","title":"Generic"},{"location":"data_model/#recursive-types-string-annotations-and-pep-563","text":"Recursive classes can be typed as they usually do, with or without PEP 563 . Here with string annotations: from dataclasses import dataclass from typing import Optional from apischema import deserialize @dataclass class Node : value : int child : Optional [ \"Node\" ] = None assert deserialize ( Node , { \"value\" : 0 , \"child\" : { \"value\" : 1 }}) == Node ( 0 , Node ( 1 )) Here with PEP 563 (requires 3.7+) from __future__ import annotations from dataclasses import dataclass from typing import Optional from apischema import deserialize @dataclass class Node : value : int child : Optional [ Node ] = None assert deserialize ( Node , { \"value\" : 0 , \"child\" : { \"value\" : 1 }}) == Node ( 0 , Node ( 1 )) Warning To resolve annotations, Apischema uses typing.get_type_hints ; this doesn't work really well when used on objects defined outside of global scope. Warning (minor) Currently, PEP 585 can have surprising behavior when used outside the box, see bpo-41370","title":"Recursive types, string annotations and PEP 563"},{"location":"data_model/#custom-types-orm","text":"See conversion in order to support every possible types in a few lines of code.","title":"Custom types / ORM"},{"location":"data_model/#faq","text":"","title":"FAQ"},{"location":"data_model/#why-bytes-is-not-handled-with-other-primitives","text":"Because it's not primitive found in JSON data. It's use can differ according to user needs; some will use base64 data to represent binary, others could use another string representation. To not force anyone, handling is let to the user, but it can be done in a few lines of code (needs only 6loc for base64 handling, see conversions )","title":"Why bytes is not handled with other primitives?"},{"location":"data_model/#why-iterable-is-not-handled-with-other-primitives","text":"Iterable could be handled (actually, it was at the beginning), however, this doesn't really make sense from a data point of view. Iterable are computation objects, they can be infinite, etc. They don't correspond to a serialized data; Collection is way more appropriate in this context.","title":"Why Iterable is not handled with other primitives?"},{"location":"data_model/#why-collections-are-only-deserialized-from-list-and-not-other-iterables","text":"When you parse a JSON array, do you get something else than a list ? Handling of other types would make no sense. By the way, all the data model is pure Python types and code, so, the same way than a dataclass can be instantiated with its constructor in the code, an arbitrary collection can be instantiated from arbitrary thing in the code. Deserialization is meant to be applied concreteate (JSON-)serialized data.","title":"Why collections are only deserialized from list (and not other iterables)?"},{"location":"de_serialization/","text":"(De)serialization: the basics \u00b6 Apischema aims to help API with deserialization/serialization of data, mostly JSON. Let start again with the overview example from dataclasses import dataclass , field from typing import List from uuid import UUID , uuid4 from pytest import raises from apischema import ValidationError , deserialize , serialize # Define a schema with standard dataclasses @dataclass class Resource : id : UUID name : str tags : List [ str ] = field ( default_factory = list ) # Get some data uuid = uuid4 () data = { \"id\" : str ( uuid ), \"name\" : \"wyfo\" , \"tags\" : [ \"some_tag\" ]} # Deserialize with `from_data` resource = deserialize ( Resource , data ) assert resource == Resource ( uuid , \"wyfo\" , [ \"some_tag\" ]) # Serialize with `to_data` assert serialize ( resource ) == data # Validate during deserialization with raises ( ValidationError ) as err : # pytest check exception is raised deserialize ( Resource , { \"id\" : \"42\" , \"name\" : \"wyfo\" }) assert serialize ( err . value ) == [ { \"loc\" : [ \"id\" ], \"err\" : [ \"badly formed hexadecimal UUID string\" ]} ] Deserialization \u00b6 Deserialization is done through the function apischema.deserialize with the following simplified signature: def deserialize ( cls : Type [ T ], data : Any ) -> T : ... cls can be a dataclass as well as a list[int] a NewType , or whatever you want (see conversions to extend deserialization support to every type you want). data must be a JSON-like serialized data: dict / list / str / int / float / bool / None , in short, what you get when you execute json.loads . Deserialization performs a validation of data, based on typing annotations and other information (see schema and validation ). from dataclasses import dataclass from typing import Collection, Mapping, NewType from apischema import deserialize @dataclass class Foo: bar: str MyInt = NewType(\"MyInt\", int) assert deserialize(Foo, {\"bar\": \"bar\"}) == Foo(\"bar\") assert deserialize(MyInt, 0) == MyInt(0) == 0 assert deserialize(Mapping[str, Collection[Foo]], {\"key\": [{\"bar\": \"42\"}]}) == { \"key\": (Foo(\"42\"),) } Strictness \u00b6 Coercion \u00b6 Apischema is strict by default. You ask for an integer, you have to receive an integer. However, in some cases, particularly concerning stringified configuration (see Configuration management ), data must be coerced. That can be done using coerce parameter; when set to True , all primitive types will be coerce to the expected type of the data model like the following: from pytest import raises from apischema import ValidationError , deserialize with raises ( ValidationError ): deserialize ( bool , \"ok\" ) assert deserialize ( bool , \"ok\" , coerce = True ) bool can be coerced from str with the following case-insensitive mapping: False True 0 1 f t n y no yes false true off on ko ok Note bool coercion from str is just a global dict[str, bool] named apischema.data.coercion.STR_TO_BOOL and it can be customized according to your need (but keys have to be lower cased). There is also a global set[str] named apischema.data.coercion.STR_NONE_VALUES for None coercion. Additional properties \u00b6 Apischema is strict too about number of fields received for an object . In JSON-schema terms, Apischema put \"additionalProperties\": false by default (this can be configured by class with properties field ). This behavior can be controlled by additional_properties parameter. When set to True , it prevents the reject of unexpected properties. from __future__ import annotations from dataclasses import dataclass from pytest import raises from apischema import ValidationError , deserialize @dataclass class Foo : bar : str data = { \"bar\" : \"bar\" , \"other\" : 42 } with raises ( ValidationError ): deserialize ( Foo , data ) assert deserialize ( Foo , data , additional_properties = True ) == Foo ( \"bar\" ) Default fallback \u00b6 Validation error can happen when deserializing an ill-formed field. However, if this field has a default value/factory, deserialization can fallback to this default; this is enabled by default_fallback parameter. from dataclasses import dataclass from pytest import raises from apischema import ValidationError , deserialize @dataclass class Foo : bar : str = \"bar\" data = { \"bar\" : 42 } with raises ( ValidationError ): deserialize ( Foo , data ) assert deserialize ( Foo , data , default_fallback = True ) == Foo ( \"bar\" ) Strictness configuration \u00b6 Default strictness of the library can be configured with the function apischema.deserialization.set_strictness defined as following: def set_strictness ( * , additional_properties : bool = ... , coerce : bool = ... , default_fallback : bool = ... , ): ... Fields set \u00b6 Sometimes, it can be useful to know which field has been set by the deserialization, for example inthe case of a PATCH requests, to know which field has been updated. In fact, for nullable fields which are typed Optional , a default None value could mean that the field was not present in payload or that the field was present and must be set to null . Because Apischema use vanilla dataclasses, this feature is not enabled by default and must be set explicitly on a per-class basis. Apischema provides a simple API to get/set this metadata. from dataclasses import dataclass from typing import Optional from apischema import deserialize from apischema.fields import ( get_fields_set , mark_set_fields , unmark_set_fields , with_fields_set , ) # This decorator enable the feature @with_fields_set @dataclass class Foo : bar : int baz : Optional [ str ] = None # Retrieve fields set foo1 = Foo ( 0 , None ) assert get_fields_set ( foo1 ) == { \"bar\" , \"baz\" } foo2 = Foo ( 0 ) assert get_fields_set ( foo2 ) == { \"bar\" } # Mark/unmark fields as set mark_set_fields ( foo2 , \"baz\" ) assert get_fields_set ( foo2 ) == { \"bar\" , \"baz\" } unmark_set_fields ( foo2 , \"baz\" ) assert get_fields_set ( foo2 ) == { \"bar\" } mark_set_fields ( foo2 , \"baz\" , overwrite = True ) assert get_fields_set ( foo2 ) == { \"baz\" } # Fields modification are taken in account foo2 . bar = 0 assert get_fields_set ( foo2 ) == { \"bar\" , \"baz\" } # Because deserialization use normal constructor, it works with the feature foo3 = deserialize ( Foo , { \"bar\" : 0 }) assert get_fields_set ( foo3 ) == { \"bar\" } Warning with_fields_set decorator MUST be put before dataclass . This is because both of them modify __init__ method, but only the first is built to take the second in account. Serialization \u00b6 Serialization is simpler than deserialization; serialize(obj) will generate a JSON-like serialized obj . There is no validation, objects provided are trusted \u2014 they are supposed to be statically type-checked. When there from dataclasses import dataclass from apischema import serialize @dataclass class Foo : bar : str assert serialize ( Foo ( \"bar\" )) == { \"bar\" : \"bar\" } assert serialize (( 0 , 1 )) == [ 0 , 1 ] assert serialize ({ \"key\" : ( \"value\" , 42 )}) == { \"key\" : [ \"value\" , 42 ]} Exclude unset fields \u00b6 When a class has a lot of optional fields, it can be convenient to not include all of them, to avoid a bunch of useless fields in your serialized data. Using the previous feature of fields set tracking , serialize can exclude unset fields using its exclude_unset parameter; this parameter is defaulted to True . from dataclasses import dataclass from typing import Optional from apischema import serialize from apischema.fields import with_fields_set # Decorator needed to benefit from the feature @with_fields_set @dataclass class Foo : bar : int baz : Optional [ str ] = None assert serialize ( Foo ( 0 )) == { \"bar\" : 0 } assert serialize ( Foo ( 0 ), exclude_unset = False ) == { \"bar\" : 0 , \"baz\" : None } Note As written in comment in the example, with_fields_set is necessary to benefit from the feature. If the dataclass don't use it, the feature will have no effect. Unsupported \u00b6 When a type is not supported by deserialization, an Unsupported exception is raised. See section about conversions from pytest import raises from apischema import Unsupported , deserialize , serialize class Foo : pass with raises ( Unsupported ): deserialize ( Foo , {}) with raises ( Unsupported ): serialize ( Foo ()) FAQ \u00b6 Why coercion is not default behavior? \u00b6 Because ill-formed data can be symptomatic of problems, and it has been decided that highlighting them would be better than hiding them. By the way, this is easily globally configurable. Why with_fields_set feature is not enable by default? \u00b6 It's true that this feature has the little cost of adding a decorator everywhere. However, keeping dataclass decorator allows IDEs/linters/type checkers/etc. to handle the class as such, so there is no need to develop a plugin for them. Standard compliance can be worth the additional decorator. (And little overhead can be avoided when not useful)","title":"(De)serialization: The basics"},{"location":"de_serialization/#deserialization-the-basics","text":"Apischema aims to help API with deserialization/serialization of data, mostly JSON. Let start again with the overview example from dataclasses import dataclass , field from typing import List from uuid import UUID , uuid4 from pytest import raises from apischema import ValidationError , deserialize , serialize # Define a schema with standard dataclasses @dataclass class Resource : id : UUID name : str tags : List [ str ] = field ( default_factory = list ) # Get some data uuid = uuid4 () data = { \"id\" : str ( uuid ), \"name\" : \"wyfo\" , \"tags\" : [ \"some_tag\" ]} # Deserialize with `from_data` resource = deserialize ( Resource , data ) assert resource == Resource ( uuid , \"wyfo\" , [ \"some_tag\" ]) # Serialize with `to_data` assert serialize ( resource ) == data # Validate during deserialization with raises ( ValidationError ) as err : # pytest check exception is raised deserialize ( Resource , { \"id\" : \"42\" , \"name\" : \"wyfo\" }) assert serialize ( err . value ) == [ { \"loc\" : [ \"id\" ], \"err\" : [ \"badly formed hexadecimal UUID string\" ]} ]","title":"(De)serialization: the basics"},{"location":"de_serialization/#deserialization","text":"Deserialization is done through the function apischema.deserialize with the following simplified signature: def deserialize ( cls : Type [ T ], data : Any ) -> T : ... cls can be a dataclass as well as a list[int] a NewType , or whatever you want (see conversions to extend deserialization support to every type you want). data must be a JSON-like serialized data: dict / list / str / int / float / bool / None , in short, what you get when you execute json.loads . Deserialization performs a validation of data, based on typing annotations and other information (see schema and validation ). from dataclasses import dataclass from typing import Collection, Mapping, NewType from apischema import deserialize @dataclass class Foo: bar: str MyInt = NewType(\"MyInt\", int) assert deserialize(Foo, {\"bar\": \"bar\"}) == Foo(\"bar\") assert deserialize(MyInt, 0) == MyInt(0) == 0 assert deserialize(Mapping[str, Collection[Foo]], {\"key\": [{\"bar\": \"42\"}]}) == { \"key\": (Foo(\"42\"),) }","title":"Deserialization"},{"location":"de_serialization/#strictness","text":"","title":"Strictness"},{"location":"de_serialization/#coercion","text":"Apischema is strict by default. You ask for an integer, you have to receive an integer. However, in some cases, particularly concerning stringified configuration (see Configuration management ), data must be coerced. That can be done using coerce parameter; when set to True , all primitive types will be coerce to the expected type of the data model like the following: from pytest import raises from apischema import ValidationError , deserialize with raises ( ValidationError ): deserialize ( bool , \"ok\" ) assert deserialize ( bool , \"ok\" , coerce = True ) bool can be coerced from str with the following case-insensitive mapping: False True 0 1 f t n y no yes false true off on ko ok Note bool coercion from str is just a global dict[str, bool] named apischema.data.coercion.STR_TO_BOOL and it can be customized according to your need (but keys have to be lower cased). There is also a global set[str] named apischema.data.coercion.STR_NONE_VALUES for None coercion.","title":"Coercion"},{"location":"de_serialization/#additional-properties","text":"Apischema is strict too about number of fields received for an object . In JSON-schema terms, Apischema put \"additionalProperties\": false by default (this can be configured by class with properties field ). This behavior can be controlled by additional_properties parameter. When set to True , it prevents the reject of unexpected properties. from __future__ import annotations from dataclasses import dataclass from pytest import raises from apischema import ValidationError , deserialize @dataclass class Foo : bar : str data = { \"bar\" : \"bar\" , \"other\" : 42 } with raises ( ValidationError ): deserialize ( Foo , data ) assert deserialize ( Foo , data , additional_properties = True ) == Foo ( \"bar\" )","title":"Additional properties"},{"location":"de_serialization/#default-fallback","text":"Validation error can happen when deserializing an ill-formed field. However, if this field has a default value/factory, deserialization can fallback to this default; this is enabled by default_fallback parameter. from dataclasses import dataclass from pytest import raises from apischema import ValidationError , deserialize @dataclass class Foo : bar : str = \"bar\" data = { \"bar\" : 42 } with raises ( ValidationError ): deserialize ( Foo , data ) assert deserialize ( Foo , data , default_fallback = True ) == Foo ( \"bar\" )","title":"Default fallback"},{"location":"de_serialization/#strictness-configuration","text":"Default strictness of the library can be configured with the function apischema.deserialization.set_strictness defined as following: def set_strictness ( * , additional_properties : bool = ... , coerce : bool = ... , default_fallback : bool = ... , ): ...","title":"Strictness configuration"},{"location":"de_serialization/#fields-set","text":"Sometimes, it can be useful to know which field has been set by the deserialization, for example inthe case of a PATCH requests, to know which field has been updated. In fact, for nullable fields which are typed Optional , a default None value could mean that the field was not present in payload or that the field was present and must be set to null . Because Apischema use vanilla dataclasses, this feature is not enabled by default and must be set explicitly on a per-class basis. Apischema provides a simple API to get/set this metadata. from dataclasses import dataclass from typing import Optional from apischema import deserialize from apischema.fields import ( get_fields_set , mark_set_fields , unmark_set_fields , with_fields_set , ) # This decorator enable the feature @with_fields_set @dataclass class Foo : bar : int baz : Optional [ str ] = None # Retrieve fields set foo1 = Foo ( 0 , None ) assert get_fields_set ( foo1 ) == { \"bar\" , \"baz\" } foo2 = Foo ( 0 ) assert get_fields_set ( foo2 ) == { \"bar\" } # Mark/unmark fields as set mark_set_fields ( foo2 , \"baz\" ) assert get_fields_set ( foo2 ) == { \"bar\" , \"baz\" } unmark_set_fields ( foo2 , \"baz\" ) assert get_fields_set ( foo2 ) == { \"bar\" } mark_set_fields ( foo2 , \"baz\" , overwrite = True ) assert get_fields_set ( foo2 ) == { \"baz\" } # Fields modification are taken in account foo2 . bar = 0 assert get_fields_set ( foo2 ) == { \"bar\" , \"baz\" } # Because deserialization use normal constructor, it works with the feature foo3 = deserialize ( Foo , { \"bar\" : 0 }) assert get_fields_set ( foo3 ) == { \"bar\" } Warning with_fields_set decorator MUST be put before dataclass . This is because both of them modify __init__ method, but only the first is built to take the second in account.","title":"Fields set"},{"location":"de_serialization/#serialization","text":"Serialization is simpler than deserialization; serialize(obj) will generate a JSON-like serialized obj . There is no validation, objects provided are trusted \u2014 they are supposed to be statically type-checked. When there from dataclasses import dataclass from apischema import serialize @dataclass class Foo : bar : str assert serialize ( Foo ( \"bar\" )) == { \"bar\" : \"bar\" } assert serialize (( 0 , 1 )) == [ 0 , 1 ] assert serialize ({ \"key\" : ( \"value\" , 42 )}) == { \"key\" : [ \"value\" , 42 ]}","title":"Serialization"},{"location":"de_serialization/#exclude-unset-fields","text":"When a class has a lot of optional fields, it can be convenient to not include all of them, to avoid a bunch of useless fields in your serialized data. Using the previous feature of fields set tracking , serialize can exclude unset fields using its exclude_unset parameter; this parameter is defaulted to True . from dataclasses import dataclass from typing import Optional from apischema import serialize from apischema.fields import with_fields_set # Decorator needed to benefit from the feature @with_fields_set @dataclass class Foo : bar : int baz : Optional [ str ] = None assert serialize ( Foo ( 0 )) == { \"bar\" : 0 } assert serialize ( Foo ( 0 ), exclude_unset = False ) == { \"bar\" : 0 , \"baz\" : None } Note As written in comment in the example, with_fields_set is necessary to benefit from the feature. If the dataclass don't use it, the feature will have no effect.","title":"Exclude unset fields"},{"location":"de_serialization/#unsupported","text":"When a type is not supported by deserialization, an Unsupported exception is raised. See section about conversions from pytest import raises from apischema import Unsupported , deserialize , serialize class Foo : pass with raises ( Unsupported ): deserialize ( Foo , {}) with raises ( Unsupported ): serialize ( Foo ())","title":"Unsupported"},{"location":"de_serialization/#faq","text":"","title":"FAQ"},{"location":"de_serialization/#why-coercion-is-not-default-behavior","text":"Because ill-formed data can be symptomatic of problems, and it has been decided that highlighting them would be better than hiding them. By the way, this is easily globally configurable.","title":"Why coercion is not default behavior?"},{"location":"de_serialization/#why-with_fields_set-feature-is-not-enable-by-default","text":"It's true that this feature has the little cost of adding a decorator everywhere. However, keeping dataclass decorator allows IDEs/linters/type checkers/etc. to handle the class as such, so there is no need to develop a plugin for them. Standard compliance can be worth the additional decorator. (And little overhead can be avoided when not useful)","title":"Why with_fields_set feature is not enable by default?"},{"location":"miscellaneous/","text":"Miscellaneous \u00b6 (De)serialization speedup \u00b6 Even if Apischema is among the fastest library, conversion feature imply some overhead. However it's possible to get rid if this overhead by reusing the computations done each time deserialize / serialize is called. In fact: apischema . deserialize ( cls , data , ... ) # is equivalent to apischema . deserialization . make_deserializer ( cls , ... )( data ) apischema . serialize ( obj , ... ) # is equivalent to apischema . serialization . make_serialize ( ... )( obj ) Deserializers/serializers created with make_deserializer / make_serializer can be reused, the overhead being deported at the creation and thus happening only one time. Ignore Union member \u00b6 Sometimes, one of the Union members has not to be taken in account during validation; it can just be here to match the type of the default value of the field. This member can be marked as to be ignored with PEP 593 Annotated from dataclasses import dataclass from typing import Union from pytest import raises from typing_extensions import Annotated from apischema import ValidationError , deserialize , serialize from apischema.misc import Ignored @dataclass class Foo : bar : Union [ int , Annotated [ None , Ignored ]] = None with raises ( ValidationError ) as err : deserialize ( Foo , { \"bar\" : None }) assert serialize ( err . value ) == [ { \"loc\" : [ \"bar\" ], \"err\" : [ \"expected type integer, found null\" ]} ] Ignore field default value \u00b6 Sometimes you want to have a default value for a field in order to be more convenient in your code, but still make the field required. from dataclasses import dataclass , field from typing import Optional , Union from pytest import raises from typing_extensions import Annotated from apischema import ValidationError , deserialize , serialize from apischema.metadata import ignore_default from apischema.misc import Ignored @dataclass class Foo : bar : Optional [ int ] = field ( default = None , metadata = ignore_default ()) with raises ( ValidationError ) as err : deserialize ( Foo , {}) assert serialize ( err . value ) == [ { \"loc\" : [ \"bar\" ], \"err\" : [ \"missing property\" ]} ] Dataclass __post_init__ and init=False \u00b6 TO BE COMPLETED","title":"Miscellaneous"},{"location":"miscellaneous/#miscellaneous","text":"","title":"Miscellaneous"},{"location":"miscellaneous/#deserialization-speedup","text":"Even if Apischema is among the fastest library, conversion feature imply some overhead. However it's possible to get rid if this overhead by reusing the computations done each time deserialize / serialize is called. In fact: apischema . deserialize ( cls , data , ... ) # is equivalent to apischema . deserialization . make_deserializer ( cls , ... )( data ) apischema . serialize ( obj , ... ) # is equivalent to apischema . serialization . make_serialize ( ... )( obj ) Deserializers/serializers created with make_deserializer / make_serializer can be reused, the overhead being deported at the creation and thus happening only one time.","title":"(De)serialization speedup"},{"location":"miscellaneous/#ignore-union-member","text":"Sometimes, one of the Union members has not to be taken in account during validation; it can just be here to match the type of the default value of the field. This member can be marked as to be ignored with PEP 593 Annotated from dataclasses import dataclass from typing import Union from pytest import raises from typing_extensions import Annotated from apischema import ValidationError , deserialize , serialize from apischema.misc import Ignored @dataclass class Foo : bar : Union [ int , Annotated [ None , Ignored ]] = None with raises ( ValidationError ) as err : deserialize ( Foo , { \"bar\" : None }) assert serialize ( err . value ) == [ { \"loc\" : [ \"bar\" ], \"err\" : [ \"expected type integer, found null\" ]} ]","title":"Ignore Union member"},{"location":"miscellaneous/#ignore-field-default-value","text":"Sometimes you want to have a default value for a field in order to be more convenient in your code, but still make the field required. from dataclasses import dataclass , field from typing import Optional , Union from pytest import raises from typing_extensions import Annotated from apischema import ValidationError , deserialize , serialize from apischema.metadata import ignore_default from apischema.misc import Ignored @dataclass class Foo : bar : Optional [ int ] = field ( default = None , metadata = ignore_default ()) with raises ( ValidationError ) as err : deserialize ( Foo , {}) assert serialize ( err . value ) == [ { \"loc\" : [ \"bar\" ], \"err\" : [ \"missing property\" ]} ]","title":"Ignore field default value"},{"location":"miscellaneous/#dataclass-__post_init__-and-initfalse","text":"TO BE COMPLETED","title":"Dataclass __post_init__ and init=False"},{"location":"schema/","text":"(JSON-)schema \u00b6 JSON-schema generation \u00b6 Field alias \u00b6 Sometimes dataclass field names can clash with language keyword, sometimes the property name is not convenient. Hopefully, field can define an alias which will be used in schema and deserialization/serialization. from __future__ import annotations from dataclasses import dataclass , field from apischema import alias , build_input_schema , deserialize , serialize @dataclass class Foo : class_ : str = field ( metadata = alias ( \"class\" )) assert serialize ( build_input_schema ( Foo )) == { \"additionalProperties\" : False , \"properties\" : { \"class\" : { \"type\" : \"string\" }}, \"required\" : [ \"class\" ], \"type\" : \"object\" , } assert deserialize ( Foo , { \"class\" : \"bar\" }) == Foo ( \"bar\" ) assert serialize ( Foo ( \"bar\" )) == { \"class\" : \"bar\" } Field aliasing can also be done at class level by specifying an aliasing function. This aliaser is applied to field alias if defined or field name, or not applied if override=False is specified. from __future__ import annotations from dataclasses import dataclass , field from typing import Any from apischema import alias , build_input_schema , serialize @alias ( lambda s : f \"foo_ { s } \" ) @dataclass class Foo : field1 : Any field2 : Any = field ( metadata = alias ( override = False )) field3 : Any = field ( metadata = alias ( \"field03\" )) field4 : Any = field ( metadata = alias ( \"field04\" , override = False )) assert serialize ( build_input_schema ( Foo )) == { \"additionalProperties\" : False , \"properties\" : { \"foo_field1\" : {}, \"field2\" : {}, \"foo_field03\" : {}, \"field04\" : {}}, \"required\" : [ \"foo_field1\" , \"field2\" , \"foo_field03\" , \"field04\" ], \"type\" : \"object\" , } Class-level aliasing can be used to define a camelCase API. Schema annotations \u00b6 Constraints validation \u00b6 Additional properties / pattern properties \u00b6 With Mapping \u00b6 Schema of a Mapping / Dict type is naturally translated to \"additionalProperties\": <schema of the value type> . However when the schema of the key has a pattern , it will give a \"patternProperties\": {<key pattern>: <schema of the value type>} With dataclass \u00b6 additionalProperties / patternProperties can be added to dataclasses by using fields annotated with properties metadata. Properties not mapped on regular fields will be deserialized into this fields; they must have a Mapping type (or be convertible from Mapping ). ```python from dataclasses import dataclass, field from typing import Mapping from apischema import deserialize, properties @dataclass class Config: active: bool = True options: Mapping[str, bool] = field(default_factory=dict, metadata=properties()) engine_option: Mapping[str, bool] = field( default_factory=dict, metadata=properties(pattern=r\"^engine_\") ) assert deserialize( Config, {\"use_lightsaber\": True, \"engine_restart_on_failure\": False} ) == Config(True, {\"use_lightsaber\": True}, {\"engine_restart_on_failure\": False}) ``` Note Of course, a dataclass can only have a single properties field without pattern, because it makes no sens to have several additionalProperties . FAQ \u00b6 Could we have a global setting for using camelCase everywhere? \u00b6 It has been thought about, but there is an issue for application using both camelCase (to answer frontend) and snake_case (to communicate with other backend services). It could be nice to be able to select case at (de)serialization process, without affecting performance. This possibility is currently examined.","title":"(JSON-)schema"},{"location":"schema/#json-schema","text":"","title":"(JSON-)schema"},{"location":"schema/#json-schema-generation","text":"","title":"JSON-schema generation"},{"location":"schema/#field-alias","text":"Sometimes dataclass field names can clash with language keyword, sometimes the property name is not convenient. Hopefully, field can define an alias which will be used in schema and deserialization/serialization. from __future__ import annotations from dataclasses import dataclass , field from apischema import alias , build_input_schema , deserialize , serialize @dataclass class Foo : class_ : str = field ( metadata = alias ( \"class\" )) assert serialize ( build_input_schema ( Foo )) == { \"additionalProperties\" : False , \"properties\" : { \"class\" : { \"type\" : \"string\" }}, \"required\" : [ \"class\" ], \"type\" : \"object\" , } assert deserialize ( Foo , { \"class\" : \"bar\" }) == Foo ( \"bar\" ) assert serialize ( Foo ( \"bar\" )) == { \"class\" : \"bar\" } Field aliasing can also be done at class level by specifying an aliasing function. This aliaser is applied to field alias if defined or field name, or not applied if override=False is specified. from __future__ import annotations from dataclasses import dataclass , field from typing import Any from apischema import alias , build_input_schema , serialize @alias ( lambda s : f \"foo_ { s } \" ) @dataclass class Foo : field1 : Any field2 : Any = field ( metadata = alias ( override = False )) field3 : Any = field ( metadata = alias ( \"field03\" )) field4 : Any = field ( metadata = alias ( \"field04\" , override = False )) assert serialize ( build_input_schema ( Foo )) == { \"additionalProperties\" : False , \"properties\" : { \"foo_field1\" : {}, \"field2\" : {}, \"foo_field03\" : {}, \"field04\" : {}}, \"required\" : [ \"foo_field1\" , \"field2\" , \"foo_field03\" , \"field04\" ], \"type\" : \"object\" , } Class-level aliasing can be used to define a camelCase API.","title":"Field alias"},{"location":"schema/#schema-annotations","text":"","title":"Schema annotations"},{"location":"schema/#constraints-validation","text":"","title":"Constraints validation"},{"location":"schema/#additional-properties-pattern-properties","text":"","title":"Additional properties / pattern properties"},{"location":"schema/#with-mapping","text":"Schema of a Mapping / Dict type is naturally translated to \"additionalProperties\": <schema of the value type> . However when the schema of the key has a pattern , it will give a \"patternProperties\": {<key pattern>: <schema of the value type>}","title":"With Mapping"},{"location":"schema/#with-dataclass","text":"additionalProperties / patternProperties can be added to dataclasses by using fields annotated with properties metadata. Properties not mapped on regular fields will be deserialized into this fields; they must have a Mapping type (or be convertible from Mapping ). ```python from dataclasses import dataclass, field from typing import Mapping from apischema import deserialize, properties @dataclass class Config: active: bool = True options: Mapping[str, bool] = field(default_factory=dict, metadata=properties()) engine_option: Mapping[str, bool] = field( default_factory=dict, metadata=properties(pattern=r\"^engine_\") ) assert deserialize( Config, {\"use_lightsaber\": True, \"engine_restart_on_failure\": False} ) == Config(True, {\"use_lightsaber\": True}, {\"engine_restart_on_failure\": False}) ``` Note Of course, a dataclass can only have a single properties field without pattern, because it makes no sens to have several additionalProperties .","title":"With dataclass"},{"location":"schema/#faq","text":"","title":"FAQ"},{"location":"schema/#could-we-have-a-global-setting-for-using-camelcase-everywhere","text":"It has been thought about, but there is an issue for application using both camelCase (to answer frontend) and snake_case (to communicate with other backend services). It could be nice to be able to select case at (de)serialization process, without affecting performance. This possibility is currently examined.","title":"Could we have a global setting for using camelCase everywhere?"},{"location":"validation/","text":"Validation \u00b6 Validation is an important part of deserialization. By default, Apischema validate types of data according to typing annotations, and schema constraints. But custom validators can also be add for a more precise validation. Deserialization and validation error \u00b6 ValidaitonError is raised when validation fails. This exception will contains all the information about the ill-formed part of the data. By the way this exception is also serializable and can be sent back directly to client. from dataclasses import dataclass , field from typing import List , NewType from uuid import UUID , uuid4 from pytest import raises from apischema import ValidationError , deserialize , schema , serialize Tag = NewType ( \"Tag\" , str ) schema ( pattern = r \"\\w{1,16}\" )( Tag ) # Define a schema with standard dataclasses @dataclass class Resource : id : UUID tags : List [ Tag ] = field ( default_factory = list , metadata = schema ( unique = True , max_items = 3 ) ) with raises ( ValidationError ) as err : # pytest check exception is raised deserialize ( Resource , { \"id\" : \"fake_uuid\" , \"tags\" : [ \"tag1\" , \"tag2\" , \"tag2\" , \"bad tag\" ]} ) assert serialize ( err . value ) == [ { \"loc\" : [ \"id\" ], \"err\" : [ \"badly formed hexadecimal UUID string\" ]}, { \"loc\" : [ \"tags\" ], \"err\" : [ \"size greater than 3 (maxItems)\" , \"duplicate items (uniqueItems)\" ]}, { \"loc\" : [ \"tags\" , 3 ], \"err\" : [ \"unmatched pattern ' \\\\ w{1,16}'\" ]} ] As shown in the example, Apischema will not stop at the first error met but tries to validate all parts of the data. Dataclass validators \u00b6 Dataclass validation can be completed by custom validators. These are simple decorated methods which will be executed during validation, after all fields having been deserialized. from dataclasses import dataclass from pytest import raises from apischema import ValidationError , deserialize , serialize , validator @dataclass class PasswordForm : password : str confirmation : str @validator def password_match ( self ): if self . password != self . confirmation : raise ValueError ( \"password doesn't match its confirmation\" ) with raises ( ValidationError ) as err : deserialize ( PasswordForm , { \"password\" : \"p455w0rd\" , \"confirmation\" : \"...\" }) assert serialize ( err . value ) == [ { \"loc\" : [], \"err\" : [ \"password doesn't match its confirmation\" ]} ] Warning DO NOT use assert statement to validate external data, never. In fact, this statement is made to be disabled when executed in optimized mode (see documentation ), so validation would be disabled too. This warning doesn't concern only Apischema ; assert is only for internal assertion in debug environment. Computed dependencies \u00b6 It makes no sense to execute a validator using a field that is ill-formed. Hopefully, Apischema is able to compute validator dependencies \u2014 the field used in validator; validator is executed only if the all its dependencies are ok. from dataclasses import dataclass from pytest import raises from apischema import ValidationError , deserialize , serialize , validator @dataclass class PasswordForm : password : str confirmation : str @validator def password_match ( self ): if self . password != self . confirmation : raise ValueError ( \"password doesn't match its confirmation\" ) with raises ( ValidationError ) as err : deserialize ( PasswordForm , { \"password\" : \"p455w0rd\" }) assert serialize ( err . value ) == [ # validator is not executed because confirmation is missing { \"loc\" : [ \"confirmation\" ], \"err\" : [ \"missing property\" ]} ] Note Despite the fact that validator use self argument, it can be called during validation even if all the fields of the class are not ok and the class not really instantiated. In fact, instance is kind of mocked for validation with only the needed field. Yield instead of raise \u00b6 Discard \u00b6 Field validators \u00b6 At field level \u00b6 Fields are validated according to their types and schema. But it's also possible to add validators to fields from dataclasses import dataclass , field from typing import Optional from pytest import raises from apischema import ValidationError , deserialize , serialize from apischema.metadata import field_validator def check_no_duplicate_digits ( n : int ): if len ( str ( n )) != len ( set ( str ( n ))): raise ValueError ( \"number has duplicate digits\" ) @dataclass class Foo : bar : str = field ( metadata = field_validator ( check_no_duplicate_digits )) with raises ( ValidationError ) as err : deserialize ( Foo , { \"bar\" : \"11\" }) assert serialize ( err . value ) == [ { \"loc\" : [ \"bar\" ], \"err\" : [ \"number has duplicate digits\" ]} ] When validation fails for a field, it is discarded and cannot be used in class validators, as it is the case when field schema validation fails. Note field_validator allow to reuse the the same validator for several fields. However in this case, using a custom type (for example a NewType ) with validators (see next section ) could often be a better solution. Using other fields \u00b6 A common pattern can be to validate a field using other fields values. This is achieved with dataclass validators seen above. However there is a shortcut for this use case: from dataclasses import dataclass , field from enum import Enum from pytest import raises from apischema import ValidationError , deserialize , serialize , validator from apischema.fields import get_fields class Parity ( Enum ): EVEN = \"even\" ODD = \"odd\" @dataclass class NumberWithParity : parity : Parity number : int = field () # field must be assign, even with empty `field()` @validator ( number ) def check_sum ( self ): if ( self . parity == Parity . EVEN ) != ( self . number % 2 == 0 ): yield \"number doesn't respect parity\" # using field argument adds automatically discard argument # and the name of the field to error path @validator ( discard = [ number ]) def check_sum_equivalent ( self ): if ( self . parity == Parity . EVEN ) != ( self . number % 2 == 0 ): yield get_fields ( self ) . number , \"number doesn't respect parity\" with raises ( ValidationError ) as err : deserialize ( NumberWithParity , { \"parity\" : \"even\" , \"number\" : 1 }) assert serialize ( err . value ) == [ { \"loc\" : [ \"number\" ], \"err\" : [ \"number doesn't respect parity\" ]} ] Validators for every (new) types \u00b6 Manual validation \u00b6 Validation is only performed by deserialization; if you just instantiate a class having some validators, there will be no validation. It's however possible to trigger it with apischema.validate function. It could be use for example to ensure some conversion result. from dataclasses import dataclass , field from pytest import raises from apischema import ValidationError , schema , serialize , validator from apischema.validation import validate @dataclass class Foo : bar : int = field ( metadata = schema ( min = 0 , max = 10 )) baz : int @validator def not_equal ( self ): if self . bar == self . baz : yield \"bar cannot be equal to baz\" foo1 = Foo ( - 1 , 0 ) with raises ( ValidationError ) as err : validate ( foo1 ) assert serialize ( err . value ) == [{ \"loc\" : [ \"bar\" ], \"err\" : [ \"less than 0 (minimum)\" ]}] with raises ( ValidationError ) as err : foo2 = validate ( Foo ( 2 , 2 )) assert serialize ( err . value ) == [{ \"loc\" : [], \"err\" : [ \"bar cannot be equal to baz\" ]}] FAQ \u00b6 How are computed validator depedencies? \u00b6 ast.NodeVisitor and the Python black magic begins... Why only validate at deserialization and not at instantiation? \u00b6 Dataclass are typed-checked, so data put in the constructor are supposed to be typed-checked too, so validation would be useless most of the time. That's why it doesn't seem justified to add this high overhead everywhere for everyone. By the way, if it was implemented, in order to be consistent, validation should be performed each time the instance is modified (if you don't trust __init__ argument, why would it be different for __setattr__ ) and it would add even more overhead.","title":"Validation"},{"location":"validation/#validation","text":"Validation is an important part of deserialization. By default, Apischema validate types of data according to typing annotations, and schema constraints. But custom validators can also be add for a more precise validation.","title":"Validation"},{"location":"validation/#deserialization-and-validation-error","text":"ValidaitonError is raised when validation fails. This exception will contains all the information about the ill-formed part of the data. By the way this exception is also serializable and can be sent back directly to client. from dataclasses import dataclass , field from typing import List , NewType from uuid import UUID , uuid4 from pytest import raises from apischema import ValidationError , deserialize , schema , serialize Tag = NewType ( \"Tag\" , str ) schema ( pattern = r \"\\w{1,16}\" )( Tag ) # Define a schema with standard dataclasses @dataclass class Resource : id : UUID tags : List [ Tag ] = field ( default_factory = list , metadata = schema ( unique = True , max_items = 3 ) ) with raises ( ValidationError ) as err : # pytest check exception is raised deserialize ( Resource , { \"id\" : \"fake_uuid\" , \"tags\" : [ \"tag1\" , \"tag2\" , \"tag2\" , \"bad tag\" ]} ) assert serialize ( err . value ) == [ { \"loc\" : [ \"id\" ], \"err\" : [ \"badly formed hexadecimal UUID string\" ]}, { \"loc\" : [ \"tags\" ], \"err\" : [ \"size greater than 3 (maxItems)\" , \"duplicate items (uniqueItems)\" ]}, { \"loc\" : [ \"tags\" , 3 ], \"err\" : [ \"unmatched pattern ' \\\\ w{1,16}'\" ]} ] As shown in the example, Apischema will not stop at the first error met but tries to validate all parts of the data.","title":"Deserialization and validation error"},{"location":"validation/#dataclass-validators","text":"Dataclass validation can be completed by custom validators. These are simple decorated methods which will be executed during validation, after all fields having been deserialized. from dataclasses import dataclass from pytest import raises from apischema import ValidationError , deserialize , serialize , validator @dataclass class PasswordForm : password : str confirmation : str @validator def password_match ( self ): if self . password != self . confirmation : raise ValueError ( \"password doesn't match its confirmation\" ) with raises ( ValidationError ) as err : deserialize ( PasswordForm , { \"password\" : \"p455w0rd\" , \"confirmation\" : \"...\" }) assert serialize ( err . value ) == [ { \"loc\" : [], \"err\" : [ \"password doesn't match its confirmation\" ]} ] Warning DO NOT use assert statement to validate external data, never. In fact, this statement is made to be disabled when executed in optimized mode (see documentation ), so validation would be disabled too. This warning doesn't concern only Apischema ; assert is only for internal assertion in debug environment.","title":"Dataclass validators"},{"location":"validation/#computed-dependencies","text":"It makes no sense to execute a validator using a field that is ill-formed. Hopefully, Apischema is able to compute validator dependencies \u2014 the field used in validator; validator is executed only if the all its dependencies are ok. from dataclasses import dataclass from pytest import raises from apischema import ValidationError , deserialize , serialize , validator @dataclass class PasswordForm : password : str confirmation : str @validator def password_match ( self ): if self . password != self . confirmation : raise ValueError ( \"password doesn't match its confirmation\" ) with raises ( ValidationError ) as err : deserialize ( PasswordForm , { \"password\" : \"p455w0rd\" }) assert serialize ( err . value ) == [ # validator is not executed because confirmation is missing { \"loc\" : [ \"confirmation\" ], \"err\" : [ \"missing property\" ]} ] Note Despite the fact that validator use self argument, it can be called during validation even if all the fields of the class are not ok and the class not really instantiated. In fact, instance is kind of mocked for validation with only the needed field.","title":"Computed dependencies"},{"location":"validation/#yield-instead-of-raise","text":"","title":"Yield instead of raise"},{"location":"validation/#discard","text":"","title":"Discard"},{"location":"validation/#field-validators","text":"","title":"Field validators"},{"location":"validation/#at-field-level","text":"Fields are validated according to their types and schema. But it's also possible to add validators to fields from dataclasses import dataclass , field from typing import Optional from pytest import raises from apischema import ValidationError , deserialize , serialize from apischema.metadata import field_validator def check_no_duplicate_digits ( n : int ): if len ( str ( n )) != len ( set ( str ( n ))): raise ValueError ( \"number has duplicate digits\" ) @dataclass class Foo : bar : str = field ( metadata = field_validator ( check_no_duplicate_digits )) with raises ( ValidationError ) as err : deserialize ( Foo , { \"bar\" : \"11\" }) assert serialize ( err . value ) == [ { \"loc\" : [ \"bar\" ], \"err\" : [ \"number has duplicate digits\" ]} ] When validation fails for a field, it is discarded and cannot be used in class validators, as it is the case when field schema validation fails. Note field_validator allow to reuse the the same validator for several fields. However in this case, using a custom type (for example a NewType ) with validators (see next section ) could often be a better solution.","title":"At field level"},{"location":"validation/#using-other-fields","text":"A common pattern can be to validate a field using other fields values. This is achieved with dataclass validators seen above. However there is a shortcut for this use case: from dataclasses import dataclass , field from enum import Enum from pytest import raises from apischema import ValidationError , deserialize , serialize , validator from apischema.fields import get_fields class Parity ( Enum ): EVEN = \"even\" ODD = \"odd\" @dataclass class NumberWithParity : parity : Parity number : int = field () # field must be assign, even with empty `field()` @validator ( number ) def check_sum ( self ): if ( self . parity == Parity . EVEN ) != ( self . number % 2 == 0 ): yield \"number doesn't respect parity\" # using field argument adds automatically discard argument # and the name of the field to error path @validator ( discard = [ number ]) def check_sum_equivalent ( self ): if ( self . parity == Parity . EVEN ) != ( self . number % 2 == 0 ): yield get_fields ( self ) . number , \"number doesn't respect parity\" with raises ( ValidationError ) as err : deserialize ( NumberWithParity , { \"parity\" : \"even\" , \"number\" : 1 }) assert serialize ( err . value ) == [ { \"loc\" : [ \"number\" ], \"err\" : [ \"number doesn't respect parity\" ]} ]","title":"Using other fields"},{"location":"validation/#validators-for-every-new-types","text":"","title":"Validators for every (new) types"},{"location":"validation/#manual-validation","text":"Validation is only performed by deserialization; if you just instantiate a class having some validators, there will be no validation. It's however possible to trigger it with apischema.validate function. It could be use for example to ensure some conversion result. from dataclasses import dataclass , field from pytest import raises from apischema import ValidationError , schema , serialize , validator from apischema.validation import validate @dataclass class Foo : bar : int = field ( metadata = schema ( min = 0 , max = 10 )) baz : int @validator def not_equal ( self ): if self . bar == self . baz : yield \"bar cannot be equal to baz\" foo1 = Foo ( - 1 , 0 ) with raises ( ValidationError ) as err : validate ( foo1 ) assert serialize ( err . value ) == [{ \"loc\" : [ \"bar\" ], \"err\" : [ \"less than 0 (minimum)\" ]}] with raises ( ValidationError ) as err : foo2 = validate ( Foo ( 2 , 2 )) assert serialize ( err . value ) == [{ \"loc\" : [], \"err\" : [ \"bar cannot be equal to baz\" ]}]","title":"Manual validation"},{"location":"validation/#faq","text":"","title":"FAQ"},{"location":"validation/#how-are-computed-validator-depedencies","text":"ast.NodeVisitor and the Python black magic begins...","title":"How are computed validator depedencies?"},{"location":"validation/#why-only-validate-at-deserialization-and-not-at-instantiation","text":"Dataclass are typed-checked, so data put in the constructor are supposed to be typed-checked too, so validation would be useless most of the time. That's why it doesn't seem justified to add this high overhead everywhere for everyone. By the way, if it was implemented, in order to be consistent, validation should be performed each time the instance is modified (if you don't trust __init__ argument, why would it be different for __setattr__ ) and it would add even more overhead.","title":"Why only validate at deserialization and not at instantiation?"}]}