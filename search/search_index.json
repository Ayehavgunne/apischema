{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Overview \u00b6 Apischema \u00b6 Makes your life easier when it comes to python API. JSON (de)serialization + GraphQL and JSON schema generation through python typing, with a spoonful of sugar. Install \u00b6 pip install apischema It requires only Python 3.6+ (and dataclasses official backport for version 3.6 only) PyPy3 is fully supported. Why another library? \u00b6 This library fulfill the following goals: stay as close as possible to the standard library (dataclasses, typing, etc.) to be as accessible as possible \u2014 as a consequence do not need plugins for editor/linter/etc.; be additive and tunable, be able to work with user own types (ORM, etc.) as well as foreign libraries ones; do not need a PR for handling new types like bson.ObjectId , avoid subclassing; avoid dynamic things like using string for attribute name; support GraphQL ; ( Bonus ) be the fastest. No known alternative achieves all of this. Note If you wonder what the difference is with pydantic library, see the dedicated section . Note Actually, Apischema is even adaptable enough to enable support of competitor libraries in a few dozens of line of code ( pydantic support example using conversions feature ) Example \u00b6 from collections.abc import Collection from dataclasses import dataclass , field from uuid import UUID , uuid4 from graphql import print_schema from pytest import raises from apischema import ValidationError , deserialize , serialize from apischema.graphql import graphql_schema from apischema.json_schema import deserialization_schema # Define a schema with standard dataclasses @dataclass class Resource : id : UUID name : str tags : set [ str ] = field ( default_factory = set ) # Get some data uuid = uuid4 () data = { \"id\" : str ( uuid ), \"name\" : \"wyfo\" , \"tags\" : [ \"some_tag\" ]} # Deserialize data resource = deserialize ( Resource , data ) assert resource == Resource ( uuid , \"wyfo\" , { \"some_tag\" }) # Serialize objects assert serialize ( resource ) == data # Validate during deserialization with raises ( ValidationError ) as err : # pytest check exception is raised deserialize ( Resource , { \"id\" : \"42\" , \"name\" : \"wyfo\" }) assert serialize ( err . value ) == [ # ValidationError is serializable { \"loc\" : [ \"id\" ], \"err\" : [ \"badly formed hexadecimal UUID string\" ]} ] # Generate JSON Schema assert deserialization_schema ( Resource ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"type\" : \"object\" , \"properties\" : { \"id\" : { \"type\" : \"string\" , \"format\" : \"uuid\" }, \"name\" : { \"type\" : \"string\" }, \"tags\" : { \"type\" : \"array\" , \"items\" : { \"type\" : \"string\" }, \"uniqueItems\" : True }, }, \"required\" : [ \"id\" , \"name\" ], \"additionalProperties\" : False , } # Define GraphQL operations def resources ( tags : Collection [ str ] = None ) -> Collection [ Resource ]: ... # Generate GraphQL schema schema = graphql_schema ( query = [ resources ], id_types = { UUID }) schema_str = \"\"\" \\ type Query { resources(tags: [String!]): [Resource!] } type Resource { id: ID! name: String! tags: [String!]! } \"\"\" assert print_schema ( schema ) == schema_str Apischema works out of the box with you data model. Note This example and further ones are using pytest stuff because they are in fact run as tests in the library CI GraphQL \u00b6 GraphQL integration is detailed further in the documentation . Performance \u00b6 Apischema is the fastest JSON deserialization and validation library according to benchmarks . FAQ \u00b6 What is the difference between Apischema and pydantic ? \u00b6 See the dedicated section to answer this question. I already have my data model with my SQLAlchemy /ORM tables, will I have to duplicate my code, making one dataclass by table? \u00b6 Why would you have to duplicate them? Apischema can \"work with user own types as well as foreign libraries ones\". Some teasing of conversion feature: you can add default serialization for all your tables, or register different serializer that you can select according to your API endpoint, or both. So SQLAlchemy is supported? Does it support others library? \u00b6 No, in fact, no library are supported, even SQLAlchemy ; it was a choice made to be as small and generic as possible, and to support only the standard library (with types like datetime , UUID ). However, the library is flexible enough to code yourself the support you need with, I hope, the minimal effort. It's of course not excluded to add support in additional small plugin libraries. Feedbacks are welcome about the best way to do things. I need more accurate validation than \"ensure this is an integer and no a string \", can I do that? \u00b6 See the validation section. You can use standard JSON schema validation ( maxItems , pattern , etc.) that will be embedded in your schema or add custom Python validators for each class/fields/ NewType you want. Let's start the Apischema tour.","title":"Overview"},{"location":"#overview","text":"","title":"Overview"},{"location":"#apischema","text":"Makes your life easier when it comes to python API. JSON (de)serialization + GraphQL and JSON schema generation through python typing, with a spoonful of sugar.","title":"Apischema"},{"location":"#install","text":"pip install apischema It requires only Python 3.6+ (and dataclasses official backport for version 3.6 only) PyPy3 is fully supported.","title":"Install"},{"location":"#why-another-library","text":"This library fulfill the following goals: stay as close as possible to the standard library (dataclasses, typing, etc.) to be as accessible as possible \u2014 as a consequence do not need plugins for editor/linter/etc.; be additive and tunable, be able to work with user own types (ORM, etc.) as well as foreign libraries ones; do not need a PR for handling new types like bson.ObjectId , avoid subclassing; avoid dynamic things like using string for attribute name; support GraphQL ; ( Bonus ) be the fastest. No known alternative achieves all of this. Note If you wonder what the difference is with pydantic library, see the dedicated section . Note Actually, Apischema is even adaptable enough to enable support of competitor libraries in a few dozens of line of code ( pydantic support example using conversions feature )","title":"Why another library?"},{"location":"#example","text":"from collections.abc import Collection from dataclasses import dataclass , field from uuid import UUID , uuid4 from graphql import print_schema from pytest import raises from apischema import ValidationError , deserialize , serialize from apischema.graphql import graphql_schema from apischema.json_schema import deserialization_schema # Define a schema with standard dataclasses @dataclass class Resource : id : UUID name : str tags : set [ str ] = field ( default_factory = set ) # Get some data uuid = uuid4 () data = { \"id\" : str ( uuid ), \"name\" : \"wyfo\" , \"tags\" : [ \"some_tag\" ]} # Deserialize data resource = deserialize ( Resource , data ) assert resource == Resource ( uuid , \"wyfo\" , { \"some_tag\" }) # Serialize objects assert serialize ( resource ) == data # Validate during deserialization with raises ( ValidationError ) as err : # pytest check exception is raised deserialize ( Resource , { \"id\" : \"42\" , \"name\" : \"wyfo\" }) assert serialize ( err . value ) == [ # ValidationError is serializable { \"loc\" : [ \"id\" ], \"err\" : [ \"badly formed hexadecimal UUID string\" ]} ] # Generate JSON Schema assert deserialization_schema ( Resource ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"type\" : \"object\" , \"properties\" : { \"id\" : { \"type\" : \"string\" , \"format\" : \"uuid\" }, \"name\" : { \"type\" : \"string\" }, \"tags\" : { \"type\" : \"array\" , \"items\" : { \"type\" : \"string\" }, \"uniqueItems\" : True }, }, \"required\" : [ \"id\" , \"name\" ], \"additionalProperties\" : False , } # Define GraphQL operations def resources ( tags : Collection [ str ] = None ) -> Collection [ Resource ]: ... # Generate GraphQL schema schema = graphql_schema ( query = [ resources ], id_types = { UUID }) schema_str = \"\"\" \\ type Query { resources(tags: [String!]): [Resource!] } type Resource { id: ID! name: String! tags: [String!]! } \"\"\" assert print_schema ( schema ) == schema_str Apischema works out of the box with you data model. Note This example and further ones are using pytest stuff because they are in fact run as tests in the library CI","title":"Example"},{"location":"#graphql","text":"GraphQL integration is detailed further in the documentation .","title":"GraphQL"},{"location":"#performance","text":"Apischema is the fastest JSON deserialization and validation library according to benchmarks .","title":"Performance"},{"location":"#faq","text":"","title":"FAQ"},{"location":"#what-is-the-difference-between-apischema-and-pydantic","text":"See the dedicated section to answer this question.","title":"What is the difference between Apischema and pydantic?"},{"location":"#i-already-have-my-data-model-with-my-sqlalchemyorm-tables-will-i-have-to-duplicate-my-code-making-one-dataclass-by-table","text":"Why would you have to duplicate them? Apischema can \"work with user own types as well as foreign libraries ones\". Some teasing of conversion feature: you can add default serialization for all your tables, or register different serializer that you can select according to your API endpoint, or both.","title":"I already have my data model with my SQLAlchemy/ORM tables, will I have to duplicate my code, making one dataclass by table?"},{"location":"#so-sqlalchemy-is-supported-does-it-support-others-library","text":"No, in fact, no library are supported, even SQLAlchemy ; it was a choice made to be as small and generic as possible, and to support only the standard library (with types like datetime , UUID ). However, the library is flexible enough to code yourself the support you need with, I hope, the minimal effort. It's of course not excluded to add support in additional small plugin libraries. Feedbacks are welcome about the best way to do things.","title":"So SQLAlchemy is supported? Does it support others library?"},{"location":"#i-need-more-accurate-validation-than-ensure-this-is-an-integer-and-no-a-string-can-i-do-that","text":"See the validation section. You can use standard JSON schema validation ( maxItems , pattern , etc.) that will be embedded in your schema or add custom Python validators for each class/fields/ NewType you want. Let's start the Apischema tour.","title":"I need more accurate validation than \"ensure this is an integer and no a string \", can I do that?"},{"location":"benchmark/","text":"Benchmark \u00b6 Note Benchmark presented is just Pydantic benchmark where Apischema has been \"inserted\" . Benchmark is run without Cython compilation . Below are the results of crude benchmark comparing apischema to pydantic and other validation libraries. Package Version Relative Performance Mean deserialization time apischema 0.12.1 49.7\u03bcs pydantic 1.7.3 1.5x slower 75.3\u03bcs valideer 0.4.2 2.4x slower 118.4\u03bcs attrs + cattrs 20.2.0 2.6x slower 126.8\u03bcs marshmallow 3.8.0 4.1x slower 202.1\u03bcs voluptuous 0.12.0 5.2x slower 256.6\u03bcs trafaret 2.1.0 5.7x slower 282.3\u03bcs django-rest-framework 3.12.1 20.5x slower 1019.0\u03bcs cerberus 1.3.2 41.3x slower 2051.0\u03bcs Package Version Relative Performance Mean serialization time apischema 0.12.1 29.4\u03bcs pydantic 1.7.3 1.6x slower 47.6\u03bcs Benchmarks were run with Python 3.8 ( CPython ) and the package versions listed above installed via pypi on macOs 10.15.7 Note A few precisions have to be written about these results: pydantic version executed is not Cythonised ; by the way, even with Cython , Apischema is still faster than pydantic Apischema is optimized enough to not have a real performance improvement using Pypy instead of CPython pydantic benchmark is biased by the implementation of datetime parsing for cattrs (see this post about it); in fact, if cattrs use a decently fast implementation, like the standard datetime.fromisoformat , cattrs becomes 3 times faster than pydantic , even faster than Apischema . Of course, you don't get the same features, like complete error handling, aggregate fields, etc. In fact, performance difference between Apischema and cattrs comes mostly of error handling ( cattrs doesn't catch errors to gather them) and the gap between them is a lot reduced when playing benchmark only on valid cases. Pydantic benchmark mixes valid with invalid data (around 50/50) \u2014 in real use, I hope your APIs receive less than 50% bad request. It means that error handling is very (too much?) important in this benchmark, and libraries like cattrs which raise and end simply at the first error encountered have a big advantage. FAQ \u00b6 Why not ask directly for integration to pydantic benchmark? \u00b6 Done, but rejected because \"apischema doesn't have enough usage\". Let's change that!","title":"Benchmark"},{"location":"benchmark/#benchmark","text":"Note Benchmark presented is just Pydantic benchmark where Apischema has been \"inserted\" . Benchmark is run without Cython compilation . Below are the results of crude benchmark comparing apischema to pydantic and other validation libraries. Package Version Relative Performance Mean deserialization time apischema 0.12.1 49.7\u03bcs pydantic 1.7.3 1.5x slower 75.3\u03bcs valideer 0.4.2 2.4x slower 118.4\u03bcs attrs + cattrs 20.2.0 2.6x slower 126.8\u03bcs marshmallow 3.8.0 4.1x slower 202.1\u03bcs voluptuous 0.12.0 5.2x slower 256.6\u03bcs trafaret 2.1.0 5.7x slower 282.3\u03bcs django-rest-framework 3.12.1 20.5x slower 1019.0\u03bcs cerberus 1.3.2 41.3x slower 2051.0\u03bcs Package Version Relative Performance Mean serialization time apischema 0.12.1 29.4\u03bcs pydantic 1.7.3 1.6x slower 47.6\u03bcs Benchmarks were run with Python 3.8 ( CPython ) and the package versions listed above installed via pypi on macOs 10.15.7 Note A few precisions have to be written about these results: pydantic version executed is not Cythonised ; by the way, even with Cython , Apischema is still faster than pydantic Apischema is optimized enough to not have a real performance improvement using Pypy instead of CPython pydantic benchmark is biased by the implementation of datetime parsing for cattrs (see this post about it); in fact, if cattrs use a decently fast implementation, like the standard datetime.fromisoformat , cattrs becomes 3 times faster than pydantic , even faster than Apischema . Of course, you don't get the same features, like complete error handling, aggregate fields, etc. In fact, performance difference between Apischema and cattrs comes mostly of error handling ( cattrs doesn't catch errors to gather them) and the gap between them is a lot reduced when playing benchmark only on valid cases. Pydantic benchmark mixes valid with invalid data (around 50/50) \u2014 in real use, I hope your APIs receive less than 50% bad request. It means that error handling is very (too much?) important in this benchmark, and libraries like cattrs which raise and end simply at the first error encountered have a big advantage.","title":"Benchmark"},{"location":"benchmark/#faq","text":"","title":"FAQ"},{"location":"benchmark/#why-not-ask-directly-for-integration-to-pydantic-benchmark","text":"Done, but rejected because \"apischema doesn't have enough usage\". Let's change that!","title":"Why not ask directly for integration to pydantic benchmark?"},{"location":"conversions/","text":"Conversions \u2013 (de)serialization customization \u00b6 Apischema covers majority of standard data types, but it's of course not enough, that's why it gives you the way to add support for all your classes and the libraries you use. Actually, Apischema uses internally its own feature to support standard library data types like UUID/datetime/etc. (see std_types.py ) ORM support can easily be achieved with this feature (see SQLAlchemy example ). In fact, you can even add support of competitor libraries like Pydantic (see Pydantic compatibility example ) Principle - Apischema conversions \u00b6 An Apischema conversion is composed of a source type, let's call it Source , a target type Target and a function of signature (Source) -> Target . When a type (actually, a non-builtin type, so not int / list[str] /etc.) is deserialized, Apischema will look if there is a conversion where this type is the target. If found, the source type of conversion will be deserialized, then conversion function will be applied to get an object of the expected type. Serialization works the same (inverted) way: look for a conversion with type as source, apply conversion (normally get the target type). Conversions are also handled in schema generation: for a deserialization schema, source schema is used (after merging target schema annotations) while target schema is used (after merging source schema annotations) for a serialization schema. Register a conversion \u00b6 Conversion is registered using deserializer / serializer for deserialization/serialization respectively. When used as a decorator, the Source / Target types are directly extracted from conversion function signature. They can also be passed as argument when the function has no type annotations (builtins like datetime.isoformat or foreign library functions). Methods can be used for serializer , as well as classmethod / staticmethod for both (especially deserializer ) from dataclasses import dataclass from apischema import deserialize , deserializer , schema , serialize , serializer from apischema.json_schema import deserialization_schema , serialization_schema @schema ( pattern = r \"^#[0-9a-fA-F] {6} $\" ) @dataclass class RGB : red : int green : int blue : int @serializer def hexa ( self ) -> str : return f \"# { self . red : 02x }{ self . green : 02x }{ self . blue : 02x } \" @deserializer def from_hexa ( hexa : str ) -> RGB : return RGB ( int ( hexa [ 1 : 3 ], 16 ), int ( hexa [ 3 : 5 ], 16 ), int ( hexa [ 5 : 7 ], 16 )) assert deserialize ( RGB , \"#000000\" ) == RGB ( 0 , 0 , 0 ) assert serialize ( RGB ( 0 , 0 , 42 )) == \"#00002a\" assert ( deserialization_schema ( RGB ) == serialization_schema ( RGB ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"type\" : \"string\" , \"pattern\" : \"^#[0-9a-fA-F] {6} $\" , } ) Warning (De)serializer methods cannot be used with typing.NamedTuple ; in fact, Apischema uses __set_name__ magic method but it is not called on NamedTuple subclass fields. Multiple deserializers \u00b6 Sometimes, you want to have several possibilities to deserialize a type. If it's possible to register a deserializer with an Union param, it's not very practical. That's why Apischema make it possible to register several deserializers for the same type. They will be handled with an Union source type (ordered by deserializers registration), with the right serializer selected according to the matching alternative. import os import time from datetime import datetime from apischema import deserialize , deserializer from apischema.json_schema import deserialization_schema # Set UTC timezone for example os . environ [ \"TZ\" ] = \"UTC\" time . tzset () # There is already `deserializer(datetime.fromisoformat, str, datetime) in apischema # Let's add an other deserializer for datetime from a timestamp @deserializer def datetime_from_timestamp ( timestamp : int ) -> datetime : return datetime . fromtimestamp ( timestamp ) assert deserialization_schema ( datetime ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"anyOf\" : [{ \"type\" : \"string\" , \"format\" : \"date-time\" }, { \"type\" : \"integer\" }], } assert ( deserialize ( datetime , \"2019-10-13\" ) == datetime ( 2019 , 10 , 13 ) == deserialize ( datetime , 1570924800 ) ) Note If it seems that the deserializer declared is equivalent to deserializer(datetime.fromtimestamp, int, datetime) , there is actually a slight difference: in the example, the deserializer makes 2 function calls ( datetime_from_timestamp and datetime.fromtimestamp ) while the second inlined form imply only one function call to datetime.fromtimestamp . In Python, function calls are heavy, so it's good to know. On the other hand, serializer registration overwrite the previous registration if any. That's how the default serialization of builtin types like datetime can be modified (because it's just a serializer call in Apischema code). This is not possible to overwrite this way deserializers (because they stack), but reset_deserializers can be used to reset them before adding new ones. Also, self_deserializer can be used to add a class itself as a deserializer (when it's a supported type like a dataclass). Inheritance \u00b6 All serializers are naturally inherited. In fact, with a conversion function (Source) -> Target , you can always pass a subtype of Source and get a Target in return. Moreover, when serializer is a method, overriding this method in a subclass will override the inherited serializer. from apischema import serialize , serializer class Foo : pass @serializer def serialize_foo ( foo : Foo ) -> int : return 0 class Foo2 ( Foo ): pass # Deserializer is inherited assert serialize ( Foo ()) == serialize ( Foo2 ()) == 0 class Bar : @serializer def serialize ( self ) -> int : return 0 class Bar2 ( Bar ): def serialize ( self ) -> int : return 1 # Deserializer is inherited and overridden assert serialize ( Bar ()) == 0 != serialize ( Bar2 ()) == 1 On the other hand, deserializers cannot be inherited, because the same Source passed to a conversion function (Source) -> Target will always give the same Target (not ensured to be the desired subtype). However, there is one way to do it by using a classmethod and the special decorator inherited_deserializer ; the class parameter of the method is then assumed to be used to instantiate the return. from apischema import deserialize from apischema.conversions import inherited_deserializer class Foo : def __init__ ( self , n : int ): self . n = int def __eq__ ( self , other ): return type ( self ) == type ( other ) and self . n == other . n @inherited_deserializer @classmethod def from_int ( cls , n : int ): return cls ( n ) class Bar ( Foo ): pass assert deserialize ( Foo , 0 ) == Foo ( 0 ) assert deserialize ( Bar , 0 ) == Bar ( 0 ) != Foo ( 0 ) Note An \"other\" way to achieve that would be to use __init_subclass__ method in order to add a deserializer to each subclass. In fact, that's what inherited_deserializer is doing behind the scene. Extra conversions - choose the conversion you want \u00b6 Conversion is a powerful feature, but, registering only one (de)serialization by type may not be enough. Some types may have different representations, or you may have different serialization for a given entity with more or less data (for example a \"simple\" and a \"detailed\" view). Hopefully, Apischema let you register as many conversion as you want for your classes and gives you the possibility to select the one you want. Conversions registered with deserializer / serializer are the default ones, they are selected when no conversion is precised. Other conversions are registered extra_deserializer / extra_serializer (they have the same signature than the previous ones). Conversions can then be selected using the conversions parameter of Apischema functions deserialize / serialize / deserialization_schema / serialization_schema . This parameter must be mapping of types: for deserialization, target as key and source(s) as value for serialization, source as key and target as value (Actually, the type for which is registered the conversion is in key) For deserialization, if there is several possible source , conversions values can also be a collection of types. It will again result in a Union deserialization. Note For definitions_schema , conversions can be added with types by using a tuple instead, for example definitions_schema(serializations=[(list[Foo], {Foo: Bar})]) . import os import time from dataclasses import dataclass from datetime import datetime from typing import NewType from apischema import deserialize , serialize from apischema.conversions import extra_deserializer , extra_serializer # Set UTC timezone for example os . environ [ \"TZ\" ] = \"UTC\" time . tzset () @extra_deserializer def datetime_from_timestamp ( timestamp : int ) -> datetime : return datetime . fromtimestamp ( timestamp ) date = datetime ( 2017 , 9 , 2 ) assert deserialize ( datetime , 1504310400 , conversions = { datetime : int }) == date Diff = NewType ( \"Diff\" , int ) @dataclass class Foo : bar : int baz : int @extra_serializer def sum ( self ) -> int : return self . bar + self . baz # You can use NewType to disambiguate conversions to int # Actually, it could work using something like Diff = Annotated[int, \"diff\"] @extra_serializer def diff ( self ) -> Diff : return Diff ( self . bar - self . baz ) assert serialize ( Foo ( 0 , 1 )) == { \"bar\" : 0 , \"baz\" : 1 } assert serialize ( Foo ( 0 , 1 ), conversions = { Foo : Foo }) == { \"bar\" : 0 , \"baz\" : 1 } assert serialize ( Foo ( 0 , 1 ), conversions = { Foo : int }) == 1 assert serialize ( Foo ( 0 , 1 ), conversions = { Foo : Diff }) == - 1 Chain conversions \u00b6 Conversions mapping put in conversions parameter is not used in all the deserialization/serialization. In fact it is \"reset\" as soon as a non-builtin type (so, not int / list[int] / NewType instances/etc.) is encountered. Not having this reset would completely break the possibility to have $ref in generated schema, because a conversions could then change the serialization of a field of a dataclass in one particular schema but not in another (and bye-bye OpenAPI components schema). But everything is not lost. Let's illustrate with an example. As previously mentioned, Apischema uses its own feature internally at several places. One of them is schema generation. JSON schema is generated using an internal JsonSchema type, and is then serialized; the JSON schema version selection result in fact in a conversion that is selected according to the version (by JsonSchemaVersion.conversions property). However, JSON schema is recursive and the serialization of JsonSchema returns a dictionary which can contain other JsonSchema ... but it has been written above that conversions is reset. That's why deserializer and others conversion registers have a conversions parameter that will be taken as the new conversions after the conversion application. In JSON schema example, it allows sub- JsonSchema to be serialized with the correct conversions . The following example is extracted from Apischema code : from collections.abc import Mapping from typing import Any , NewType from apischema.conversions import extra_serializer class JsonSchema ( dict [ str , Any ]): pass JsonSchema7 = NewType ( \"JsonSchema7\" , Mapping [ str , Any ]) def isolate_ref ( schema : dict [ str , Any ]): if \"$ref\" in schema and len ( schema ) > 1 : schema . setdefault ( \"allOf\" , []) . append ({ \"$ref\" : schema . pop ( \"$ref\" )}) @extra_serializer ( conversions = { JsonSchema : JsonSchema7 }) def to_json_schema_7 ( schema : JsonSchema ) -> JsonSchema7 : result = schema . copy () isolate_ref ( result ) if \"$defs\" in result : result [ \"definitions\" ] = { ** result . pop ( \"$defs\" ), ** result . get ( \"definitions\" , {})} if \"dependentRequired\" in result : result [ \"dependencies\" ] = { ** result . pop ( \"dependentRequired\" ), ** result . get ( \"dependencies\" , {}), } return JsonSchema7 ( result ) Field conversions \u00b6 Dataclass fields conversions can also be customized using conversions metadata. import os import time from dataclasses import dataclass , field from datetime import datetime from apischema import deserialize , serialize from apischema.conversions import extra_deserializer , extra_serializer from apischema.metadata import conversions # Set UTC timezone for example os . environ [ \"TZ\" ] = \"UTC\" time . tzset () extra_deserializer ( datetime . fromtimestamp , int , datetime ) @extra_serializer def to_timestamp ( d : datetime ) -> int : return int ( d . timestamp ()) @dataclass class Foo : some_date : datetime = field ( metadata = conversions ( int )) # `conversions(int)` is equivalent to # `conversions(deserialization={datetime: int}, serialization={datetime: int})` other_date : datetime assert deserialize ( Foo , { \"some_date\" : 0 , \"other_date\" : \"2019-10-13\" }) == Foo ( datetime ( 1970 , 1 , 1 ), datetime ( 2019 , 10 , 13 ) ) assert serialize ( Foo ( datetime ( 1970 , 1 , 1 ), datetime ( 2019 , 10 , 13 ))) == { \"some_date\" : 0 , \"other_date\" : \"2019-10-13T00:00:00\" , } Field (de)serializer \u00b6 conversions metadata can also be used to add directly a (de)serializer to a field \u2014 deserialization and serialization are then applied after the (de)serializer as chained conversions import os import time from dataclasses import dataclass , field from datetime import datetime from apischema import deserialize , serialize from apischema.metadata import conversions # Set UTC timezone for example os . environ [ \"TZ\" ] = \"UTC\" time . tzset () def from_timestamp ( t : int ) -> datetime : return datetime . fromtimestamp ( t ) def to_timestamp ( d : datetime ) -> int : return int ( d . timestamp ()) @dataclass class Foo : some_date : datetime = field ( metadata = conversions ( deserializer = from_timestamp , serializer = to_timestamp ) ) other_date : datetime assert deserialize ( Foo , { \"some_date\" : 0 , \"other_date\" : \"2019-10-13\" }) == Foo ( datetime ( 1970 , 1 , 1 ), datetime ( 2019 , 10 , 13 ) ) assert serialize ( Foo ( datetime ( 1970 , 1 , 1 ), datetime ( 2019 , 10 , 13 ))) == { \"some_date\" : 0 , \"other_date\" : \"2019-10-13T00:00:00\" , } Generic converters are handled naturally from dataclasses import dataclass , field from operator import itemgetter from typing import Dict , Mapping , Sequence , TypeVar from apischema import alias , serialize from apischema.json_schema import serialization_schema from apischema.metadata import conversions T = TypeVar ( \"T\" ) def sort_by_priority ( values_with_priority : Mapping [ int , T ]) -> Sequence [ T ]: return [ v for _ , v in sorted ( values_with_priority . items (), key = itemgetter ( 0 ))] assert sort_by_priority ({ 1 : \"a\" , 0 : \"b\" }) == [ \"b\" , \"a\" ] @dataclass class Foo : values_with_priority : Dict [ int , str ] = field ( metadata = alias ( \"values\" ) | conversions ( serializer = sort_by_priority ) ) assert serialize ( Foo ({ 1 : \"a\" , 0 : \"b\" })) == { \"values\" : [ \"b\" , \"a\" ]} assert serialization_schema ( Foo ) == { \"type\" : \"object\" , \"properties\" : { \"values\" : { \"type\" : \"array\" , \"items\" : { \"type\" : \"string\" }}}, \"required\" : [ \"values\" ], \"additionalProperties\" : False , \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , } Generic conversions \u00b6 Generic conversions are supported out of the box. import sys from typing import Generic , TypeVar from pytest import raises from apischema import ValidationError , deserialize , deserializer , serialize , serializer from apischema.json_schema import deserialization_schema , serialization_schema T = TypeVar ( \"T\" ) class Wrapper ( Generic [ T ]): def __init__ ( self , wrapped : T ): self . wrapped = wrapped # serializer methods of generic class are not handled in Python 3.6 def unwrap ( self ) -> T : return self . wrapped serializer ( Wrapper . unwrap , Wrapper [ T ], T ) deserializer ( Wrapper , T , Wrapper [ T ]) assert deserialize ( Wrapper [ list [ int ]], [ 0 , 1 ]) . wrapped == [ 0 , 1 ] with raises ( ValidationError ): deserialize ( Wrapper [ int ], \"wrapped\" ) assert serialize ( Wrapper ( \"wrapped\" )) == \"wrapped\" assert ( deserialization_schema ( Wrapper [ int ]) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"type\" : \"integer\" } == serialization_schema ( Wrapper [ int ]) ) Warning (De)serializer methods of Generic classes are not handled before 3.7 Note Apischema doesn't support specialization of Generic conversion like Foo[bool] -> int . By the way, it's also possible to use extra conversions and select them the following way: from dataclasses import dataclass from typing import Generic , TypeVar from apischema import serialize from apischema.conversions import extra_serializer T = TypeVar ( \"T\" ) @dataclass class Foo ( Generic [ T ]): bar : T @extra_serializer def to_bar ( foo : Foo [ T ]) -> T : return foo . bar assert serialize ( Foo ( 0 )) == { \"bar\" : 0 } # {Foo: ([T], T)} means a conversions Foo[T] -> T assert serialize ( Foo ( 0 ), conversions = { Foo : ([ T ], T )}) == 0 # Conversion is not tied to a specific TypeVar U = TypeVar ( \"U\" ) assert serialize ( Foo ( 0 ), conversions = { Foo : ([ U ], U )}) == 0 Dataclass model - automatic conversion from/to dataclass \u00b6 Conversions are a powerful tool, which allows to support every type you need. If it is particularly well suited for scalar types ( datetime.datetime , bson.ObjectId , etc.), it may seem a little bit complex for object types. In fact, the conversion would often be a simple mapping of fields between the type and a dataclass. That's why Apischema provides a shortcut for this case: apischema.dataclass_model ; it allows to specify a dataclass which will be used as a typed model for a given class : each field of the dataclass will be mapped on the attributes of the class instances. Dataclass can also be declared dynamically with dataclasses.make_dataclasses . That's especially useful when it comes to add support for libraries like ORM. The following example show how to add a basic support for SQLAlchemy : from dataclasses import MISSING , make_dataclass from inspect import getmembers from typing import Collection from graphql import print_schema from sqlalchemy import Column , Integer from sqlalchemy.ext.declarative import as_declarative from apischema import Undefined , deserialize , serialize from apischema.conversions import dataclass_model from apischema.graphql import graphql_schema from apischema.json_schema import serialization_schema def has_default ( column : Column ) -> bool : return ( column . nullable or column . default is not None or column . server_default is not None ) # Very basic SQLAlchemy support @as_declarative () class Base : def __init_subclass__ ( cls ): columns = getmembers ( cls , lambda m : isinstance ( m , Column )) if not columns : return fields = [ ( column . name or field_name , column . type . python_type , Undefined if has_default ( column ) else MISSING , ) for field_name , column in columns ] dataclass_model ( cls )( make_dataclass ( cls . __name__ , fields )) class Foo ( Base ): __tablename__ = \"foo\" bar = Column ( Integer , primary_key = True ) foo = deserialize ( Foo , { \"bar\" : 0 }) assert isinstance ( foo , Foo ) assert foo . bar == 0 assert serialize ( foo ) == { \"bar\" : 0 } assert serialization_schema ( Foo ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"type\" : \"object\" , \"properties\" : { \"bar\" : { \"type\" : \"integer\" }}, \"required\" : [ \"bar\" ], \"additionalProperties\" : False , } def foos () -> Collection [ Foo ]: ... schema = graphql_schema ( query = [ foos ]) schema_str = \"\"\" \\ type Query { foos: [Foo!] } type Foo { bar: Int! } \"\"\" assert print_schema ( schema ) == schema_str There are two differences with regular conversions : The dataclass model computation can be deferred until it's needed. This is because some libraries do some resolutions after class definition (for example SQLAchemy resolves dynamic string references in relationships). So you can replace the following line in the example # dataclass_model(cls)(make_dataclass(cls.__name__, fields)) dataclass_model ( cls )( lambda : make_dataclass ( cls . __name__ , fields )) Serialized methods/properties of the class are automatically added to the dataclass model (but you can also declare serialized methods in the dataclass model).","title":"Conversions \u2013 (de)serialization customization"},{"location":"conversions/#conversions-deserialization-customization","text":"Apischema covers majority of standard data types, but it's of course not enough, that's why it gives you the way to add support for all your classes and the libraries you use. Actually, Apischema uses internally its own feature to support standard library data types like UUID/datetime/etc. (see std_types.py ) ORM support can easily be achieved with this feature (see SQLAlchemy example ). In fact, you can even add support of competitor libraries like Pydantic (see Pydantic compatibility example )","title":"Conversions \u2013 (de)serialization customization"},{"location":"conversions/#principle-apischema-conversions","text":"An Apischema conversion is composed of a source type, let's call it Source , a target type Target and a function of signature (Source) -> Target . When a type (actually, a non-builtin type, so not int / list[str] /etc.) is deserialized, Apischema will look if there is a conversion where this type is the target. If found, the source type of conversion will be deserialized, then conversion function will be applied to get an object of the expected type. Serialization works the same (inverted) way: look for a conversion with type as source, apply conversion (normally get the target type). Conversions are also handled in schema generation: for a deserialization schema, source schema is used (after merging target schema annotations) while target schema is used (after merging source schema annotations) for a serialization schema.","title":"Principle - Apischema conversions"},{"location":"conversions/#register-a-conversion","text":"Conversion is registered using deserializer / serializer for deserialization/serialization respectively. When used as a decorator, the Source / Target types are directly extracted from conversion function signature. They can also be passed as argument when the function has no type annotations (builtins like datetime.isoformat or foreign library functions). Methods can be used for serializer , as well as classmethod / staticmethod for both (especially deserializer ) from dataclasses import dataclass from apischema import deserialize , deserializer , schema , serialize , serializer from apischema.json_schema import deserialization_schema , serialization_schema @schema ( pattern = r \"^#[0-9a-fA-F] {6} $\" ) @dataclass class RGB : red : int green : int blue : int @serializer def hexa ( self ) -> str : return f \"# { self . red : 02x }{ self . green : 02x }{ self . blue : 02x } \" @deserializer def from_hexa ( hexa : str ) -> RGB : return RGB ( int ( hexa [ 1 : 3 ], 16 ), int ( hexa [ 3 : 5 ], 16 ), int ( hexa [ 5 : 7 ], 16 )) assert deserialize ( RGB , \"#000000\" ) == RGB ( 0 , 0 , 0 ) assert serialize ( RGB ( 0 , 0 , 42 )) == \"#00002a\" assert ( deserialization_schema ( RGB ) == serialization_schema ( RGB ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"type\" : \"string\" , \"pattern\" : \"^#[0-9a-fA-F] {6} $\" , } ) Warning (De)serializer methods cannot be used with typing.NamedTuple ; in fact, Apischema uses __set_name__ magic method but it is not called on NamedTuple subclass fields.","title":"Register a conversion"},{"location":"conversions/#multiple-deserializers","text":"Sometimes, you want to have several possibilities to deserialize a type. If it's possible to register a deserializer with an Union param, it's not very practical. That's why Apischema make it possible to register several deserializers for the same type. They will be handled with an Union source type (ordered by deserializers registration), with the right serializer selected according to the matching alternative. import os import time from datetime import datetime from apischema import deserialize , deserializer from apischema.json_schema import deserialization_schema # Set UTC timezone for example os . environ [ \"TZ\" ] = \"UTC\" time . tzset () # There is already `deserializer(datetime.fromisoformat, str, datetime) in apischema # Let's add an other deserializer for datetime from a timestamp @deserializer def datetime_from_timestamp ( timestamp : int ) -> datetime : return datetime . fromtimestamp ( timestamp ) assert deserialization_schema ( datetime ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"anyOf\" : [{ \"type\" : \"string\" , \"format\" : \"date-time\" }, { \"type\" : \"integer\" }], } assert ( deserialize ( datetime , \"2019-10-13\" ) == datetime ( 2019 , 10 , 13 ) == deserialize ( datetime , 1570924800 ) ) Note If it seems that the deserializer declared is equivalent to deserializer(datetime.fromtimestamp, int, datetime) , there is actually a slight difference: in the example, the deserializer makes 2 function calls ( datetime_from_timestamp and datetime.fromtimestamp ) while the second inlined form imply only one function call to datetime.fromtimestamp . In Python, function calls are heavy, so it's good to know. On the other hand, serializer registration overwrite the previous registration if any. That's how the default serialization of builtin types like datetime can be modified (because it's just a serializer call in Apischema code). This is not possible to overwrite this way deserializers (because they stack), but reset_deserializers can be used to reset them before adding new ones. Also, self_deserializer can be used to add a class itself as a deserializer (when it's a supported type like a dataclass).","title":"Multiple deserializers"},{"location":"conversions/#inheritance","text":"All serializers are naturally inherited. In fact, with a conversion function (Source) -> Target , you can always pass a subtype of Source and get a Target in return. Moreover, when serializer is a method, overriding this method in a subclass will override the inherited serializer. from apischema import serialize , serializer class Foo : pass @serializer def serialize_foo ( foo : Foo ) -> int : return 0 class Foo2 ( Foo ): pass # Deserializer is inherited assert serialize ( Foo ()) == serialize ( Foo2 ()) == 0 class Bar : @serializer def serialize ( self ) -> int : return 0 class Bar2 ( Bar ): def serialize ( self ) -> int : return 1 # Deserializer is inherited and overridden assert serialize ( Bar ()) == 0 != serialize ( Bar2 ()) == 1 On the other hand, deserializers cannot be inherited, because the same Source passed to a conversion function (Source) -> Target will always give the same Target (not ensured to be the desired subtype). However, there is one way to do it by using a classmethod and the special decorator inherited_deserializer ; the class parameter of the method is then assumed to be used to instantiate the return. from apischema import deserialize from apischema.conversions import inherited_deserializer class Foo : def __init__ ( self , n : int ): self . n = int def __eq__ ( self , other ): return type ( self ) == type ( other ) and self . n == other . n @inherited_deserializer @classmethod def from_int ( cls , n : int ): return cls ( n ) class Bar ( Foo ): pass assert deserialize ( Foo , 0 ) == Foo ( 0 ) assert deserialize ( Bar , 0 ) == Bar ( 0 ) != Foo ( 0 ) Note An \"other\" way to achieve that would be to use __init_subclass__ method in order to add a deserializer to each subclass. In fact, that's what inherited_deserializer is doing behind the scene.","title":"Inheritance"},{"location":"conversions/#extra-conversions-choose-the-conversion-you-want","text":"Conversion is a powerful feature, but, registering only one (de)serialization by type may not be enough. Some types may have different representations, or you may have different serialization for a given entity with more or less data (for example a \"simple\" and a \"detailed\" view). Hopefully, Apischema let you register as many conversion as you want for your classes and gives you the possibility to select the one you want. Conversions registered with deserializer / serializer are the default ones, they are selected when no conversion is precised. Other conversions are registered extra_deserializer / extra_serializer (they have the same signature than the previous ones). Conversions can then be selected using the conversions parameter of Apischema functions deserialize / serialize / deserialization_schema / serialization_schema . This parameter must be mapping of types: for deserialization, target as key and source(s) as value for serialization, source as key and target as value (Actually, the type for which is registered the conversion is in key) For deserialization, if there is several possible source , conversions values can also be a collection of types. It will again result in a Union deserialization. Note For definitions_schema , conversions can be added with types by using a tuple instead, for example definitions_schema(serializations=[(list[Foo], {Foo: Bar})]) . import os import time from dataclasses import dataclass from datetime import datetime from typing import NewType from apischema import deserialize , serialize from apischema.conversions import extra_deserializer , extra_serializer # Set UTC timezone for example os . environ [ \"TZ\" ] = \"UTC\" time . tzset () @extra_deserializer def datetime_from_timestamp ( timestamp : int ) -> datetime : return datetime . fromtimestamp ( timestamp ) date = datetime ( 2017 , 9 , 2 ) assert deserialize ( datetime , 1504310400 , conversions = { datetime : int }) == date Diff = NewType ( \"Diff\" , int ) @dataclass class Foo : bar : int baz : int @extra_serializer def sum ( self ) -> int : return self . bar + self . baz # You can use NewType to disambiguate conversions to int # Actually, it could work using something like Diff = Annotated[int, \"diff\"] @extra_serializer def diff ( self ) -> Diff : return Diff ( self . bar - self . baz ) assert serialize ( Foo ( 0 , 1 )) == { \"bar\" : 0 , \"baz\" : 1 } assert serialize ( Foo ( 0 , 1 ), conversions = { Foo : Foo }) == { \"bar\" : 0 , \"baz\" : 1 } assert serialize ( Foo ( 0 , 1 ), conversions = { Foo : int }) == 1 assert serialize ( Foo ( 0 , 1 ), conversions = { Foo : Diff }) == - 1","title":"Extra conversions - choose the conversion you want"},{"location":"conversions/#chain-conversions","text":"Conversions mapping put in conversions parameter is not used in all the deserialization/serialization. In fact it is \"reset\" as soon as a non-builtin type (so, not int / list[int] / NewType instances/etc.) is encountered. Not having this reset would completely break the possibility to have $ref in generated schema, because a conversions could then change the serialization of a field of a dataclass in one particular schema but not in another (and bye-bye OpenAPI components schema). But everything is not lost. Let's illustrate with an example. As previously mentioned, Apischema uses its own feature internally at several places. One of them is schema generation. JSON schema is generated using an internal JsonSchema type, and is then serialized; the JSON schema version selection result in fact in a conversion that is selected according to the version (by JsonSchemaVersion.conversions property). However, JSON schema is recursive and the serialization of JsonSchema returns a dictionary which can contain other JsonSchema ... but it has been written above that conversions is reset. That's why deserializer and others conversion registers have a conversions parameter that will be taken as the new conversions after the conversion application. In JSON schema example, it allows sub- JsonSchema to be serialized with the correct conversions . The following example is extracted from Apischema code : from collections.abc import Mapping from typing import Any , NewType from apischema.conversions import extra_serializer class JsonSchema ( dict [ str , Any ]): pass JsonSchema7 = NewType ( \"JsonSchema7\" , Mapping [ str , Any ]) def isolate_ref ( schema : dict [ str , Any ]): if \"$ref\" in schema and len ( schema ) > 1 : schema . setdefault ( \"allOf\" , []) . append ({ \"$ref\" : schema . pop ( \"$ref\" )}) @extra_serializer ( conversions = { JsonSchema : JsonSchema7 }) def to_json_schema_7 ( schema : JsonSchema ) -> JsonSchema7 : result = schema . copy () isolate_ref ( result ) if \"$defs\" in result : result [ \"definitions\" ] = { ** result . pop ( \"$defs\" ), ** result . get ( \"definitions\" , {})} if \"dependentRequired\" in result : result [ \"dependencies\" ] = { ** result . pop ( \"dependentRequired\" ), ** result . get ( \"dependencies\" , {}), } return JsonSchema7 ( result )","title":"Chain conversions"},{"location":"conversions/#field-conversions","text":"Dataclass fields conversions can also be customized using conversions metadata. import os import time from dataclasses import dataclass , field from datetime import datetime from apischema import deserialize , serialize from apischema.conversions import extra_deserializer , extra_serializer from apischema.metadata import conversions # Set UTC timezone for example os . environ [ \"TZ\" ] = \"UTC\" time . tzset () extra_deserializer ( datetime . fromtimestamp , int , datetime ) @extra_serializer def to_timestamp ( d : datetime ) -> int : return int ( d . timestamp ()) @dataclass class Foo : some_date : datetime = field ( metadata = conversions ( int )) # `conversions(int)` is equivalent to # `conversions(deserialization={datetime: int}, serialization={datetime: int})` other_date : datetime assert deserialize ( Foo , { \"some_date\" : 0 , \"other_date\" : \"2019-10-13\" }) == Foo ( datetime ( 1970 , 1 , 1 ), datetime ( 2019 , 10 , 13 ) ) assert serialize ( Foo ( datetime ( 1970 , 1 , 1 ), datetime ( 2019 , 10 , 13 ))) == { \"some_date\" : 0 , \"other_date\" : \"2019-10-13T00:00:00\" , }","title":"Field conversions"},{"location":"conversions/#field-deserializer","text":"conversions metadata can also be used to add directly a (de)serializer to a field \u2014 deserialization and serialization are then applied after the (de)serializer as chained conversions import os import time from dataclasses import dataclass , field from datetime import datetime from apischema import deserialize , serialize from apischema.metadata import conversions # Set UTC timezone for example os . environ [ \"TZ\" ] = \"UTC\" time . tzset () def from_timestamp ( t : int ) -> datetime : return datetime . fromtimestamp ( t ) def to_timestamp ( d : datetime ) -> int : return int ( d . timestamp ()) @dataclass class Foo : some_date : datetime = field ( metadata = conversions ( deserializer = from_timestamp , serializer = to_timestamp ) ) other_date : datetime assert deserialize ( Foo , { \"some_date\" : 0 , \"other_date\" : \"2019-10-13\" }) == Foo ( datetime ( 1970 , 1 , 1 ), datetime ( 2019 , 10 , 13 ) ) assert serialize ( Foo ( datetime ( 1970 , 1 , 1 ), datetime ( 2019 , 10 , 13 ))) == { \"some_date\" : 0 , \"other_date\" : \"2019-10-13T00:00:00\" , } Generic converters are handled naturally from dataclasses import dataclass , field from operator import itemgetter from typing import Dict , Mapping , Sequence , TypeVar from apischema import alias , serialize from apischema.json_schema import serialization_schema from apischema.metadata import conversions T = TypeVar ( \"T\" ) def sort_by_priority ( values_with_priority : Mapping [ int , T ]) -> Sequence [ T ]: return [ v for _ , v in sorted ( values_with_priority . items (), key = itemgetter ( 0 ))] assert sort_by_priority ({ 1 : \"a\" , 0 : \"b\" }) == [ \"b\" , \"a\" ] @dataclass class Foo : values_with_priority : Dict [ int , str ] = field ( metadata = alias ( \"values\" ) | conversions ( serializer = sort_by_priority ) ) assert serialize ( Foo ({ 1 : \"a\" , 0 : \"b\" })) == { \"values\" : [ \"b\" , \"a\" ]} assert serialization_schema ( Foo ) == { \"type\" : \"object\" , \"properties\" : { \"values\" : { \"type\" : \"array\" , \"items\" : { \"type\" : \"string\" }}}, \"required\" : [ \"values\" ], \"additionalProperties\" : False , \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , }","title":"Field (de)serializer"},{"location":"conversions/#generic-conversions","text":"Generic conversions are supported out of the box. import sys from typing import Generic , TypeVar from pytest import raises from apischema import ValidationError , deserialize , deserializer , serialize , serializer from apischema.json_schema import deserialization_schema , serialization_schema T = TypeVar ( \"T\" ) class Wrapper ( Generic [ T ]): def __init__ ( self , wrapped : T ): self . wrapped = wrapped # serializer methods of generic class are not handled in Python 3.6 def unwrap ( self ) -> T : return self . wrapped serializer ( Wrapper . unwrap , Wrapper [ T ], T ) deserializer ( Wrapper , T , Wrapper [ T ]) assert deserialize ( Wrapper [ list [ int ]], [ 0 , 1 ]) . wrapped == [ 0 , 1 ] with raises ( ValidationError ): deserialize ( Wrapper [ int ], \"wrapped\" ) assert serialize ( Wrapper ( \"wrapped\" )) == \"wrapped\" assert ( deserialization_schema ( Wrapper [ int ]) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"type\" : \"integer\" } == serialization_schema ( Wrapper [ int ]) ) Warning (De)serializer methods of Generic classes are not handled before 3.7 Note Apischema doesn't support specialization of Generic conversion like Foo[bool] -> int . By the way, it's also possible to use extra conversions and select them the following way: from dataclasses import dataclass from typing import Generic , TypeVar from apischema import serialize from apischema.conversions import extra_serializer T = TypeVar ( \"T\" ) @dataclass class Foo ( Generic [ T ]): bar : T @extra_serializer def to_bar ( foo : Foo [ T ]) -> T : return foo . bar assert serialize ( Foo ( 0 )) == { \"bar\" : 0 } # {Foo: ([T], T)} means a conversions Foo[T] -> T assert serialize ( Foo ( 0 ), conversions = { Foo : ([ T ], T )}) == 0 # Conversion is not tied to a specific TypeVar U = TypeVar ( \"U\" ) assert serialize ( Foo ( 0 ), conversions = { Foo : ([ U ], U )}) == 0","title":"Generic conversions"},{"location":"conversions/#dataclass-model-automatic-conversion-fromto-dataclass","text":"Conversions are a powerful tool, which allows to support every type you need. If it is particularly well suited for scalar types ( datetime.datetime , bson.ObjectId , etc.), it may seem a little bit complex for object types. In fact, the conversion would often be a simple mapping of fields between the type and a dataclass. That's why Apischema provides a shortcut for this case: apischema.dataclass_model ; it allows to specify a dataclass which will be used as a typed model for a given class : each field of the dataclass will be mapped on the attributes of the class instances. Dataclass can also be declared dynamically with dataclasses.make_dataclasses . That's especially useful when it comes to add support for libraries like ORM. The following example show how to add a basic support for SQLAlchemy : from dataclasses import MISSING , make_dataclass from inspect import getmembers from typing import Collection from graphql import print_schema from sqlalchemy import Column , Integer from sqlalchemy.ext.declarative import as_declarative from apischema import Undefined , deserialize , serialize from apischema.conversions import dataclass_model from apischema.graphql import graphql_schema from apischema.json_schema import serialization_schema def has_default ( column : Column ) -> bool : return ( column . nullable or column . default is not None or column . server_default is not None ) # Very basic SQLAlchemy support @as_declarative () class Base : def __init_subclass__ ( cls ): columns = getmembers ( cls , lambda m : isinstance ( m , Column )) if not columns : return fields = [ ( column . name or field_name , column . type . python_type , Undefined if has_default ( column ) else MISSING , ) for field_name , column in columns ] dataclass_model ( cls )( make_dataclass ( cls . __name__ , fields )) class Foo ( Base ): __tablename__ = \"foo\" bar = Column ( Integer , primary_key = True ) foo = deserialize ( Foo , { \"bar\" : 0 }) assert isinstance ( foo , Foo ) assert foo . bar == 0 assert serialize ( foo ) == { \"bar\" : 0 } assert serialization_schema ( Foo ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"type\" : \"object\" , \"properties\" : { \"bar\" : { \"type\" : \"integer\" }}, \"required\" : [ \"bar\" ], \"additionalProperties\" : False , } def foos () -> Collection [ Foo ]: ... schema = graphql_schema ( query = [ foos ]) schema_str = \"\"\" \\ type Query { foos: [Foo!] } type Foo { bar: Int! } \"\"\" assert print_schema ( schema ) == schema_str There are two differences with regular conversions : The dataclass model computation can be deferred until it's needed. This is because some libraries do some resolutions after class definition (for example SQLAchemy resolves dynamic string references in relationships). So you can replace the following line in the example # dataclass_model(cls)(make_dataclass(cls.__name__, fields)) dataclass_model ( cls )( lambda : make_dataclass ( cls . __name__ , fields )) Serialized methods/properties of the class are automatically added to the dataclass model (but you can also declare serialized methods in the dataclass model).","title":"Dataclass model - automatic conversion from/to dataclass"},{"location":"data_model/","text":"Data model \u00b6 Apischema handle every classes/types you need. By the way, it's done in an additive way, meaning that it doesn't affect your types. PEP 585 \u00b6 With Python 3.9 and PEP 585 , typing is substantially shaken up; all collection types of typing module are now deprecated. Apischema fully support 3.9 and PEP 585, as shown in the different examples. Dataclasses \u00b6 Because the library aims to bring the minimum boilerplate, it's build on the top of standard library. Dataclasses are thus the core structure of the data model. Dataclasses bring the possibility of field customization, with more than just a default value. In addition to the common parameters of dataclasses.field , customization is done with metadata parameter. With some teasing of features presented later: from dataclasses import dataclass , field from apischema import alias , schema from apischema.metadata import required @dataclass class Foo : bar : int = field ( default = 0 , metadata = alias ( \"foo_bar\" ) | schema ( title = \"foo! bar!\" , min = 0 , max = 42 ) | required , ) Note Field's metadata are just an ordinary dict ; Apischema provides some functions to enrich these metadata with it's own keys ( alias(\"foo_bar) is roughly equivalent to `{\"_apischema_alias\": \"foo_bar\"}) and use them when the time comes, but metadata are not reserved to Apischema and other keys can be added. Because PEP 584 is painfully missing before Python 3.9, Apischema metadata use their own subclass of dict just to add | operator for convenience. Dataclasses __post_init__ and field(init=False) are fully supported. Implication of this feature usage is documented in the relative sections. Warning Before 3.8, InitVar is doing type erasure , that's why it's not possible for Apischema to retrieve type information of init variables. To fix this behavior, a field metadata init_var can be used to put back the type of the field ( init_var also accepts stringified type annotations). Standard library types \u00b6 Apischema handle natively most of the types provided by the standard library. They are sorted in the following categories: Primitive \u00b6 str , int , float , bool , None , subclasses of them They correspond to JSON primitive types. Collection \u00b6 collection.abc.Collection ( typing.Collection ) collection.abc.Sequence ( typing.Sequence ) tuple ( typing.Tuple ) collection.abc.MutableSequence ( typing.MutableSequence ) list ( typing.List ) collection.abc.Set ( typing.AbstractSet ) collection.abc.MutableSet ( typing.MutableSet ) frozenset ( typing.FrozenSet ) set ( typing.Set ) They correspond to JSON array and are serialized to list . Some of them are abstract; deserialization will instantiate a concrete child class. For example collection.abc.Sequence will be instantiated with tuple while collection.MutableSequence will be instantiated with list . Mapping \u00b6 collection.abc.Mapping ( typing.Mapping ) collection.abc.MutableMapping ( typing.MutableMapping ) dict ( typing.Dict ) They correpond to JSON object and are serialized to dict . Enumeration \u00b6 enum.Enum subclasses, typing.Literal For Enum , this is the value and not the attribute name that is serialized Typing facilities \u00b6 typing.Optional / typing.Union ( Optional[T] is strictly equivalent to Union[T, None] ) Deserialization select the first matching alternative (see below how to skip some union member ) tuple ( typing.Tuple ) Can be used as collection as well as true tuple, like tuple[str, int] typing.NewType Serialized according to its base type typing.TypedDict , typing.NamedTuple Kind of discount dataclass without field customization typing.Any Untouched by deserialization Other standard library types \u00b6 bytes with str (de)serialization using base64 encoding datetime.datetime datetime.date datetime.time Supported only in 3.7+ with fromisoformat / isoformat Decimal With float (de)serialization ipaddress.IPv4Address ipaddress.IPv4Interface ipaddress.IPv4Network ipaddress.IPv6Address ipaddress.IPv6Interface ipaddress.IPv6Network pathlib.Path re.Pattern ( typing.Pattern ) uuid.UUID With str (de)serialization Generic \u00b6 typing.Generic can be used out of the box like in the following example: from dataclasses import dataclass from typing import Generic , TypeVar from pytest import raises from apischema import ValidationError , deserialize T = TypeVar ( \"T\" ) @dataclass class Box ( Generic [ T ]): content : T shaken : bool = False assert deserialize ( Box [ str ], { \"content\" : \"void\" }) == Box ( \"void\" ) with raises ( ValidationError ): deserialize ( Box [ str ], { \"content\" : 42 }) Recursive types, string annotations and PEP 563 \u00b6 Recursive classes can be typed as they usually do, with or without PEP 563 . Here with string annotations: from dataclasses import dataclass from typing import Optional from apischema import deserialize @dataclass class Node : value : int child : Optional [ \"Node\" ] = None assert deserialize ( Node , { \"value\" : 0 , \"child\" : { \"value\" : 1 }}) == Node ( 0 , Node ( 1 )) Here with PEP 563 (requires 3.7+) from __future__ import annotations from dataclasses import dataclass from typing import Optional from apischema import deserialize @dataclass class Node : value : int child : Optional [ Node ] = None assert deserialize ( Node , { \"value\" : 0 , \"child\" : { \"value\" : 1 }}) == Node ( 0 , Node ( 1 )) Warning To resolve annotations, Apischema uses typing.get_type_hints ; this doesn't work really well when used on objects defined outside of global scope. Warning (minor) Currently, PEP 585 can have surprising behavior when used outside the box, see bpo-41370 null vs undefined \u00b6 Contrary to Javascript, Python doesn't have an undefined equivalent (if we consider None to be null equivalent). But it can be useful to distinguish (especially when thinkinn about HTTP PATCH method) between a null field and an undefined /absent field. That's why Apischema provides an Undefined constant (a single instance of UndefinedType class) which can be used as a default value everywhere where this distinction is needed. In fact, default values are used when field are absent, thus a default Undefined will mark the field as absent. Dataclass/ NamedTuple fields are ignored by serialization when Undefined . from dataclasses import dataclass from typing import Union from apischema import Undefined , UndefinedType , deserialize , serialize from apischema.json_schema import deserialization_schema @dataclass class Foo : bar : Union [ int , UndefinedType ] = Undefined baz : Union [ int , UndefinedType , None ] = Undefined assert deserialize ( Foo , { \"bar\" : 0 , \"baz\" : None }) == Foo ( 0 , None ) assert deserialize ( Foo , {}) == Foo ( Undefined , Undefined ) assert serialize ( Foo ( Undefined , 42 )) == { \"baz\" : 42 } # Foo.bar and Foo.baz are not required assert deserialization_schema ( Foo ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"type\" : \"object\" , \"properties\" : { \"bar\" : { \"type\" : \"integer\" }, \"baz\" : { \"type\" : [ \"integer\" , \"null\" ]}}, \"additionalProperties\" : False , } Note UndefinedType must only be used inside an Union , as it has no sense as a standalone type. By the way, no suitable name was found to shorten Union[T, UndefinedType] but propositions are welcomed. Note Undefined is a falsy constant, i.e. bool(Undefined) is False . Annotated - PEP 593 \u00b6 PEP 593 is fully supported; annotations stranger to Apischema are simlply ignored. Skip Union member \u00b6 Sometimes, one of the Union members has not to be taken in account during validation; it can just be here to match the type of the default value of the field. This member can be marked as to be skipped with PEP 593 Annotated and apischema.skip.Skip from dataclasses import dataclass from typing import Annotated , Union from pytest import raises from apischema import ValidationError , deserialize , serialize from apischema.skip import Skip @dataclass class Foo : bar : Union [ int , Annotated [ None , Skip ]] = None with raises ( ValidationError ) as err : deserialize ( Foo , { \"bar\" : None }) assert serialize ( err . value ) == [ { \"loc\" : [ \"bar\" ], \"err\" : [ \"expected type integer, found null\" ]} ] Note Skip(schema_only=True) can also be used to skip the member only for JSON schema generation Optional vs. NotNull \u00b6 Optional type is not always appropriate, because it allows deserialized value to be null , but sometimes, you just want None as a default value for unset fields, not an authorized one. That's why Apischema defines a NotNull type; in fact, NotNull = Union[T, Annotated[None, Skip]] . from dataclasses import dataclass from pytest import raises from apischema import ValidationError , deserialize , serialize from apischema.skip import NotNull @dataclass class Foo : # NotNull is exactly like Optional for type checkers, # it's only interpreted differently by Apischema bar : NotNull [ int ] = None with raises ( ValidationError ) as err : deserialize ( Foo , { \"bar\" : None }) assert serialize ( err . value ) == [ { \"loc\" : [ \"bar\" ], \"err\" : [ \"expected type integer, found null\" ]} ] Note You can also use Undefined , but it can be more convenient to directly manipulate an Optional field, especially in the rest of the code unrelated to (de)serialization. Fields set feature can be used to avoid unwanted serialization of a None default value of a NotNull field. Skip dataclass field \u00b6 Dataclass fields can be excluded from Apischema processing by using apischema.metadata.skip in the field metadata from dataclasses import dataclass , field from apischema.json_schema import deserialization_schema from apischema.metadata import skip @dataclass class Foo : bar : int baz : str = field ( default = \"baz\" , metadata = skip ) assert deserialization_schema ( Foo ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"type\" : \"object\" , \"properties\" : { \"bar\" : { \"type\" : \"integer\" }}, \"required\" : [ \"bar\" ], \"additionalProperties\" : False , } Composition over inheritance - composed dataclasses merging \u00b6 Dataclass fields which are themselves dataclass can be \"merged\" into the owning one by using merged metadata. Then, when the class will be (de)serialized, \"merged\" fields will be (de)serialized at the same level than the owning class. from dataclasses import dataclass , field from typing import Union from apischema import Undefined , UndefinedType , alias , deserialize , serialize from apischema.fields import with_fields_set from apischema.json_schema import deserialization_schema from apischema.metadata import merged @dataclass class JsonSchema : title : Union [ str , UndefinedType ] = Undefined description : Union [ str , UndefinedType ] = Undefined format : Union [ str , UndefinedType ] = Undefined ... @with_fields_set @dataclass class RootJsonSchema : schema : Union [ str , UndefinedType ] = field ( default = Undefined , metadata = alias ( \"$schema\" ) ) defs : list [ JsonSchema ] = field ( default_factory = list , metadata = alias ( \"$defs\" )) # This field schema is merged inside the owning one json_schema : JsonSchema = field ( default = JsonSchema (), metadata = merged ) data = { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"title\" : \"merged example\" , } root_schema = RootJsonSchema ( schema = \"http://json-schema.org/draft/2019-09/schema#\" , json_schema = JsonSchema ( title = \"merged example\" ), ) assert deserialize ( RootJsonSchema , data ) == root_schema assert serialize ( root_schema ) == data assert deserialization_schema ( RootJsonSchema ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"$defs\" : { \"JsonSchema\" : { \"type\" : \"object\" , \"properties\" : { \"title\" : { \"type\" : \"string\" }, \"description\" : { \"type\" : \"string\" }, \"format\" : { \"type\" : \"string\" }, }, \"additionalProperties\" : False , } }, \"type\" : \"object\" , # It results in allOf + unevaluatedProperties=False \"allOf\" : [ # RootJsonSchema (without JsonSchema) { \"type\" : \"object\" , \"properties\" : { \"$schema\" : { \"type\" : \"string\" }, \"$defs\" : { \"type\" : \"array\" , \"items\" : { \"$ref\" : \"#/$defs/JsonSchema\" }}, }, \"additionalProperties\" : False , }, # JonsSchema { \"$ref\" : \"#/$defs/JsonSchema\" }, ], \"unevaluatedProperties\" : False , } Note This feature use JSON schema draft 2019-09 unevaluatedProperties keyword . However, this keyword is removed when JSON schema is converted in a version that doesn't support it, like OpenAPI 3.0. This feature is very convenient for building model by composing smaller components. If some kind of reuse could also be achieved with inheritance, it can be less practical when it comes to use it in code, because there is no easy way to build an inherited class when you have an instance of the super class ; you have to copy all the fields by hand. On the other hand, using composition (of merged fields), it's easy to instantiate the class when the smaller component is just a field of it. Custom types / ORM \u00b6 See conversion in order to support every possible types in a few lines of code. Unsupported types \u00b6 When Apischema encounters a type that it doesn't support, Unsupported exception will be raised. from pytest import raises from apischema import Unsupported , deserialize , serialize class Foo : pass with raises ( Unsupported ): deserialize ( Foo , {}) with raises ( Unsupported ): serialize ( Foo ()) See conversion section to make Apischema support all your classes. FAQ \u00b6 Why Iterable is not handled with other collection type? \u00b6 Iterable could be handled (actually, it was at the beginning), however, this doesn't really make sense from a data point of view. Iterable are computation objects, they can be infinite, etc. They don't correspond to a serialized data; Collection is way more appropriate in this context. What happens if I override dataclass __init__ ? \u00b6 Apischema always assumes that dataclass __init__ can be called with with all its fields as kwargs parameters. If that's no more the case after a modification of __init__ (what means if an exception is thrown when the constructor is called because of bad parameters), Apischema treats then the class as not supported .","title":"Data model"},{"location":"data_model/#data-model","text":"Apischema handle every classes/types you need. By the way, it's done in an additive way, meaning that it doesn't affect your types.","title":"Data model"},{"location":"data_model/#pep-585","text":"With Python 3.9 and PEP 585 , typing is substantially shaken up; all collection types of typing module are now deprecated. Apischema fully support 3.9 and PEP 585, as shown in the different examples.","title":"PEP 585"},{"location":"data_model/#dataclasses","text":"Because the library aims to bring the minimum boilerplate, it's build on the top of standard library. Dataclasses are thus the core structure of the data model. Dataclasses bring the possibility of field customization, with more than just a default value. In addition to the common parameters of dataclasses.field , customization is done with metadata parameter. With some teasing of features presented later: from dataclasses import dataclass , field from apischema import alias , schema from apischema.metadata import required @dataclass class Foo : bar : int = field ( default = 0 , metadata = alias ( \"foo_bar\" ) | schema ( title = \"foo! bar!\" , min = 0 , max = 42 ) | required , ) Note Field's metadata are just an ordinary dict ; Apischema provides some functions to enrich these metadata with it's own keys ( alias(\"foo_bar) is roughly equivalent to `{\"_apischema_alias\": \"foo_bar\"}) and use them when the time comes, but metadata are not reserved to Apischema and other keys can be added. Because PEP 584 is painfully missing before Python 3.9, Apischema metadata use their own subclass of dict just to add | operator for convenience. Dataclasses __post_init__ and field(init=False) are fully supported. Implication of this feature usage is documented in the relative sections. Warning Before 3.8, InitVar is doing type erasure , that's why it's not possible for Apischema to retrieve type information of init variables. To fix this behavior, a field metadata init_var can be used to put back the type of the field ( init_var also accepts stringified type annotations).","title":"Dataclasses"},{"location":"data_model/#standard-library-types","text":"Apischema handle natively most of the types provided by the standard library. They are sorted in the following categories:","title":"Standard library types"},{"location":"data_model/#primitive","text":"str , int , float , bool , None , subclasses of them They correspond to JSON primitive types.","title":"Primitive"},{"location":"data_model/#collection","text":"collection.abc.Collection ( typing.Collection ) collection.abc.Sequence ( typing.Sequence ) tuple ( typing.Tuple ) collection.abc.MutableSequence ( typing.MutableSequence ) list ( typing.List ) collection.abc.Set ( typing.AbstractSet ) collection.abc.MutableSet ( typing.MutableSet ) frozenset ( typing.FrozenSet ) set ( typing.Set ) They correspond to JSON array and are serialized to list . Some of them are abstract; deserialization will instantiate a concrete child class. For example collection.abc.Sequence will be instantiated with tuple while collection.MutableSequence will be instantiated with list .","title":"Collection"},{"location":"data_model/#mapping","text":"collection.abc.Mapping ( typing.Mapping ) collection.abc.MutableMapping ( typing.MutableMapping ) dict ( typing.Dict ) They correpond to JSON object and are serialized to dict .","title":"Mapping"},{"location":"data_model/#enumeration","text":"enum.Enum subclasses, typing.Literal For Enum , this is the value and not the attribute name that is serialized","title":"Enumeration"},{"location":"data_model/#typing-facilities","text":"typing.Optional / typing.Union ( Optional[T] is strictly equivalent to Union[T, None] ) Deserialization select the first matching alternative (see below how to skip some union member ) tuple ( typing.Tuple ) Can be used as collection as well as true tuple, like tuple[str, int] typing.NewType Serialized according to its base type typing.TypedDict , typing.NamedTuple Kind of discount dataclass without field customization typing.Any Untouched by deserialization","title":"Typing facilities"},{"location":"data_model/#other-standard-library-types","text":"bytes with str (de)serialization using base64 encoding datetime.datetime datetime.date datetime.time Supported only in 3.7+ with fromisoformat / isoformat Decimal With float (de)serialization ipaddress.IPv4Address ipaddress.IPv4Interface ipaddress.IPv4Network ipaddress.IPv6Address ipaddress.IPv6Interface ipaddress.IPv6Network pathlib.Path re.Pattern ( typing.Pattern ) uuid.UUID With str (de)serialization","title":"Other standard library types"},{"location":"data_model/#generic","text":"typing.Generic can be used out of the box like in the following example: from dataclasses import dataclass from typing import Generic , TypeVar from pytest import raises from apischema import ValidationError , deserialize T = TypeVar ( \"T\" ) @dataclass class Box ( Generic [ T ]): content : T shaken : bool = False assert deserialize ( Box [ str ], { \"content\" : \"void\" }) == Box ( \"void\" ) with raises ( ValidationError ): deserialize ( Box [ str ], { \"content\" : 42 })","title":"Generic"},{"location":"data_model/#recursive-types-string-annotations-and-pep-563","text":"Recursive classes can be typed as they usually do, with or without PEP 563 . Here with string annotations: from dataclasses import dataclass from typing import Optional from apischema import deserialize @dataclass class Node : value : int child : Optional [ \"Node\" ] = None assert deserialize ( Node , { \"value\" : 0 , \"child\" : { \"value\" : 1 }}) == Node ( 0 , Node ( 1 )) Here with PEP 563 (requires 3.7+) from __future__ import annotations from dataclasses import dataclass from typing import Optional from apischema import deserialize @dataclass class Node : value : int child : Optional [ Node ] = None assert deserialize ( Node , { \"value\" : 0 , \"child\" : { \"value\" : 1 }}) == Node ( 0 , Node ( 1 )) Warning To resolve annotations, Apischema uses typing.get_type_hints ; this doesn't work really well when used on objects defined outside of global scope. Warning (minor) Currently, PEP 585 can have surprising behavior when used outside the box, see bpo-41370","title":"Recursive types, string annotations and PEP 563"},{"location":"data_model/#null-vs-undefined","text":"Contrary to Javascript, Python doesn't have an undefined equivalent (if we consider None to be null equivalent). But it can be useful to distinguish (especially when thinkinn about HTTP PATCH method) between a null field and an undefined /absent field. That's why Apischema provides an Undefined constant (a single instance of UndefinedType class) which can be used as a default value everywhere where this distinction is needed. In fact, default values are used when field are absent, thus a default Undefined will mark the field as absent. Dataclass/ NamedTuple fields are ignored by serialization when Undefined . from dataclasses import dataclass from typing import Union from apischema import Undefined , UndefinedType , deserialize , serialize from apischema.json_schema import deserialization_schema @dataclass class Foo : bar : Union [ int , UndefinedType ] = Undefined baz : Union [ int , UndefinedType , None ] = Undefined assert deserialize ( Foo , { \"bar\" : 0 , \"baz\" : None }) == Foo ( 0 , None ) assert deserialize ( Foo , {}) == Foo ( Undefined , Undefined ) assert serialize ( Foo ( Undefined , 42 )) == { \"baz\" : 42 } # Foo.bar and Foo.baz are not required assert deserialization_schema ( Foo ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"type\" : \"object\" , \"properties\" : { \"bar\" : { \"type\" : \"integer\" }, \"baz\" : { \"type\" : [ \"integer\" , \"null\" ]}}, \"additionalProperties\" : False , } Note UndefinedType must only be used inside an Union , as it has no sense as a standalone type. By the way, no suitable name was found to shorten Union[T, UndefinedType] but propositions are welcomed. Note Undefined is a falsy constant, i.e. bool(Undefined) is False .","title":"null vs undefined"},{"location":"data_model/#annotated-pep-593","text":"PEP 593 is fully supported; annotations stranger to Apischema are simlply ignored.","title":"Annotated - PEP 593"},{"location":"data_model/#skip-union-member","text":"Sometimes, one of the Union members has not to be taken in account during validation; it can just be here to match the type of the default value of the field. This member can be marked as to be skipped with PEP 593 Annotated and apischema.skip.Skip from dataclasses import dataclass from typing import Annotated , Union from pytest import raises from apischema import ValidationError , deserialize , serialize from apischema.skip import Skip @dataclass class Foo : bar : Union [ int , Annotated [ None , Skip ]] = None with raises ( ValidationError ) as err : deserialize ( Foo , { \"bar\" : None }) assert serialize ( err . value ) == [ { \"loc\" : [ \"bar\" ], \"err\" : [ \"expected type integer, found null\" ]} ] Note Skip(schema_only=True) can also be used to skip the member only for JSON schema generation","title":"Skip Union member"},{"location":"data_model/#optional-vs-notnull","text":"Optional type is not always appropriate, because it allows deserialized value to be null , but sometimes, you just want None as a default value for unset fields, not an authorized one. That's why Apischema defines a NotNull type; in fact, NotNull = Union[T, Annotated[None, Skip]] . from dataclasses import dataclass from pytest import raises from apischema import ValidationError , deserialize , serialize from apischema.skip import NotNull @dataclass class Foo : # NotNull is exactly like Optional for type checkers, # it's only interpreted differently by Apischema bar : NotNull [ int ] = None with raises ( ValidationError ) as err : deserialize ( Foo , { \"bar\" : None }) assert serialize ( err . value ) == [ { \"loc\" : [ \"bar\" ], \"err\" : [ \"expected type integer, found null\" ]} ] Note You can also use Undefined , but it can be more convenient to directly manipulate an Optional field, especially in the rest of the code unrelated to (de)serialization. Fields set feature can be used to avoid unwanted serialization of a None default value of a NotNull field.","title":"Optional vs. NotNull"},{"location":"data_model/#skip-dataclass-field","text":"Dataclass fields can be excluded from Apischema processing by using apischema.metadata.skip in the field metadata from dataclasses import dataclass , field from apischema.json_schema import deserialization_schema from apischema.metadata import skip @dataclass class Foo : bar : int baz : str = field ( default = \"baz\" , metadata = skip ) assert deserialization_schema ( Foo ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"type\" : \"object\" , \"properties\" : { \"bar\" : { \"type\" : \"integer\" }}, \"required\" : [ \"bar\" ], \"additionalProperties\" : False , }","title":"Skip dataclass field"},{"location":"data_model/#composition-over-inheritance-composed-dataclasses-merging","text":"Dataclass fields which are themselves dataclass can be \"merged\" into the owning one by using merged metadata. Then, when the class will be (de)serialized, \"merged\" fields will be (de)serialized at the same level than the owning class. from dataclasses import dataclass , field from typing import Union from apischema import Undefined , UndefinedType , alias , deserialize , serialize from apischema.fields import with_fields_set from apischema.json_schema import deserialization_schema from apischema.metadata import merged @dataclass class JsonSchema : title : Union [ str , UndefinedType ] = Undefined description : Union [ str , UndefinedType ] = Undefined format : Union [ str , UndefinedType ] = Undefined ... @with_fields_set @dataclass class RootJsonSchema : schema : Union [ str , UndefinedType ] = field ( default = Undefined , metadata = alias ( \"$schema\" ) ) defs : list [ JsonSchema ] = field ( default_factory = list , metadata = alias ( \"$defs\" )) # This field schema is merged inside the owning one json_schema : JsonSchema = field ( default = JsonSchema (), metadata = merged ) data = { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"title\" : \"merged example\" , } root_schema = RootJsonSchema ( schema = \"http://json-schema.org/draft/2019-09/schema#\" , json_schema = JsonSchema ( title = \"merged example\" ), ) assert deserialize ( RootJsonSchema , data ) == root_schema assert serialize ( root_schema ) == data assert deserialization_schema ( RootJsonSchema ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"$defs\" : { \"JsonSchema\" : { \"type\" : \"object\" , \"properties\" : { \"title\" : { \"type\" : \"string\" }, \"description\" : { \"type\" : \"string\" }, \"format\" : { \"type\" : \"string\" }, }, \"additionalProperties\" : False , } }, \"type\" : \"object\" , # It results in allOf + unevaluatedProperties=False \"allOf\" : [ # RootJsonSchema (without JsonSchema) { \"type\" : \"object\" , \"properties\" : { \"$schema\" : { \"type\" : \"string\" }, \"$defs\" : { \"type\" : \"array\" , \"items\" : { \"$ref\" : \"#/$defs/JsonSchema\" }}, }, \"additionalProperties\" : False , }, # JonsSchema { \"$ref\" : \"#/$defs/JsonSchema\" }, ], \"unevaluatedProperties\" : False , } Note This feature use JSON schema draft 2019-09 unevaluatedProperties keyword . However, this keyword is removed when JSON schema is converted in a version that doesn't support it, like OpenAPI 3.0. This feature is very convenient for building model by composing smaller components. If some kind of reuse could also be achieved with inheritance, it can be less practical when it comes to use it in code, because there is no easy way to build an inherited class when you have an instance of the super class ; you have to copy all the fields by hand. On the other hand, using composition (of merged fields), it's easy to instantiate the class when the smaller component is just a field of it.","title":"Composition over inheritance - composed dataclasses merging"},{"location":"data_model/#custom-types-orm","text":"See conversion in order to support every possible types in a few lines of code.","title":"Custom types / ORM"},{"location":"data_model/#unsupported-types","text":"When Apischema encounters a type that it doesn't support, Unsupported exception will be raised. from pytest import raises from apischema import Unsupported , deserialize , serialize class Foo : pass with raises ( Unsupported ): deserialize ( Foo , {}) with raises ( Unsupported ): serialize ( Foo ()) See conversion section to make Apischema support all your classes.","title":"Unsupported types"},{"location":"data_model/#faq","text":"","title":"FAQ"},{"location":"data_model/#why-iterable-is-not-handled-with-other-collection-type","text":"Iterable could be handled (actually, it was at the beginning), however, this doesn't really make sense from a data point of view. Iterable are computation objects, they can be infinite, etc. They don't correspond to a serialized data; Collection is way more appropriate in this context.","title":"Why Iterable is not handled with other collection type?"},{"location":"data_model/#what-happens-if-i-override-dataclass-__init__","text":"Apischema always assumes that dataclass __init__ can be called with with all its fields as kwargs parameters. If that's no more the case after a modification of __init__ (what means if an exception is thrown when the constructor is called because of bad parameters), Apischema treats then the class as not supported .","title":"What happens if I override dataclass __init__?"},{"location":"de_serialization/","text":"(De)serialization \u00b6 Apischema aims to help API with deserialization/serialization of data, mostly JSON. Let start again with the overview example from collections.abc import Collection from dataclasses import dataclass , field from uuid import UUID , uuid4 from graphql import print_schema from pytest import raises from apischema import ValidationError , deserialize , serialize from apischema.graphql import graphql_schema from apischema.json_schema import deserialization_schema # Define a schema with standard dataclasses @dataclass class Resource : id : UUID name : str tags : set [ str ] = field ( default_factory = set ) # Get some data uuid = uuid4 () data = { \"id\" : str ( uuid ), \"name\" : \"wyfo\" , \"tags\" : [ \"some_tag\" ]} # Deserialize data resource = deserialize ( Resource , data ) assert resource == Resource ( uuid , \"wyfo\" , { \"some_tag\" }) # Serialize objects assert serialize ( resource ) == data # Validate during deserialization with raises ( ValidationError ) as err : # pytest check exception is raised deserialize ( Resource , { \"id\" : \"42\" , \"name\" : \"wyfo\" }) assert serialize ( err . value ) == [ # ValidationError is serializable { \"loc\" : [ \"id\" ], \"err\" : [ \"badly formed hexadecimal UUID string\" ]} ] # Generate JSON Schema assert deserialization_schema ( Resource ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"type\" : \"object\" , \"properties\" : { \"id\" : { \"type\" : \"string\" , \"format\" : \"uuid\" }, \"name\" : { \"type\" : \"string\" }, \"tags\" : { \"type\" : \"array\" , \"items\" : { \"type\" : \"string\" }, \"uniqueItems\" : True }, }, \"required\" : [ \"id\" , \"name\" ], \"additionalProperties\" : False , } # Define GraphQL operations def resources ( tags : Collection [ str ] = None ) -> Collection [ Resource ]: ... # Generate GraphQL schema schema = graphql_schema ( query = [ resources ], id_types = { UUID }) schema_str = \"\"\" \\ type Query { resources(tags: [String!]): [Resource!] } type Resource { id: ID! name: String! tags: [String!]! } \"\"\" assert print_schema ( schema ) == schema_str Deserialization \u00b6 Deserialization is done through the function apischema.deserialize with the following simplified signature: def deserialize ( cls : Type [ T ], data : Any ) -> T : ... cls can be a dataclass as well as a list[int] a NewType , or whatever you want (see conversions to extend deserialization support to every type you want). data must be a JSON-like serialized data: dict / list / str / int / float / bool / None , in short, what you get when you execute json.loads . Deserialization performs a validation of data, based on typing annotations and other information (see schema and validation ). from collections.abc import Collection, Mapping from dataclasses import dataclass from typing import NewType from apischema import deserialize @dataclass class Foo: bar: str MyInt = NewType(\"MyInt\", int) assert deserialize(Foo, {\"bar\": \"bar\"}) == Foo(\"bar\") assert deserialize(MyInt, 0) == MyInt(0) == 0 assert deserialize(Mapping[str, Collection[Foo]], {\"key\": [{\"bar\": \"42\"}]}) == { \"key\": (Foo(\"42\"),) } Strictness \u00b6 Coercion \u00b6 Apischema is strict by default. You ask for an integer, you have to receive an integer. However, in some cases, data has to be be coerced, for example when parsing aconfiguration file. That can be done using coerce parameter; when set to True , all primitive types will be coerce to the expected type of the data model like the following: from pytest import raises from apischema import ValidationError , deserialize with raises ( ValidationError ): deserialize ( bool , \"ok\" ) assert deserialize ( bool , \"ok\" , coercion = True ) bool can be coerced from str with the following case-insensitive mapping: False True 0 1 f t n y no yes false true off on ko ok Note bool coercion from str is just a global dict[str, bool] named apischema.data.coercion.STR_TO_BOOL and it can be customized according to your need (but keys have to be lower cased). There is also a global set[str] named apischema.data.coercion.STR_NONE_VALUES for None coercion. coercion parameter can also receive a coercion function which will then be used instead of default one. from typing import TypeVar , cast from pytest import raises from apischema import ValidationError , deserialize T = TypeVar ( \"T\" ) def coerce ( cls : type [ T ], data ) -> T : \"\"\"Only coerce int to bool\"\"\" if cls is bool and isinstance ( data , int ): return cast ( T , bool ( data )) else : return data with raises ( ValidationError ): deserialize ( bool , 0 ) with raises ( ValidationError ): assert deserialize ( bool , \"ok\" , coercion = coerce ) assert deserialize ( bool , 1 , coercion = coerce ) Note If coercer result is not an instance of class passed in argument, a ValidationError will be raised with an appropriate error message Warning Coercer first argument is a primitive json type str / bool / int / float / list / dict / type(None) ; it can be type(None) , so returning cls(data) will fail in this case. Additional properties \u00b6 Apischema is strict too about number of fields received for an object . In JSON schema terms, Apischema put \"additionalProperties\": false by default (this can be configured by class with properties field ). This behavior can be controlled by additional_properties parameter. When set to True , it prevents the reject of unexpected properties. from dataclasses import dataclass from pytest import raises from apischema import ValidationError , deserialize @dataclass class Foo : bar : str data = { \"bar\" : \"bar\" , \"other\" : 42 } with raises ( ValidationError ): deserialize ( Foo , data ) assert deserialize ( Foo , data , additional_properties = True ) == Foo ( \"bar\" ) Default fallback \u00b6 Validation error can happen when deserializing an ill-formed field. However, if this field has a default value/factory, deserialization can fallback to this default; this is enabled by default_fallback parameter. This behavior can also be configured for each field using metadata. from dataclasses import dataclass , field from pytest import raises from apischema import ValidationError , deserialize from apischema.metadata import default_fallback @dataclass class Foo : bar : str = \"bar\" baz : str = field ( default = \"baz\" , metadata = default_fallback ) with raises ( ValidationError ): deserialize ( Foo , { \"bar\" : 0 }) assert deserialize ( Foo , { \"bar\" : 0 }, default_fallback = True ) == Foo () assert deserialize ( Foo , { \"baz\" : 0 }) == Foo () Strictness configuration \u00b6 Apischema global configuration is managed through apischema.settings module. This module has, among other, three global variables settings.additional_properties , settings.coercion and settings.default_fallback whose values are used as default parameter values for the deserialize function. Global coercion function can be set with settings.coercer following this example: import json from apischema import ValidationError , settings prev_coercer = settings . coercer () @settings . coercer def coercer ( cls , data ): \"\"\"In case of coercion failures, try to deserialize json data\"\"\" try : return prev_coercer ( cls , data ) except ValidationError as err : if not isinstance ( data , str ): raise try : return json . loads ( data ) except json . JSONDecodeError : raise err Note Like all settings function, coercer has an overloaded signature. Without argument, it returns the current settings function, and with an argument, it set the settings function. Fields set \u00b6 Sometimes, it can be useful to know which field has been set by the deserialization, for example in the case of a PATCH requests, to know which field has been updated. Moreover, it is also used in serialization to limit the fields serialized (see next section ) Because Apischema use vanilla dataclasses, this feature is not enabled by default and must be set explicitly on a per-class basis. Apischema provides a simple API to get/set this metadata. from dataclasses import dataclass from typing import Optional from apischema import deserialize from apischema.fields import ( fields_set , is_set , set_fields , unset_fields , with_fields_set , ) # This decorator enable the feature @with_fields_set @dataclass class Foo : bar : int baz : Optional [ str ] = None # Retrieve fields set foo1 = Foo ( 0 , None ) assert fields_set ( foo1 ) == { \"bar\" , \"baz\" } foo2 = Foo ( 0 ) assert fields_set ( foo2 ) == { \"bar\" } # Test fields individually (with autocompletion and refactoring) assert is_set ( foo1 ) . baz assert not is_set ( foo2 ) . baz # Mark fields as set/unset set_fields ( foo2 , \"baz\" ) assert fields_set ( foo2 ) == { \"bar\" , \"baz\" } unset_fields ( foo2 , \"baz\" ) assert fields_set ( foo2 ) == { \"bar\" } set_fields ( foo2 , \"baz\" , overwrite = True ) assert fields_set ( foo2 ) == { \"baz\" } # Fields modification are taken in account foo2 . bar = 0 assert fields_set ( foo2 ) == { \"bar\" , \"baz\" } # Because deserialization use normal constructor, it works with the feature foo3 = deserialize ( Foo , { \"bar\" : 0 }) assert fields_set ( foo3 ) == { \"bar\" } Warning with_fields_set decorator MUST be put above dataclass one. This is because both of them modify __init__ method, but only the first is built to take the second in account. Warning dataclasses.replace works by setting all the fields of the replaced object. Because of this issue, Apischema provides a little wrapper apischema.dataclasses.replace . Serialization \u00b6 Serialization is simpler than deserialization; serialize(obj) will generate a JSON-like serialized obj . There is no validation, objects provided are trusted \u2014 they are supposed to be statically type-checked. When there from dataclasses import dataclass from apischema import serialize @dataclass class Foo : bar : str assert serialize ( Foo ( \"bar\" )) == { \"bar\" : \"bar\" } assert serialize (( 0 , 1 )) == [ 0 , 1 ] assert serialize ({ \"key\" : ( \"value\" , 42 )}) == { \"key\" : [ \"value\" , 42 ]} Serialized methods/properties \u00b6 Apischema can execute methods/properties during serialization and add the computed values with the other fields values; just put apischema.serialized decorator on top of methods/properties you want to be serialized. from dataclasses import dataclass from apischema import serialize , serialized from apischema.json_schema import serialization_schema @dataclass class Foo : @serialized @property def bar ( self ) -> int : return 0 @serialized def baz ( self , some_arg_with_default : str = \"\" ) -> str : return some_arg_with_default @serialized ( \"aliased\" ) @property def aliased_property ( self ) -> bool : return True assert serialize ( Foo ()) == { \"bar\" : 0 , \"baz\" : \"\" , \"aliased\" : True } assert serialization_schema ( Foo ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"type\" : \"object\" , \"properties\" : { \"bar\" : { \"readOnly\" : True , \"type\" : \"integer\" }, \"baz\" : { \"readOnly\" : True , \"type\" : \"string\" }, \"aliased\" : { \"readOnly\" : True , \"type\" : \"boolean\" }, }, \"additionalProperties\" : False , } Note The serialized methods must not have parameters without default, as Apischema need to execute them without arguments Exclude unset fields \u00b6 When a class has a lot of optional fields, it can be convenient to not include all of them, to avoid a bunch of useless fields in your serialized data. Using the previous feature of fields set tracking , serialize can exclude unset fields using its exclude_unset parameter; this parameter is defaulted to True . from dataclasses import dataclass from typing import Optional from apischema import serialize from apischema.fields import with_fields_set # Decorator needed to benefit from the feature @with_fields_set @dataclass class Foo : bar : int baz : Optional [ str ] = None assert serialize ( Foo ( 0 )) == { \"bar\" : 0 } assert serialize ( Foo ( 0 ), exclude_unset = False ) == { \"bar\" : 0 , \"baz\" : None } Note As written in comment in the example, with_fields_set is necessary to benefit from the feature. If the dataclass don't use it, the feature will have no effect. Sometimes, some fields must be serialized, even with their default value; this behavior can be enforced using field metadata. With it, field will be marked as set even if its default value is used at initialization. from dataclasses import dataclass , field from typing import Optional from apischema import serialize from apischema.fields import with_fields_set from apischema.metadata import default_as_set # Decorator needed to benefit from the feature @with_fields_set @dataclass class Foo : bar : Optional [ int ] = field ( default = None , metadata = default_as_set ) assert serialize ( Foo ()) == { \"bar\" : None } assert serialize ( Foo ( 0 )) == { \"bar\" : 0 } Note This metadata has effect only in combination with with_fields_set decorator. FAQ \u00b6 Why coercion is not default behavior? \u00b6 Because ill-formed data can be symptomatic of problems, and it has been decided that highlighting them would be better than hiding them. By the way, this is easily globally configurable. Why with_fields_set feature is not enable by default? \u00b6 It's true that this feature has the little cost of adding a decorator everywhere. However, keeping dataclass decorator allows IDEs/linters/type checkers/etc. to handle the class as such, so there is no need to develop a plugin for them. Standard compliance can be worth the additional decorator. (And little overhead can be avoided when not useful)","title":"(De)serialization"},{"location":"de_serialization/#deserialization","text":"Apischema aims to help API with deserialization/serialization of data, mostly JSON. Let start again with the overview example from collections.abc import Collection from dataclasses import dataclass , field from uuid import UUID , uuid4 from graphql import print_schema from pytest import raises from apischema import ValidationError , deserialize , serialize from apischema.graphql import graphql_schema from apischema.json_schema import deserialization_schema # Define a schema with standard dataclasses @dataclass class Resource : id : UUID name : str tags : set [ str ] = field ( default_factory = set ) # Get some data uuid = uuid4 () data = { \"id\" : str ( uuid ), \"name\" : \"wyfo\" , \"tags\" : [ \"some_tag\" ]} # Deserialize data resource = deserialize ( Resource , data ) assert resource == Resource ( uuid , \"wyfo\" , { \"some_tag\" }) # Serialize objects assert serialize ( resource ) == data # Validate during deserialization with raises ( ValidationError ) as err : # pytest check exception is raised deserialize ( Resource , { \"id\" : \"42\" , \"name\" : \"wyfo\" }) assert serialize ( err . value ) == [ # ValidationError is serializable { \"loc\" : [ \"id\" ], \"err\" : [ \"badly formed hexadecimal UUID string\" ]} ] # Generate JSON Schema assert deserialization_schema ( Resource ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"type\" : \"object\" , \"properties\" : { \"id\" : { \"type\" : \"string\" , \"format\" : \"uuid\" }, \"name\" : { \"type\" : \"string\" }, \"tags\" : { \"type\" : \"array\" , \"items\" : { \"type\" : \"string\" }, \"uniqueItems\" : True }, }, \"required\" : [ \"id\" , \"name\" ], \"additionalProperties\" : False , } # Define GraphQL operations def resources ( tags : Collection [ str ] = None ) -> Collection [ Resource ]: ... # Generate GraphQL schema schema = graphql_schema ( query = [ resources ], id_types = { UUID }) schema_str = \"\"\" \\ type Query { resources(tags: [String!]): [Resource!] } type Resource { id: ID! name: String! tags: [String!]! } \"\"\" assert print_schema ( schema ) == schema_str","title":"(De)serialization"},{"location":"de_serialization/#deserialization_1","text":"Deserialization is done through the function apischema.deserialize with the following simplified signature: def deserialize ( cls : Type [ T ], data : Any ) -> T : ... cls can be a dataclass as well as a list[int] a NewType , or whatever you want (see conversions to extend deserialization support to every type you want). data must be a JSON-like serialized data: dict / list / str / int / float / bool / None , in short, what you get when you execute json.loads . Deserialization performs a validation of data, based on typing annotations and other information (see schema and validation ). from collections.abc import Collection, Mapping from dataclasses import dataclass from typing import NewType from apischema import deserialize @dataclass class Foo: bar: str MyInt = NewType(\"MyInt\", int) assert deserialize(Foo, {\"bar\": \"bar\"}) == Foo(\"bar\") assert deserialize(MyInt, 0) == MyInt(0) == 0 assert deserialize(Mapping[str, Collection[Foo]], {\"key\": [{\"bar\": \"42\"}]}) == { \"key\": (Foo(\"42\"),) }","title":"Deserialization"},{"location":"de_serialization/#strictness","text":"","title":"Strictness"},{"location":"de_serialization/#coercion","text":"Apischema is strict by default. You ask for an integer, you have to receive an integer. However, in some cases, data has to be be coerced, for example when parsing aconfiguration file. That can be done using coerce parameter; when set to True , all primitive types will be coerce to the expected type of the data model like the following: from pytest import raises from apischema import ValidationError , deserialize with raises ( ValidationError ): deserialize ( bool , \"ok\" ) assert deserialize ( bool , \"ok\" , coercion = True ) bool can be coerced from str with the following case-insensitive mapping: False True 0 1 f t n y no yes false true off on ko ok Note bool coercion from str is just a global dict[str, bool] named apischema.data.coercion.STR_TO_BOOL and it can be customized according to your need (but keys have to be lower cased). There is also a global set[str] named apischema.data.coercion.STR_NONE_VALUES for None coercion. coercion parameter can also receive a coercion function which will then be used instead of default one. from typing import TypeVar , cast from pytest import raises from apischema import ValidationError , deserialize T = TypeVar ( \"T\" ) def coerce ( cls : type [ T ], data ) -> T : \"\"\"Only coerce int to bool\"\"\" if cls is bool and isinstance ( data , int ): return cast ( T , bool ( data )) else : return data with raises ( ValidationError ): deserialize ( bool , 0 ) with raises ( ValidationError ): assert deserialize ( bool , \"ok\" , coercion = coerce ) assert deserialize ( bool , 1 , coercion = coerce ) Note If coercer result is not an instance of class passed in argument, a ValidationError will be raised with an appropriate error message Warning Coercer first argument is a primitive json type str / bool / int / float / list / dict / type(None) ; it can be type(None) , so returning cls(data) will fail in this case.","title":"Coercion"},{"location":"de_serialization/#additional-properties","text":"Apischema is strict too about number of fields received for an object . In JSON schema terms, Apischema put \"additionalProperties\": false by default (this can be configured by class with properties field ). This behavior can be controlled by additional_properties parameter. When set to True , it prevents the reject of unexpected properties. from dataclasses import dataclass from pytest import raises from apischema import ValidationError , deserialize @dataclass class Foo : bar : str data = { \"bar\" : \"bar\" , \"other\" : 42 } with raises ( ValidationError ): deserialize ( Foo , data ) assert deserialize ( Foo , data , additional_properties = True ) == Foo ( \"bar\" )","title":"Additional properties"},{"location":"de_serialization/#default-fallback","text":"Validation error can happen when deserializing an ill-formed field. However, if this field has a default value/factory, deserialization can fallback to this default; this is enabled by default_fallback parameter. This behavior can also be configured for each field using metadata. from dataclasses import dataclass , field from pytest import raises from apischema import ValidationError , deserialize from apischema.metadata import default_fallback @dataclass class Foo : bar : str = \"bar\" baz : str = field ( default = \"baz\" , metadata = default_fallback ) with raises ( ValidationError ): deserialize ( Foo , { \"bar\" : 0 }) assert deserialize ( Foo , { \"bar\" : 0 }, default_fallback = True ) == Foo () assert deserialize ( Foo , { \"baz\" : 0 }) == Foo ()","title":"Default fallback"},{"location":"de_serialization/#strictness-configuration","text":"Apischema global configuration is managed through apischema.settings module. This module has, among other, three global variables settings.additional_properties , settings.coercion and settings.default_fallback whose values are used as default parameter values for the deserialize function. Global coercion function can be set with settings.coercer following this example: import json from apischema import ValidationError , settings prev_coercer = settings . coercer () @settings . coercer def coercer ( cls , data ): \"\"\"In case of coercion failures, try to deserialize json data\"\"\" try : return prev_coercer ( cls , data ) except ValidationError as err : if not isinstance ( data , str ): raise try : return json . loads ( data ) except json . JSONDecodeError : raise err Note Like all settings function, coercer has an overloaded signature. Without argument, it returns the current settings function, and with an argument, it set the settings function.","title":"Strictness configuration"},{"location":"de_serialization/#fields-set","text":"Sometimes, it can be useful to know which field has been set by the deserialization, for example in the case of a PATCH requests, to know which field has been updated. Moreover, it is also used in serialization to limit the fields serialized (see next section ) Because Apischema use vanilla dataclasses, this feature is not enabled by default and must be set explicitly on a per-class basis. Apischema provides a simple API to get/set this metadata. from dataclasses import dataclass from typing import Optional from apischema import deserialize from apischema.fields import ( fields_set , is_set , set_fields , unset_fields , with_fields_set , ) # This decorator enable the feature @with_fields_set @dataclass class Foo : bar : int baz : Optional [ str ] = None # Retrieve fields set foo1 = Foo ( 0 , None ) assert fields_set ( foo1 ) == { \"bar\" , \"baz\" } foo2 = Foo ( 0 ) assert fields_set ( foo2 ) == { \"bar\" } # Test fields individually (with autocompletion and refactoring) assert is_set ( foo1 ) . baz assert not is_set ( foo2 ) . baz # Mark fields as set/unset set_fields ( foo2 , \"baz\" ) assert fields_set ( foo2 ) == { \"bar\" , \"baz\" } unset_fields ( foo2 , \"baz\" ) assert fields_set ( foo2 ) == { \"bar\" } set_fields ( foo2 , \"baz\" , overwrite = True ) assert fields_set ( foo2 ) == { \"baz\" } # Fields modification are taken in account foo2 . bar = 0 assert fields_set ( foo2 ) == { \"bar\" , \"baz\" } # Because deserialization use normal constructor, it works with the feature foo3 = deserialize ( Foo , { \"bar\" : 0 }) assert fields_set ( foo3 ) == { \"bar\" } Warning with_fields_set decorator MUST be put above dataclass one. This is because both of them modify __init__ method, but only the first is built to take the second in account. Warning dataclasses.replace works by setting all the fields of the replaced object. Because of this issue, Apischema provides a little wrapper apischema.dataclasses.replace .","title":"Fields set"},{"location":"de_serialization/#serialization","text":"Serialization is simpler than deserialization; serialize(obj) will generate a JSON-like serialized obj . There is no validation, objects provided are trusted \u2014 they are supposed to be statically type-checked. When there from dataclasses import dataclass from apischema import serialize @dataclass class Foo : bar : str assert serialize ( Foo ( \"bar\" )) == { \"bar\" : \"bar\" } assert serialize (( 0 , 1 )) == [ 0 , 1 ] assert serialize ({ \"key\" : ( \"value\" , 42 )}) == { \"key\" : [ \"value\" , 42 ]}","title":"Serialization"},{"location":"de_serialization/#serialized-methodsproperties","text":"Apischema can execute methods/properties during serialization and add the computed values with the other fields values; just put apischema.serialized decorator on top of methods/properties you want to be serialized. from dataclasses import dataclass from apischema import serialize , serialized from apischema.json_schema import serialization_schema @dataclass class Foo : @serialized @property def bar ( self ) -> int : return 0 @serialized def baz ( self , some_arg_with_default : str = \"\" ) -> str : return some_arg_with_default @serialized ( \"aliased\" ) @property def aliased_property ( self ) -> bool : return True assert serialize ( Foo ()) == { \"bar\" : 0 , \"baz\" : \"\" , \"aliased\" : True } assert serialization_schema ( Foo ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"type\" : \"object\" , \"properties\" : { \"bar\" : { \"readOnly\" : True , \"type\" : \"integer\" }, \"baz\" : { \"readOnly\" : True , \"type\" : \"string\" }, \"aliased\" : { \"readOnly\" : True , \"type\" : \"boolean\" }, }, \"additionalProperties\" : False , } Note The serialized methods must not have parameters without default, as Apischema need to execute them without arguments","title":"Serialized methods/properties"},{"location":"de_serialization/#exclude-unset-fields","text":"When a class has a lot of optional fields, it can be convenient to not include all of them, to avoid a bunch of useless fields in your serialized data. Using the previous feature of fields set tracking , serialize can exclude unset fields using its exclude_unset parameter; this parameter is defaulted to True . from dataclasses import dataclass from typing import Optional from apischema import serialize from apischema.fields import with_fields_set # Decorator needed to benefit from the feature @with_fields_set @dataclass class Foo : bar : int baz : Optional [ str ] = None assert serialize ( Foo ( 0 )) == { \"bar\" : 0 } assert serialize ( Foo ( 0 ), exclude_unset = False ) == { \"bar\" : 0 , \"baz\" : None } Note As written in comment in the example, with_fields_set is necessary to benefit from the feature. If the dataclass don't use it, the feature will have no effect. Sometimes, some fields must be serialized, even with their default value; this behavior can be enforced using field metadata. With it, field will be marked as set even if its default value is used at initialization. from dataclasses import dataclass , field from typing import Optional from apischema import serialize from apischema.fields import with_fields_set from apischema.metadata import default_as_set # Decorator needed to benefit from the feature @with_fields_set @dataclass class Foo : bar : Optional [ int ] = field ( default = None , metadata = default_as_set ) assert serialize ( Foo ()) == { \"bar\" : None } assert serialize ( Foo ( 0 )) == { \"bar\" : 0 } Note This metadata has effect only in combination with with_fields_set decorator.","title":"Exclude unset fields"},{"location":"de_serialization/#faq","text":"","title":"FAQ"},{"location":"de_serialization/#why-coercion-is-not-default-behavior","text":"Because ill-formed data can be symptomatic of problems, and it has been decided that highlighting them would be better than hiding them. By the way, this is easily globally configurable.","title":"Why coercion is not default behavior?"},{"location":"de_serialization/#why-with_fields_set-feature-is-not-enable-by-default","text":"It's true that this feature has the little cost of adding a decorator everywhere. However, keeping dataclass decorator allows IDEs/linters/type checkers/etc. to handle the class as such, so there is no need to develop a plugin for them. Standard compliance can be worth the additional decorator. (And little overhead can be avoided when not useful)","title":"Why with_fields_set feature is not enable by default?"},{"location":"json_schema/","text":"JSON schema \u00b6 JSON schema generation \u00b6 JSON schema can be generated from data model. However, because of all possible customizations , schema can be differ between deserilialization and serialization. In common cases, deserialization_schema and serialization_schema will give the same result. from dataclasses import dataclass from apischema.json_schema import deserialization_schema , serialization_schema @dataclass class Foo : bar : str assert deserialization_schema ( Foo ) == serialization_schema ( Foo ) assert deserialization_schema ( Foo ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"additionalProperties\" : False , \"properties\" : { \"bar\" : { \"type\" : \"string\" }}, \"required\" : [ \"bar\" ], \"type\" : \"object\" , } Field alias \u00b6 Sometimes dataclass field names can clash with language keyword, sometimes the property name is not convenient. Hopefully, field can define an alias which will be used in schema and deserialization/serialization. from dataclasses import dataclass , field from apischema import alias , deserialize , serialize from apischema.json_schema import deserialization_schema @dataclass class Foo : class_ : str = field ( metadata = alias ( \"class\" )) assert deserialization_schema ( Foo ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"additionalProperties\" : False , \"properties\" : { \"class\" : { \"type\" : \"string\" }}, \"required\" : [ \"class\" ], \"type\" : \"object\" , } assert deserialize ( Foo , { \"class\" : \"bar\" }) == Foo ( \"bar\" ) assert serialize ( Foo ( \"bar\" )) == { \"class\" : \"bar\" } Alias all fields \u00b6 Field aliasing can also be done at class level by specifying an aliasing function. This aliaser is applied to field alias if defined or field name, or not applied if override=False is specified. from dataclasses import dataclass , field from typing import Any from apischema import alias from apischema.json_schema import deserialization_schema @alias ( lambda s : f \"foo_ { s } \" ) @dataclass class Foo : field1 : Any field2 : Any = field ( metadata = alias ( override = False )) field3 : Any = field ( metadata = alias ( \"field03\" )) field4 : Any = field ( metadata = alias ( \"field04\" , override = False )) assert deserialization_schema ( Foo ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"additionalProperties\" : False , \"properties\" : { \"foo_field1\" : {}, \"field2\" : {}, \"foo_field03\" : {}, \"field04\" : {}}, \"required\" : [ \"foo_field1\" , \"field2\" , \"foo_field03\" , \"field04\" ], \"type\" : \"object\" , } Class-level aliasing can be used to define a camelCase API. Dynamic aliasing and default aliaser \u00b6 Apischema operations deserialize / serialize / deserialization_schema / serialization_schema provide an aliaser parameter which will be applied on every fields being processed in this operation. Similar to strictness configuration , this parameter has a default value controlled by apischema.settings.aliaser . It can be used for example to make all an application use camelCase . Actually, there is a shortcut for that: Otherwise, it's used the same way than settings.coercer . from apischema import settings settings . aliaser ( camel_case = True ) Note NamedTuple fields are also alias, but not TypedDict ones; in fact, TypedDict is not a true class so it cannot be identified to apply aliaser during serialization. Note Dynamic aliaser ignores override=False Schema annotations \u00b6 Type annotations are not enough to express a complete schema, but Apischema has a function for that; schema can be used both as type decorator or field metadata. from dataclasses import dataclass , field from typing import NewType from apischema import schema from apischema.json_schema import deserialization_schema Tag = NewType ( \"Tag\" , str ) schema ( min_len = 3 , pattern = r \"^\\w*$\" , examples = [ \"available\" , \"EMEA\" ])( Tag ) @dataclass class Resource : id : int tags : list [ Tag ] = field ( default_factory = list , metadata = schema ( description = \"regroup multiple resources\" , max_items = 3 , unique = True ), ) assert deserialization_schema ( Resource ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"additionalProperties\" : False , \"properties\" : { \"id\" : { \"type\" : \"integer\" }, \"tags\" : { \"description\" : \"regroup multiple resources\" , \"items\" : { \"examples\" : [ \"available\" , \"EMEA\" ], \"minLength\" : 3 , \"pattern\" : \"^ \\\\ w*$\" , \"type\" : \"string\" , }, \"maxItems\" : 3 , \"type\" : \"array\" , \"uniqueItems\" : True , }, }, \"required\" : [ \"id\" ], \"type\" : \"object\" , } Note Schema are particularly useful with NewType . For example, if you use prefixed ids, you can use a NewType with a pattern schema to validate them, and benefit of more precise type checking. The following keys are available (they are sometimes shorten compared to JSON schema original for code concision and snake_case): Key JSON schema keyword type restriction title / / description / / default / / examples / / min minimum int max maximum int exc_min exclusiveMinimum int exc_max exclusiveMaximum int mult_of multipleOf int format / str min_len minLength str max_len maxLength str pattern / str min_items minItems list max_items maxItems list unique / list min_props minProperties dict max_props maxProperties dict Note schema function has an overloaded signature which prevents to mix incompatible keywords. Two other arguments enable a finer control of the JSON schema generated : extra enable to add arbitrary keys to schema; override=True prevents Apischema to use the annotated type schema, using only schema annotation. from typing import NewType from apischema import schema from apischema.json_schema import deserialization_schema Foo = NewType ( \"Foo\" , str ) schema ( min_len = 20 , extra = { \"typeName\" : \"Foo\" })( Foo ) Bar = NewType ( \"Bar\" , int ) schema ( extra = { \"$ref\" : \"http://some-domain.org/path/tp/schema.json#/$defs/Bar\" }, override = True , )( Bar ) assert deserialization_schema ( Foo ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"type\" : \"string\" , \"minLength\" : 20 , \"typeName\" : \"Foo\" , } assert deserialization_schema ( Bar ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"$ref\" : \"http://some-domain.org/path/tp/schema.json#/$defs/Bar\" , } # Without override=True, it would be { # \"$schema\": \"http://json-schema.org/draft/2019-09/schema#\", # \"$ref\": \"http://some-domain.org/path/tp/schema.json#/$defs/Bar\", # \"type\": \"integer\", # } default annotation \u00b6 default annotation is not added automatically when a field has a default value (see FAQ ); schema default parameter must be used in order to make it appear in the schema. However ... can be used as a placeholder to make Apischema use field default value; this one will be serialized \u2014 if serialization fails, error will be ignored as well as default annotation. Constraints validation \u00b6 JSON schema constrains the data deserialized; this constraints are naturally used for validation. from dataclasses import dataclass , field from typing import NewType from pytest import raises from apischema import ValidationError , deserialize , schema , serialize Tag = NewType ( \"Tag\" , str ) schema ( min_len = 3 , pattern = r \"^\\w*$\" , examples = [ \"available\" , \"EMEA\" ])( Tag ) @dataclass class Resource : id : int tags : list [ Tag ] = field ( default_factory = list , metadata = schema ( description = \"regroup multiple resources\" , max_items = 3 , unique = True ), ) with raises ( ValidationError ) as err : # pytest check exception is raised deserialize ( Resource , { \"id\" : 42 , \"tags\" : [ \"tag\" , \"duplicate\" , \"duplicate\" , \"bad&\" , \"_\" ]} ) assert serialize ( err . value ) == [ { \"loc\" : [ \"tags\" ], \"err\" : [ \"size greater than 3 (maxItems)\" , \"duplicate items (uniqueItems)\" ], }, { \"loc\" : [ \"tags\" , 3 ], \"err\" : [ \"'^ \\\\ w*$' not matched (pattern)\" ]}, { \"loc\" : [ \"tags\" , 4 ], \"err\" : [ \"length less than 3 (minLength)\" ]}, ] Default schema \u00b6 When no schema are defined, a default schema can be computed using settings.default_schema like this: from typing import Optional from apischema import schema , settings from apischema.json_schema.schema import Schema @settings . default_schema def default_schema ( cls ) -> Optional [ Schema ]: if not ... : return None return schema ( ... ) Default implementation returns None for every types. Required field with default value \u00b6 By default, a dataclass/namedtuple field will be tagged required if it doesn't have a default value. However, you may want to have a default value for a field in order to be more convenient in your code, but still make the field required. One could think about some schema model where version is fixed but is required, for example JSON-RPC with \"jsonrpc\": \"2.0\" . That's done with field metadata required . from dataclasses import dataclass , field from typing import Optional from pytest import raises from apischema import ValidationError , deserialize , serialize from apischema.metadata import required @dataclass class Foo : bar : Optional [ int ] = field ( default = None , metadata = required ) with raises ( ValidationError ) as err : deserialize ( Foo , {}) assert serialize ( err . value ) == [{ \"loc\" : [ \"bar\" ], \"err\" : [ \"missing property\" ]}] Additional properties / pattern properties \u00b6 With Mapping \u00b6 Schema of a Mapping / dict type is naturally translated to \"additionalProperties\": <schema of the value type> . However when the schema of the key has a pattern , it will give a \"patternProperties\": {<key pattern>: <schema of the value type>} With dataclass \u00b6 additionalProperties / patternProperties can be added to dataclasses by using fields annotated with properties metadata. Properties not mapped on regular fields will be deserialized into this fields; they must have a Mapping type (or be convertible from Mapping ) because they are instanciated with a mapping. from collections.abc import Mapping from dataclasses import dataclass , field from typing import Annotated from apischema import deserialize , properties , schema , schema_ref from apischema.json_schema import deserialization_schema @schema_ref ( None ) @dataclass class Config : active : bool = True server_options : Mapping [ str , bool ] = field ( default_factory = dict , metadata = properties ( pattern = r \"^server_\" ) ) client_options : Mapping [ Annotated [ str , schema ( pattern = r \"^client_\" )], bool # noqa: F722 ] = field ( default_factory = dict , metadata = properties ( ... )) options : Mapping [ str , bool ] = field ( default_factory = dict , metadata = properties ) assert deserialize ( Config , { \"use_lightsaber\" : True , \"server_auto_restart\" : False , \"client_timeout\" : False }, ) == Config ( True , { \"server_auto_restart\" : False }, { \"client_timeout\" : False }, { \"use_lightsaber\" : True }, ) assert deserialization_schema ( Config ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"type\" : \"object\" , \"properties\" : { \"active\" : { \"type\" : \"boolean\" }}, \"additionalProperties\" : { \"type\" : \"boolean\" }, \"patternProperties\" : { \"^server_\" : { \"type\" : \"boolean\" }, \"^client_\" : { \"type\" : \"boolean\" }, }, } Note Of course, a dataclass can only have a single properties field without pattern, because it makes no sens to have several additionalProperties . Property dependencies \u00b6 Apischema support property dependencies for dataclass through a class member. Dependencies are also used in validation. Note JSON schema draft 2019-09 renames properties dependencies dependentRequired to disambiguate with schema dependencies from dataclasses import dataclass , field from pytest import raises from apischema import ValidationError , deserialize , serialize from apischema.dependent_required import DependentRequired from apischema.json_schema import deserialization_schema from apischema.skip import NotNull @dataclass class Billing : name : str # Fields used in dependencies MUST be declared with `field` credit_card : NotNull [ int ] = field ( default = None ) billing_address : NotNull [ str ] = field ( default = None ) dependencies = DependentRequired ({ credit_card : [ billing_address ]}) assert deserialization_schema ( Billing ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"additionalProperties\" : False , \"dependentRequired\" : { \"credit_card\" : [ \"billing_address\" ]}, \"properties\" : { \"name\" : { \"type\" : \"string\" }, \"credit_card\" : { \"type\" : \"integer\" }, \"billing_address\" : { \"type\" : \"string\" }, }, \"required\" : [ \"name\" ], \"type\" : \"object\" , } with raises ( ValidationError ) as err : deserialize ( Billing , { \"name\" : \"Anonymous\" , \"credit_card\" : 1234_5678_9012_3456 }) assert serialize ( err . value ) == [ { \"loc\" : [ \"billing_address\" ], \"err\" : [ \"missing property (required by ['credit_card'])\" ], } ] Because bidirectional dependencies are a common idiom, Apischema provides a shortcut notation. Its indeed possible to write DependentRequired([credit_card, billing_adress]) . Complex/recursive types - JSON schema definitions/OpenAPI components \u00b6 For complex schema with type reuse, it's convenient to extract definitions of schema components in order to reuse them; it's even mandatory for recursive types. Then, schema use JSON pointers \"$ref\" to refer to the definitions. Apischema handles this feature natively. from dataclasses import dataclass from typing import Optional from apischema.json_schema import deserialization_schema @dataclass class Node : value : int child : Optional [ \"Node\" ] = None assert deserialization_schema ( Node ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"$ref\" : \"#/$defs/Node\" , \"$defs\" : { \"Node\" : { \"type\" : \"object\" , \"properties\" : { \"value\" : { \"type\" : \"integer\" }, \"child\" : { \"anyOf\" : [{ \"$ref\" : \"#/$defs/Node\" }, { \"type\" : \"null\" }]}, }, \"required\" : [ \"value\" ], \"additionalProperties\" : False , } }, } Use ref only for reused types \u00b6 If some types appear only once in the schema, you maybe don't want to use a $ref and a definition but inline the type definition directly. It is possible by setting schema_ref(None) (see next section ) on the the concerned type, but it could affect others schema where this types is reused several time. However, Apischema provides a parameter all_ref for this reason: - all_refs=True -> all types with a reference will be put in the definitions and referenced with $ref ; - all_refs=False -> only types which are reused in the schema are put in definitions all_refs default value depends on the JSON schema version : it's False for JSON schema drafts but True for OpenAPI . from dataclasses import dataclass from apischema.json_schema import deserialization_schema @dataclass class Bar : baz : str @dataclass class Foo : bar1 : Bar bar2 : Bar assert deserialization_schema ( Foo , all_refs = False ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"$defs\" : { \"Bar\" : { \"additionalProperties\" : False , \"properties\" : { \"baz\" : { \"type\" : \"string\" }}, \"required\" : [ \"baz\" ], \"type\" : \"object\" , } }, \"additionalProperties\" : False , \"properties\" : { \"bar1\" : { \"$ref\" : \"#/$defs/Bar\" }, \"bar2\" : { \"$ref\" : \"#/$defs/Bar\" }}, \"required\" : [ \"bar1\" , \"bar2\" ], \"type\" : \"object\" , } assert deserialization_schema ( Foo , all_refs = True ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"$defs\" : { \"Bar\" : { \"additionalProperties\" : False , \"properties\" : { \"baz\" : { \"type\" : \"string\" }}, \"required\" : [ \"baz\" ], \"type\" : \"object\" , }, \"Foo\" : { \"additionalProperties\" : False , \"properties\" : { \"bar1\" : { \"$ref\" : \"#/$defs/Bar\" }, \"bar2\" : { \"$ref\" : \"#/$defs/Bar\" }, }, \"required\" : [ \"bar1\" , \"bar2\" ], \"type\" : \"object\" , }, }, \"$ref\" : \"#/$defs/Foo\" , } Definitions schema only \u00b6 Sometimes you need to extract only the definitions in a separate schema (especially OpenAPI components). That's done with the definitions_schema function. It takes two lists deserializations and serializations of schema (or schema + conversions ) and combines the definitions of all the schema that would have been generated with types given in the list of schema. from dataclasses import dataclass from apischema.json_schema import definitions_schema @dataclass class Bar : baz : int = 0 @dataclass class Foo : bar : Bar assert definitions_schema ( deserialization = [ list [ Foo ]], all_refs = True ) == { \"Foo\" : { \"type\" : \"object\" , \"properties\" : { \"bar\" : { \"$ref\" : \"#/$defs/Bar\" }}, \"required\" : [ \"bar\" ], \"additionalProperties\" : False , }, \"Bar\" : { \"type\" : \"object\" , \"properties\" : { \"baz\" : { \"type\" : \"integer\" }}, \"additionalProperties\" : False , }, } Customize $ref \u00b6 Add $ref to every types \u00b6 In the previous example, only dataclasses has a $ref , but it can be fully customized. You can use schema_ref on any defined types ( NewType / class / Annotated /etc.). schema_ref argument can be: - str -> this string will be used directly in schema generation - ... -> schema generation will use the name of the type - None -> this type will have no $ref in schema from collections.abc import Set from dataclasses import dataclass from typing import NewType from apischema import schema_ref from apischema.json_schema import deserialization_schema Tags = NewType ( \"Tags\" , Set [ str ]) schema_ref ( ... )( Tags ) @dataclass class Resource : id : int tags : Tags assert deserialization_schema ( Resource , all_refs = True ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"$defs\" : { \"Resource\" : { \"type\" : \"object\" , \"properties\" : { \"id\" : { \"type\" : \"integer\" }, \"tags\" : { \"$ref\" : \"#/$defs/Tags\" }}, \"required\" : [ \"id\" , \"tags\" ], \"additionalProperties\" : False , }, \"Tags\" : { \"type\" : \"array\" , \"items\" : { \"type\" : \"string\" }, \"uniqueItems\" : True }, }, \"$ref\" : \"#/$defs/Resource\" , } Note Actually, there is a small restriction with NewType : you cannot put a schema_ref if the super type is not a builtin type ( list[...] / int /etc.). In fact, NewType super type serialization could be affected by different conversions and a same $ref would embed different schema. Default $ref \u00b6 There is a default schema_ref for each type; following types get a ... ref (which means a ref with their name): dataclass NewType TypedDict NamedTuple every types decorated with schema This default behavior is customizable by setting settings.default_ref function like this from apischema import settings @settings . default_ref def default_ref ( cls ): return None # This example remove default ref for every types Ref factory \u00b6 schema_ref is used to set a short ref, like the name of a class, but in schema, $ref looks like #/$defs/Foo . In fact, schema generation use the ref given by schema_ref and pass it to a ref_factory function (a parameter of schema generation functions) which will convert it to its final form. JSON schema version comes with its default ref_factory , for draft 2019-09, it prefixes the ref with #/$defs/ , while it prefixes with #/components/schema in case of OpenAPI . from dataclasses import dataclass from apischema.json_schema import deserialization_schema @dataclass class Foo : bar : int def ref_factory ( ref : str ) -> str : return f \"http://some-domain.org/path/to/ { ref } .json#\" assert deserialization_schema ( Foo , ref_factory = ref_factory ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"$ref\" : \"http://some-domain.org/path/to/Foo.json#\" , } Note When ref_factory is passed in arguments, definitions are not added to the generated schema. That's because ref_factory would surely change definitions location, so there would be no interest to add them with a wrong location. This definitions can of course be generated separately with definitions_schema . Passing ref_factory also give a default value of True for all_refs parameters. JSON schema / OpenAPI version \u00b6 JSON schema has several versions \u2014 OpenAPI is treated as a JSON schema version. If Apischema natively use the last one: draft 2019-09, it is possible to specify a schema version which will be used for the generation. from dataclasses import dataclass from typing import Optional from apischema.json_schema import ( JsonSchemaVersion , definitions_schema , deserialization_schema , ) @dataclass class Bar : baz : Optional [ int ] @dataclass class Foo : bar : Bar assert deserialization_schema ( Foo , all_refs = True ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"$ref\" : \"#/$defs/Foo\" , \"$defs\" : { \"Foo\" : { \"type\" : \"object\" , \"properties\" : { \"bar\" : { \"$ref\" : \"#/$defs/Bar\" }}, \"required\" : [ \"bar\" ], \"additionalProperties\" : False , }, \"Bar\" : { \"type\" : \"object\" , \"properties\" : { \"baz\" : { \"type\" : [ \"integer\" , \"null\" ]}}, \"required\" : [ \"baz\" ], \"additionalProperties\" : False , }, }, } assert deserialization_schema ( Foo , all_refs = True , version = JsonSchemaVersion . DRAFT_7 ) == { \"$schema\" : \"http://json-schema.org/draft-07/schema#\" , # $ref is isolated in allOf + draft 7 prefix \"allOf\" : [{ \"$ref\" : \"#/definitions/Foo\" }], \"definitions\" : { # not \"$defs\" \"Foo\" : { \"type\" : \"object\" , \"properties\" : { \"bar\" : { \"$ref\" : \"#/definitions/Bar\" }}, \"required\" : [ \"bar\" ], \"additionalProperties\" : False , }, \"Bar\" : { \"type\" : \"object\" , \"properties\" : { \"baz\" : { \"type\" : [ \"integer\" , \"null\" ]}}, \"required\" : [ \"baz\" ], \"additionalProperties\" : False , }, }, } assert deserialization_schema ( Foo , version = JsonSchemaVersion . OPEN_API_3_0 ) == { # No definitions for OpenAPI, use refs_schema for it \"$ref\" : \"#/components/schemas/Foo\" , # OpenAPI prefix } assert definitions_schema ( deserialization = [ Foo ], version = JsonSchemaVersion . OPEN_API_3_0 ) == { \"Foo\" : { \"type\" : \"object\" , \"properties\" : { \"bar\" : { \"$ref\" : \"#/components/schemas/Bar\" }}, \"required\" : [ \"bar\" ], \"additionalProperties\" : False , }, \"Bar\" : { \"type\" : \"object\" , # \"nullable\" instead of \"type\": \"null\" \"properties\" : { \"baz\" : { \"type\" : \"integer\" , \"nullable\" : True }}, \"required\" : [ \"baz\" ], \"additionalProperties\" : False , }, } readOnly / writeOnly \u00b6 Dataclasses InitVar and field(init=False) fields will be flagged respectively with \"writeOnly\": true and \"readOnly\": true in the generated schema. If deserialization and serialization schemas both appears in definition_schema , properties are merged and the resulting schema contains then readOnly and writeOnly properties. By the way, the required is not merged because it can't (it would mess up validation if some not-init field was required), so deserialization required is kept because it's more important as it can be used in validation ( OpenAPI 3.0 semantic which allows the merge has been dropped in 3.1, so it has not been judged useful to be supported) FAQ \u00b6 Why field default value is not used by default to to generate JSON schema? \u00b6 Actually, default value is not always the usable in the schema, for example NotNull fields with a None default value. Because default keyword is kind of facultative in the schema, it has been decided to not put it by default in order to not put wrong default by accident.","title":"JSON schema"},{"location":"json_schema/#json-schema","text":"","title":"JSON schema"},{"location":"json_schema/#json-schema-generation","text":"JSON schema can be generated from data model. However, because of all possible customizations , schema can be differ between deserilialization and serialization. In common cases, deserialization_schema and serialization_schema will give the same result. from dataclasses import dataclass from apischema.json_schema import deserialization_schema , serialization_schema @dataclass class Foo : bar : str assert deserialization_schema ( Foo ) == serialization_schema ( Foo ) assert deserialization_schema ( Foo ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"additionalProperties\" : False , \"properties\" : { \"bar\" : { \"type\" : \"string\" }}, \"required\" : [ \"bar\" ], \"type\" : \"object\" , }","title":"JSON schema generation"},{"location":"json_schema/#field-alias","text":"Sometimes dataclass field names can clash with language keyword, sometimes the property name is not convenient. Hopefully, field can define an alias which will be used in schema and deserialization/serialization. from dataclasses import dataclass , field from apischema import alias , deserialize , serialize from apischema.json_schema import deserialization_schema @dataclass class Foo : class_ : str = field ( metadata = alias ( \"class\" )) assert deserialization_schema ( Foo ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"additionalProperties\" : False , \"properties\" : { \"class\" : { \"type\" : \"string\" }}, \"required\" : [ \"class\" ], \"type\" : \"object\" , } assert deserialize ( Foo , { \"class\" : \"bar\" }) == Foo ( \"bar\" ) assert serialize ( Foo ( \"bar\" )) == { \"class\" : \"bar\" }","title":"Field alias"},{"location":"json_schema/#alias-all-fields","text":"Field aliasing can also be done at class level by specifying an aliasing function. This aliaser is applied to field alias if defined or field name, or not applied if override=False is specified. from dataclasses import dataclass , field from typing import Any from apischema import alias from apischema.json_schema import deserialization_schema @alias ( lambda s : f \"foo_ { s } \" ) @dataclass class Foo : field1 : Any field2 : Any = field ( metadata = alias ( override = False )) field3 : Any = field ( metadata = alias ( \"field03\" )) field4 : Any = field ( metadata = alias ( \"field04\" , override = False )) assert deserialization_schema ( Foo ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"additionalProperties\" : False , \"properties\" : { \"foo_field1\" : {}, \"field2\" : {}, \"foo_field03\" : {}, \"field04\" : {}}, \"required\" : [ \"foo_field1\" , \"field2\" , \"foo_field03\" , \"field04\" ], \"type\" : \"object\" , } Class-level aliasing can be used to define a camelCase API.","title":"Alias all fields"},{"location":"json_schema/#dynamic-aliasing-and-default-aliaser","text":"Apischema operations deserialize / serialize / deserialization_schema / serialization_schema provide an aliaser parameter which will be applied on every fields being processed in this operation. Similar to strictness configuration , this parameter has a default value controlled by apischema.settings.aliaser . It can be used for example to make all an application use camelCase . Actually, there is a shortcut for that: Otherwise, it's used the same way than settings.coercer . from apischema import settings settings . aliaser ( camel_case = True ) Note NamedTuple fields are also alias, but not TypedDict ones; in fact, TypedDict is not a true class so it cannot be identified to apply aliaser during serialization. Note Dynamic aliaser ignores override=False","title":"Dynamic aliasing and default aliaser"},{"location":"json_schema/#schema-annotations","text":"Type annotations are not enough to express a complete schema, but Apischema has a function for that; schema can be used both as type decorator or field metadata. from dataclasses import dataclass , field from typing import NewType from apischema import schema from apischema.json_schema import deserialization_schema Tag = NewType ( \"Tag\" , str ) schema ( min_len = 3 , pattern = r \"^\\w*$\" , examples = [ \"available\" , \"EMEA\" ])( Tag ) @dataclass class Resource : id : int tags : list [ Tag ] = field ( default_factory = list , metadata = schema ( description = \"regroup multiple resources\" , max_items = 3 , unique = True ), ) assert deserialization_schema ( Resource ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"additionalProperties\" : False , \"properties\" : { \"id\" : { \"type\" : \"integer\" }, \"tags\" : { \"description\" : \"regroup multiple resources\" , \"items\" : { \"examples\" : [ \"available\" , \"EMEA\" ], \"minLength\" : 3 , \"pattern\" : \"^ \\\\ w*$\" , \"type\" : \"string\" , }, \"maxItems\" : 3 , \"type\" : \"array\" , \"uniqueItems\" : True , }, }, \"required\" : [ \"id\" ], \"type\" : \"object\" , } Note Schema are particularly useful with NewType . For example, if you use prefixed ids, you can use a NewType with a pattern schema to validate them, and benefit of more precise type checking. The following keys are available (they are sometimes shorten compared to JSON schema original for code concision and snake_case): Key JSON schema keyword type restriction title / / description / / default / / examples / / min minimum int max maximum int exc_min exclusiveMinimum int exc_max exclusiveMaximum int mult_of multipleOf int format / str min_len minLength str max_len maxLength str pattern / str min_items minItems list max_items maxItems list unique / list min_props minProperties dict max_props maxProperties dict Note schema function has an overloaded signature which prevents to mix incompatible keywords. Two other arguments enable a finer control of the JSON schema generated : extra enable to add arbitrary keys to schema; override=True prevents Apischema to use the annotated type schema, using only schema annotation. from typing import NewType from apischema import schema from apischema.json_schema import deserialization_schema Foo = NewType ( \"Foo\" , str ) schema ( min_len = 20 , extra = { \"typeName\" : \"Foo\" })( Foo ) Bar = NewType ( \"Bar\" , int ) schema ( extra = { \"$ref\" : \"http://some-domain.org/path/tp/schema.json#/$defs/Bar\" }, override = True , )( Bar ) assert deserialization_schema ( Foo ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"type\" : \"string\" , \"minLength\" : 20 , \"typeName\" : \"Foo\" , } assert deserialization_schema ( Bar ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"$ref\" : \"http://some-domain.org/path/tp/schema.json#/$defs/Bar\" , } # Without override=True, it would be { # \"$schema\": \"http://json-schema.org/draft/2019-09/schema#\", # \"$ref\": \"http://some-domain.org/path/tp/schema.json#/$defs/Bar\", # \"type\": \"integer\", # }","title":"Schema annotations"},{"location":"json_schema/#default-annotation","text":"default annotation is not added automatically when a field has a default value (see FAQ ); schema default parameter must be used in order to make it appear in the schema. However ... can be used as a placeholder to make Apischema use field default value; this one will be serialized \u2014 if serialization fails, error will be ignored as well as default annotation.","title":"default annotation"},{"location":"json_schema/#constraints-validation","text":"JSON schema constrains the data deserialized; this constraints are naturally used for validation. from dataclasses import dataclass , field from typing import NewType from pytest import raises from apischema import ValidationError , deserialize , schema , serialize Tag = NewType ( \"Tag\" , str ) schema ( min_len = 3 , pattern = r \"^\\w*$\" , examples = [ \"available\" , \"EMEA\" ])( Tag ) @dataclass class Resource : id : int tags : list [ Tag ] = field ( default_factory = list , metadata = schema ( description = \"regroup multiple resources\" , max_items = 3 , unique = True ), ) with raises ( ValidationError ) as err : # pytest check exception is raised deserialize ( Resource , { \"id\" : 42 , \"tags\" : [ \"tag\" , \"duplicate\" , \"duplicate\" , \"bad&\" , \"_\" ]} ) assert serialize ( err . value ) == [ { \"loc\" : [ \"tags\" ], \"err\" : [ \"size greater than 3 (maxItems)\" , \"duplicate items (uniqueItems)\" ], }, { \"loc\" : [ \"tags\" , 3 ], \"err\" : [ \"'^ \\\\ w*$' not matched (pattern)\" ]}, { \"loc\" : [ \"tags\" , 4 ], \"err\" : [ \"length less than 3 (minLength)\" ]}, ]","title":"Constraints validation"},{"location":"json_schema/#default-schema","text":"When no schema are defined, a default schema can be computed using settings.default_schema like this: from typing import Optional from apischema import schema , settings from apischema.json_schema.schema import Schema @settings . default_schema def default_schema ( cls ) -> Optional [ Schema ]: if not ... : return None return schema ( ... ) Default implementation returns None for every types.","title":"Default schema"},{"location":"json_schema/#required-field-with-default-value","text":"By default, a dataclass/namedtuple field will be tagged required if it doesn't have a default value. However, you may want to have a default value for a field in order to be more convenient in your code, but still make the field required. One could think about some schema model where version is fixed but is required, for example JSON-RPC with \"jsonrpc\": \"2.0\" . That's done with field metadata required . from dataclasses import dataclass , field from typing import Optional from pytest import raises from apischema import ValidationError , deserialize , serialize from apischema.metadata import required @dataclass class Foo : bar : Optional [ int ] = field ( default = None , metadata = required ) with raises ( ValidationError ) as err : deserialize ( Foo , {}) assert serialize ( err . value ) == [{ \"loc\" : [ \"bar\" ], \"err\" : [ \"missing property\" ]}]","title":"Required field with default value"},{"location":"json_schema/#additional-properties-pattern-properties","text":"","title":"Additional properties / pattern properties"},{"location":"json_schema/#with-mapping","text":"Schema of a Mapping / dict type is naturally translated to \"additionalProperties\": <schema of the value type> . However when the schema of the key has a pattern , it will give a \"patternProperties\": {<key pattern>: <schema of the value type>}","title":"With Mapping"},{"location":"json_schema/#with-dataclass","text":"additionalProperties / patternProperties can be added to dataclasses by using fields annotated with properties metadata. Properties not mapped on regular fields will be deserialized into this fields; they must have a Mapping type (or be convertible from Mapping ) because they are instanciated with a mapping. from collections.abc import Mapping from dataclasses import dataclass , field from typing import Annotated from apischema import deserialize , properties , schema , schema_ref from apischema.json_schema import deserialization_schema @schema_ref ( None ) @dataclass class Config : active : bool = True server_options : Mapping [ str , bool ] = field ( default_factory = dict , metadata = properties ( pattern = r \"^server_\" ) ) client_options : Mapping [ Annotated [ str , schema ( pattern = r \"^client_\" )], bool # noqa: F722 ] = field ( default_factory = dict , metadata = properties ( ... )) options : Mapping [ str , bool ] = field ( default_factory = dict , metadata = properties ) assert deserialize ( Config , { \"use_lightsaber\" : True , \"server_auto_restart\" : False , \"client_timeout\" : False }, ) == Config ( True , { \"server_auto_restart\" : False }, { \"client_timeout\" : False }, { \"use_lightsaber\" : True }, ) assert deserialization_schema ( Config ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"type\" : \"object\" , \"properties\" : { \"active\" : { \"type\" : \"boolean\" }}, \"additionalProperties\" : { \"type\" : \"boolean\" }, \"patternProperties\" : { \"^server_\" : { \"type\" : \"boolean\" }, \"^client_\" : { \"type\" : \"boolean\" }, }, } Note Of course, a dataclass can only have a single properties field without pattern, because it makes no sens to have several additionalProperties .","title":"With dataclass"},{"location":"json_schema/#property-dependencies","text":"Apischema support property dependencies for dataclass through a class member. Dependencies are also used in validation. Note JSON schema draft 2019-09 renames properties dependencies dependentRequired to disambiguate with schema dependencies from dataclasses import dataclass , field from pytest import raises from apischema import ValidationError , deserialize , serialize from apischema.dependent_required import DependentRequired from apischema.json_schema import deserialization_schema from apischema.skip import NotNull @dataclass class Billing : name : str # Fields used in dependencies MUST be declared with `field` credit_card : NotNull [ int ] = field ( default = None ) billing_address : NotNull [ str ] = field ( default = None ) dependencies = DependentRequired ({ credit_card : [ billing_address ]}) assert deserialization_schema ( Billing ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"additionalProperties\" : False , \"dependentRequired\" : { \"credit_card\" : [ \"billing_address\" ]}, \"properties\" : { \"name\" : { \"type\" : \"string\" }, \"credit_card\" : { \"type\" : \"integer\" }, \"billing_address\" : { \"type\" : \"string\" }, }, \"required\" : [ \"name\" ], \"type\" : \"object\" , } with raises ( ValidationError ) as err : deserialize ( Billing , { \"name\" : \"Anonymous\" , \"credit_card\" : 1234_5678_9012_3456 }) assert serialize ( err . value ) == [ { \"loc\" : [ \"billing_address\" ], \"err\" : [ \"missing property (required by ['credit_card'])\" ], } ] Because bidirectional dependencies are a common idiom, Apischema provides a shortcut notation. Its indeed possible to write DependentRequired([credit_card, billing_adress]) .","title":"Property dependencies"},{"location":"json_schema/#complexrecursive-types-json-schema-definitionsopenapi-components","text":"For complex schema with type reuse, it's convenient to extract definitions of schema components in order to reuse them; it's even mandatory for recursive types. Then, schema use JSON pointers \"$ref\" to refer to the definitions. Apischema handles this feature natively. from dataclasses import dataclass from typing import Optional from apischema.json_schema import deserialization_schema @dataclass class Node : value : int child : Optional [ \"Node\" ] = None assert deserialization_schema ( Node ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"$ref\" : \"#/$defs/Node\" , \"$defs\" : { \"Node\" : { \"type\" : \"object\" , \"properties\" : { \"value\" : { \"type\" : \"integer\" }, \"child\" : { \"anyOf\" : [{ \"$ref\" : \"#/$defs/Node\" }, { \"type\" : \"null\" }]}, }, \"required\" : [ \"value\" ], \"additionalProperties\" : False , } }, }","title":"Complex/recursive types - JSON schema definitions/OpenAPI components"},{"location":"json_schema/#use-ref-only-for-reused-types","text":"If some types appear only once in the schema, you maybe don't want to use a $ref and a definition but inline the type definition directly. It is possible by setting schema_ref(None) (see next section ) on the the concerned type, but it could affect others schema where this types is reused several time. However, Apischema provides a parameter all_ref for this reason: - all_refs=True -> all types with a reference will be put in the definitions and referenced with $ref ; - all_refs=False -> only types which are reused in the schema are put in definitions all_refs default value depends on the JSON schema version : it's False for JSON schema drafts but True for OpenAPI . from dataclasses import dataclass from apischema.json_schema import deserialization_schema @dataclass class Bar : baz : str @dataclass class Foo : bar1 : Bar bar2 : Bar assert deserialization_schema ( Foo , all_refs = False ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"$defs\" : { \"Bar\" : { \"additionalProperties\" : False , \"properties\" : { \"baz\" : { \"type\" : \"string\" }}, \"required\" : [ \"baz\" ], \"type\" : \"object\" , } }, \"additionalProperties\" : False , \"properties\" : { \"bar1\" : { \"$ref\" : \"#/$defs/Bar\" }, \"bar2\" : { \"$ref\" : \"#/$defs/Bar\" }}, \"required\" : [ \"bar1\" , \"bar2\" ], \"type\" : \"object\" , } assert deserialization_schema ( Foo , all_refs = True ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"$defs\" : { \"Bar\" : { \"additionalProperties\" : False , \"properties\" : { \"baz\" : { \"type\" : \"string\" }}, \"required\" : [ \"baz\" ], \"type\" : \"object\" , }, \"Foo\" : { \"additionalProperties\" : False , \"properties\" : { \"bar1\" : { \"$ref\" : \"#/$defs/Bar\" }, \"bar2\" : { \"$ref\" : \"#/$defs/Bar\" }, }, \"required\" : [ \"bar1\" , \"bar2\" ], \"type\" : \"object\" , }, }, \"$ref\" : \"#/$defs/Foo\" , }","title":"Use ref only for reused types"},{"location":"json_schema/#definitions-schema-only","text":"Sometimes you need to extract only the definitions in a separate schema (especially OpenAPI components). That's done with the definitions_schema function. It takes two lists deserializations and serializations of schema (or schema + conversions ) and combines the definitions of all the schema that would have been generated with types given in the list of schema. from dataclasses import dataclass from apischema.json_schema import definitions_schema @dataclass class Bar : baz : int = 0 @dataclass class Foo : bar : Bar assert definitions_schema ( deserialization = [ list [ Foo ]], all_refs = True ) == { \"Foo\" : { \"type\" : \"object\" , \"properties\" : { \"bar\" : { \"$ref\" : \"#/$defs/Bar\" }}, \"required\" : [ \"bar\" ], \"additionalProperties\" : False , }, \"Bar\" : { \"type\" : \"object\" , \"properties\" : { \"baz\" : { \"type\" : \"integer\" }}, \"additionalProperties\" : False , }, }","title":"Definitions schema only"},{"location":"json_schema/#customize-ref","text":"","title":"Customize $ref"},{"location":"json_schema/#add-ref-to-every-types","text":"In the previous example, only dataclasses has a $ref , but it can be fully customized. You can use schema_ref on any defined types ( NewType / class / Annotated /etc.). schema_ref argument can be: - str -> this string will be used directly in schema generation - ... -> schema generation will use the name of the type - None -> this type will have no $ref in schema from collections.abc import Set from dataclasses import dataclass from typing import NewType from apischema import schema_ref from apischema.json_schema import deserialization_schema Tags = NewType ( \"Tags\" , Set [ str ]) schema_ref ( ... )( Tags ) @dataclass class Resource : id : int tags : Tags assert deserialization_schema ( Resource , all_refs = True ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"$defs\" : { \"Resource\" : { \"type\" : \"object\" , \"properties\" : { \"id\" : { \"type\" : \"integer\" }, \"tags\" : { \"$ref\" : \"#/$defs/Tags\" }}, \"required\" : [ \"id\" , \"tags\" ], \"additionalProperties\" : False , }, \"Tags\" : { \"type\" : \"array\" , \"items\" : { \"type\" : \"string\" }, \"uniqueItems\" : True }, }, \"$ref\" : \"#/$defs/Resource\" , } Note Actually, there is a small restriction with NewType : you cannot put a schema_ref if the super type is not a builtin type ( list[...] / int /etc.). In fact, NewType super type serialization could be affected by different conversions and a same $ref would embed different schema.","title":"Add $ref to every types"},{"location":"json_schema/#default-ref","text":"There is a default schema_ref for each type; following types get a ... ref (which means a ref with their name): dataclass NewType TypedDict NamedTuple every types decorated with schema This default behavior is customizable by setting settings.default_ref function like this from apischema import settings @settings . default_ref def default_ref ( cls ): return None # This example remove default ref for every types","title":"Default $ref"},{"location":"json_schema/#ref-factory","text":"schema_ref is used to set a short ref, like the name of a class, but in schema, $ref looks like #/$defs/Foo . In fact, schema generation use the ref given by schema_ref and pass it to a ref_factory function (a parameter of schema generation functions) which will convert it to its final form. JSON schema version comes with its default ref_factory , for draft 2019-09, it prefixes the ref with #/$defs/ , while it prefixes with #/components/schema in case of OpenAPI . from dataclasses import dataclass from apischema.json_schema import deserialization_schema @dataclass class Foo : bar : int def ref_factory ( ref : str ) -> str : return f \"http://some-domain.org/path/to/ { ref } .json#\" assert deserialization_schema ( Foo , ref_factory = ref_factory ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"$ref\" : \"http://some-domain.org/path/to/Foo.json#\" , } Note When ref_factory is passed in arguments, definitions are not added to the generated schema. That's because ref_factory would surely change definitions location, so there would be no interest to add them with a wrong location. This definitions can of course be generated separately with definitions_schema . Passing ref_factory also give a default value of True for all_refs parameters.","title":"Ref factory"},{"location":"json_schema/#json-schema-openapi-version","text":"JSON schema has several versions \u2014 OpenAPI is treated as a JSON schema version. If Apischema natively use the last one: draft 2019-09, it is possible to specify a schema version which will be used for the generation. from dataclasses import dataclass from typing import Optional from apischema.json_schema import ( JsonSchemaVersion , definitions_schema , deserialization_schema , ) @dataclass class Bar : baz : Optional [ int ] @dataclass class Foo : bar : Bar assert deserialization_schema ( Foo , all_refs = True ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"$ref\" : \"#/$defs/Foo\" , \"$defs\" : { \"Foo\" : { \"type\" : \"object\" , \"properties\" : { \"bar\" : { \"$ref\" : \"#/$defs/Bar\" }}, \"required\" : [ \"bar\" ], \"additionalProperties\" : False , }, \"Bar\" : { \"type\" : \"object\" , \"properties\" : { \"baz\" : { \"type\" : [ \"integer\" , \"null\" ]}}, \"required\" : [ \"baz\" ], \"additionalProperties\" : False , }, }, } assert deserialization_schema ( Foo , all_refs = True , version = JsonSchemaVersion . DRAFT_7 ) == { \"$schema\" : \"http://json-schema.org/draft-07/schema#\" , # $ref is isolated in allOf + draft 7 prefix \"allOf\" : [{ \"$ref\" : \"#/definitions/Foo\" }], \"definitions\" : { # not \"$defs\" \"Foo\" : { \"type\" : \"object\" , \"properties\" : { \"bar\" : { \"$ref\" : \"#/definitions/Bar\" }}, \"required\" : [ \"bar\" ], \"additionalProperties\" : False , }, \"Bar\" : { \"type\" : \"object\" , \"properties\" : { \"baz\" : { \"type\" : [ \"integer\" , \"null\" ]}}, \"required\" : [ \"baz\" ], \"additionalProperties\" : False , }, }, } assert deserialization_schema ( Foo , version = JsonSchemaVersion . OPEN_API_3_0 ) == { # No definitions for OpenAPI, use refs_schema for it \"$ref\" : \"#/components/schemas/Foo\" , # OpenAPI prefix } assert definitions_schema ( deserialization = [ Foo ], version = JsonSchemaVersion . OPEN_API_3_0 ) == { \"Foo\" : { \"type\" : \"object\" , \"properties\" : { \"bar\" : { \"$ref\" : \"#/components/schemas/Bar\" }}, \"required\" : [ \"bar\" ], \"additionalProperties\" : False , }, \"Bar\" : { \"type\" : \"object\" , # \"nullable\" instead of \"type\": \"null\" \"properties\" : { \"baz\" : { \"type\" : \"integer\" , \"nullable\" : True }}, \"required\" : [ \"baz\" ], \"additionalProperties\" : False , }, }","title":"JSON schema / OpenAPI version"},{"location":"json_schema/#readonly-writeonly","text":"Dataclasses InitVar and field(init=False) fields will be flagged respectively with \"writeOnly\": true and \"readOnly\": true in the generated schema. If deserialization and serialization schemas both appears in definition_schema , properties are merged and the resulting schema contains then readOnly and writeOnly properties. By the way, the required is not merged because it can't (it would mess up validation if some not-init field was required), so deserialization required is kept because it's more important as it can be used in validation ( OpenAPI 3.0 semantic which allows the merge has been dropped in 3.1, so it has not been judged useful to be supported)","title":"readOnly / writeOnly"},{"location":"json_schema/#faq","text":"","title":"FAQ"},{"location":"json_schema/#why-field-default-value-is-not-used-by-default-to-to-generate-json-schema","text":"Actually, default value is not always the usable in the schema, for example NotNull fields with a None default value. Because default keyword is kind of facultative in the schema, it has been decided to not put it by default in order to not put wrong default by accident.","title":"Why field default value is not used by default to to generate JSON schema?"},{"location":"pydantic_difference/","text":"Difference with pydantic \u00b6 The question is often asked, so it is answered in a dedicated section. Here are some the key differences between Apischema and pydantic Apischema is faster \u00b6 pydantic uses Cython to improve its performance (with some side effects in its code); Apischema doesn't need it and is still 1.5x faster \u2014 more than 2x when pydantic is not compiled with Cython. Better performance, and but also with more functionalities: dynamic aliasing , conversions , merged fields , etc. Apischema can generate GraphQL schema from your resolvers \u00b6 Not just a simple printable schema but a complete graphql.GraphQLSchema which can be used to execute your queries/mutations/subscriptions through your resolvers/subscribers, powered by Apischema (de)serialization and conversions features. Types and resolvers can be used both in traditional JSON-oriented API and GraphQL API Apischema uses standard dataclasses and types \u00b6 pydantic uses its own BaseModel class, or it's own pseudo- dataclass , so you are forced to tie all your code the library, and you cannot easily reuse code written in a more standard way or in external libraries. By the way, Pydantic use expressions in typing annotations ( conint , etc.), while it's not recommended and treated as an error by tools like Mypy Apischema doesn't require external plugins for editors, linters, etc. \u00b6 pydantic requires a plugin to allow Mypy to type checked BaseModel and others pydantic singularities (and to not raise errors on it); plugin are also needed for editors. Apischema for its part doesn't have borderline stuff like conint annotations and because it uses standard dataclasses, it doesn't need anything else that dataclass support, which is standard on editors and type checkers. Apischema truly works out-of-the-box with forward type references (especially for recursive model) \u00b6 pydantic requires calling update_forward_refs method on recursive types, while Apischema \"just works\". Apischema doesn't mix up (de)serialization with your code \u00b6 pydantic mix up model constructor with deserializer. That ruins the concept of type checking if you want to instantiate a model from your code. Apischema use dedicated functions for its features, meaning your dataclasses are instanciated normally with type checking. In your code, you manipulate objects; (de)serialization is for input/output. Apischema also doesn't mix up validation of external data with your statically checked code; there is no runtime validation in constructors. Apischema conversions feature allows to support any type defined in your code, but also in external libraries \u00b6 pydantic is limited to the type you define in your own code (and to those it defines in its code); you cannot deserialize directly a bson.ObjectID . You are forced to use pseudo types to overload what you want and by using inheritance (see issue on bson.ObjectId ). Note In fact, you could dynamically add a method __get_validators__ to bson.ObjectID , but that's not intuitive, and it doesn't work with builtin types like collection.deque and other types written in C. Apischema has no limit, and it only requires a few lines of code to support what you want, from bson.ObjectId to SQLAlchemy models by way of builtin and generic like collection.deque , and even pydantic . Here is a comparison of a custom type support: import re from typing import NamedTuple import apischema # Serialization has to be handled in each class which has an RGB field # or at each call of of json method class RGB ( NamedTuple ): red : int green : int blue : int @classmethod def __modify_schema__ ( cls , field_schema ) -> None : field_schema . update ({ \"type\" : \"string\" , \"pattern\" : r \"#[0-9A-Fa-f] {6} \" }) field_schema . pop ( \"items\" , ... ) @classmethod def __get_validators__ ( cls ): yield cls . validate @classmethod def validate ( cls , value ) -> 'RGB' : if not isinstance ( value , str ) or re . fullmatch ( r \"#[0-9A-Fa-f] {6} \" , value ) is None : raise ValueError ( \"Invalid RGB\" ) return RGB ( red = int ( value [ 1 : 3 ], 16 ), green = int ( value [ 3 : 5 ], 16 ), blue = int ( value [ 5 : 7 ], 16 )) # Simplified with apischema @apischema . schema ( pattern = r \"#[0-9A-Fa-f] {6} \" ) class RGB ( NamedTuple ): red : int green : int blue : int @apischema . serializer def to_hexa ( rgb : RGB ) -> str : return f \"# { rgb . red : 02x }{ rgb . green : 02x }{ rgb . blue : 02x } \" @apischema . deserializer def from_hexa ( hexa : str ) -> RGB : return RGB ( int ( hexa [ 1 : 3 ], 16 ), int ( hexa [ 3 : 5 ], 16 ), int ( hexa [ 5 : 7 ], 16 )) Apischema has a functional approach, pydantic has an object one, with its limitations \u00b6 Every functionality of pydantic is a method of BaseModel . You have to have a BaseModel instance to do something, even if you manipulate only an integer. So you have to use complex stuff like root type , and to have BaseModel namespace mixing up with your class namespace. Also, because Python has limited object features (no extensions like in Swift or Kotlin), you cannot use easily types you don't defined yourself. Apischema is functional, it doesn't use method but simple functions, which works for every types. You can also register conversions for any types similarly you would implement a type class in a functional language (or adding an extension in Swift or Kotlin). This approach has far fewer limitations. It also allows to add feature in to Apischema (in the library directly or in a plugin) more easily, without breaking the paradigm; in fact, third-party plugin cannot add methods to BaseModel (without breaking static checking), and if pydantic adds a method, you have to make sure it will not mangle your model namespace. Apischema can use both camelCase and snake_case with the same types \u00b6 While pydantic field aliases are fixed at model creation, Apischema let you choose which aliasing you want at (de)serialization time. It can be convenient if you need to juggle with cases for the same models between frontend and other backend services for example. Apischema allows you to use composition over inheritance \u00b6 Merged fields is a distinctive Apischema feature that is very handy to build complexe model from smaller fragments; you don't have to merge yourself the fields of your fragments in a complex class with a lot of fields, Apischema deal with it for you, and your code is kept simple. Apischema supports Generic in Python 3.6 and without requiring additional stuff \u00b6 pydantic BaseModel cannot be used with generic model, you have to use GenericModel , and it's not supported in Python 3.6. With Apischema , you just write your generic classes normally. Apischema doesn't coerce by default \u00b6 Your API respects its schema. It can also coerce, for example to parse configuration file, and coercion can be adjusted (for example coercing list from comma-separated string). Apischema has a better integration of JSON schema/ OpenAPI \u00b6 With pydantic , if you want to have a nullable field in the generated schema, you have to put nullable into schema extra keywords. Apischema is binded to the last JSON schema version but offers conversion to other version like OpenAPI 3.0 and nullable is added for Optional types. Apischema also support more advanced features like dependentRequired or unevaluatedProperties . Reference handling is also more flexible Apischema can add JSON schema to NewType \u00b6 And that's very convenient; you can use NewType everywhere, to gain a better type checking, a better self-documented code. Apischema validators are regular methods with automatic dependencies management \u00b6 Using regular methods allows to benefit of type checking of fields, where pydantic validators use dynamic stuffs and are not type-checked or have to add redundant type annotations. Apischema validators also have automatic dependencies management. And Apischema directly supports JSON schema property dependencies . Comparison is simple with an example: from dataclasses import dataclass import apischema import pydantic class UserModel ( pydantic . BaseModel ): username : str password1 : str password2 : str @pydantic . root_validator def check_passwords_match ( cls , values ): # What is the type of of values? of values['password1']? # If you rename password1 field, validator will hardly be updated # You also have to test yourself that values are provided pw1 , pw2 = values . get ( 'password1' ), values . get ( 'password2' ) if pw1 is not None and pw2 is not None and pw1 != pw2 : raise ValueError ( 'passwords do not match' ) return values @dataclass class LoginForm : username : str password1 : str password2 : str @apischema . validator def check_password_match ( self ): # Typed checked, simpler, and not executed if error on password1 or password2 if self . password1 != self . password2 : raise ValueError ( 'passwords do not match' ) Apischema supports pydantic \u00b6 It's not a feature, is just the result of 20 lines of code .","title":"Difference with pydantic"},{"location":"pydantic_difference/#difference-with-pydantic","text":"The question is often asked, so it is answered in a dedicated section. Here are some the key differences between Apischema and pydantic","title":"Difference with pydantic"},{"location":"pydantic_difference/#apischema-is-faster","text":"pydantic uses Cython to improve its performance (with some side effects in its code); Apischema doesn't need it and is still 1.5x faster \u2014 more than 2x when pydantic is not compiled with Cython. Better performance, and but also with more functionalities: dynamic aliasing , conversions , merged fields , etc.","title":"Apischema is faster"},{"location":"pydantic_difference/#apischema-can-generate-graphql-schema-from-your-resolvers","text":"Not just a simple printable schema but a complete graphql.GraphQLSchema which can be used to execute your queries/mutations/subscriptions through your resolvers/subscribers, powered by Apischema (de)serialization and conversions features. Types and resolvers can be used both in traditional JSON-oriented API and GraphQL API","title":"Apischema can generate GraphQL schema from your resolvers"},{"location":"pydantic_difference/#apischema-uses-standard-dataclasses-and-types","text":"pydantic uses its own BaseModel class, or it's own pseudo- dataclass , so you are forced to tie all your code the library, and you cannot easily reuse code written in a more standard way or in external libraries. By the way, Pydantic use expressions in typing annotations ( conint , etc.), while it's not recommended and treated as an error by tools like Mypy","title":"Apischema uses standard dataclasses and types"},{"location":"pydantic_difference/#apischema-doesnt-require-external-plugins-for-editors-linters-etc","text":"pydantic requires a plugin to allow Mypy to type checked BaseModel and others pydantic singularities (and to not raise errors on it); plugin are also needed for editors. Apischema for its part doesn't have borderline stuff like conint annotations and because it uses standard dataclasses, it doesn't need anything else that dataclass support, which is standard on editors and type checkers.","title":"Apischema doesn't require external plugins for editors, linters, etc."},{"location":"pydantic_difference/#apischema-truly-works-out-of-the-box-with-forward-type-references-especially-for-recursive-model","text":"pydantic requires calling update_forward_refs method on recursive types, while Apischema \"just works\".","title":"Apischema truly works out-of-the-box with forward type references (especially for recursive model)"},{"location":"pydantic_difference/#apischema-doesnt-mix-up-deserialization-with-your-code","text":"pydantic mix up model constructor with deserializer. That ruins the concept of type checking if you want to instantiate a model from your code. Apischema use dedicated functions for its features, meaning your dataclasses are instanciated normally with type checking. In your code, you manipulate objects; (de)serialization is for input/output. Apischema also doesn't mix up validation of external data with your statically checked code; there is no runtime validation in constructors.","title":"Apischema doesn't mix up (de)serialization with your code"},{"location":"pydantic_difference/#apischema-conversions-feature-allows-to-support-any-type-defined-in-your-code-but-also-in-external-libraries","text":"pydantic is limited to the type you define in your own code (and to those it defines in its code); you cannot deserialize directly a bson.ObjectID . You are forced to use pseudo types to overload what you want and by using inheritance (see issue on bson.ObjectId ). Note In fact, you could dynamically add a method __get_validators__ to bson.ObjectID , but that's not intuitive, and it doesn't work with builtin types like collection.deque and other types written in C. Apischema has no limit, and it only requires a few lines of code to support what you want, from bson.ObjectId to SQLAlchemy models by way of builtin and generic like collection.deque , and even pydantic . Here is a comparison of a custom type support: import re from typing import NamedTuple import apischema # Serialization has to be handled in each class which has an RGB field # or at each call of of json method class RGB ( NamedTuple ): red : int green : int blue : int @classmethod def __modify_schema__ ( cls , field_schema ) -> None : field_schema . update ({ \"type\" : \"string\" , \"pattern\" : r \"#[0-9A-Fa-f] {6} \" }) field_schema . pop ( \"items\" , ... ) @classmethod def __get_validators__ ( cls ): yield cls . validate @classmethod def validate ( cls , value ) -> 'RGB' : if not isinstance ( value , str ) or re . fullmatch ( r \"#[0-9A-Fa-f] {6} \" , value ) is None : raise ValueError ( \"Invalid RGB\" ) return RGB ( red = int ( value [ 1 : 3 ], 16 ), green = int ( value [ 3 : 5 ], 16 ), blue = int ( value [ 5 : 7 ], 16 )) # Simplified with apischema @apischema . schema ( pattern = r \"#[0-9A-Fa-f] {6} \" ) class RGB ( NamedTuple ): red : int green : int blue : int @apischema . serializer def to_hexa ( rgb : RGB ) -> str : return f \"# { rgb . red : 02x }{ rgb . green : 02x }{ rgb . blue : 02x } \" @apischema . deserializer def from_hexa ( hexa : str ) -> RGB : return RGB ( int ( hexa [ 1 : 3 ], 16 ), int ( hexa [ 3 : 5 ], 16 ), int ( hexa [ 5 : 7 ], 16 ))","title":"Apischema conversions feature allows to support any type defined in your code, but also in external libraries"},{"location":"pydantic_difference/#apischema-has-a-functional-approach-pydantic-has-an-object-one-with-its-limitations","text":"Every functionality of pydantic is a method of BaseModel . You have to have a BaseModel instance to do something, even if you manipulate only an integer. So you have to use complex stuff like root type , and to have BaseModel namespace mixing up with your class namespace. Also, because Python has limited object features (no extensions like in Swift or Kotlin), you cannot use easily types you don't defined yourself. Apischema is functional, it doesn't use method but simple functions, which works for every types. You can also register conversions for any types similarly you would implement a type class in a functional language (or adding an extension in Swift or Kotlin). This approach has far fewer limitations. It also allows to add feature in to Apischema (in the library directly or in a plugin) more easily, without breaking the paradigm; in fact, third-party plugin cannot add methods to BaseModel (without breaking static checking), and if pydantic adds a method, you have to make sure it will not mangle your model namespace.","title":"Apischema has a functional approach, pydantic has an object one, with its limitations"},{"location":"pydantic_difference/#apischema-can-use-both-camelcase-and-snake_case-with-the-same-types","text":"While pydantic field aliases are fixed at model creation, Apischema let you choose which aliasing you want at (de)serialization time. It can be convenient if you need to juggle with cases for the same models between frontend and other backend services for example.","title":"Apischema can use both camelCase and snake_case with the same types"},{"location":"pydantic_difference/#apischema-allows-you-to-use-composition-over-inheritance","text":"Merged fields is a distinctive Apischema feature that is very handy to build complexe model from smaller fragments; you don't have to merge yourself the fields of your fragments in a complex class with a lot of fields, Apischema deal with it for you, and your code is kept simple.","title":"Apischema allows you to use composition over inheritance"},{"location":"pydantic_difference/#apischema-supports-generic-in-python-36-and-without-requiring-additional-stuff","text":"pydantic BaseModel cannot be used with generic model, you have to use GenericModel , and it's not supported in Python 3.6. With Apischema , you just write your generic classes normally.","title":"Apischema supports Generic in Python 3.6 and without requiring additional stuff"},{"location":"pydantic_difference/#apischema-doesnt-coerce-by-default","text":"Your API respects its schema. It can also coerce, for example to parse configuration file, and coercion can be adjusted (for example coercing list from comma-separated string).","title":"Apischema doesn't coerce by default"},{"location":"pydantic_difference/#apischema-has-a-better-integration-of-json-schemaopenapi","text":"With pydantic , if you want to have a nullable field in the generated schema, you have to put nullable into schema extra keywords. Apischema is binded to the last JSON schema version but offers conversion to other version like OpenAPI 3.0 and nullable is added for Optional types. Apischema also support more advanced features like dependentRequired or unevaluatedProperties . Reference handling is also more flexible","title":"Apischema has a better integration of JSON schema/OpenAPI"},{"location":"pydantic_difference/#apischema-can-add-json-schema-to-newtype","text":"And that's very convenient; you can use NewType everywhere, to gain a better type checking, a better self-documented code.","title":"Apischema can add JSON schema to NewType"},{"location":"pydantic_difference/#apischema-validators-are-regular-methods-with-automatic-dependencies-management","text":"Using regular methods allows to benefit of type checking of fields, where pydantic validators use dynamic stuffs and are not type-checked or have to add redundant type annotations. Apischema validators also have automatic dependencies management. And Apischema directly supports JSON schema property dependencies . Comparison is simple with an example: from dataclasses import dataclass import apischema import pydantic class UserModel ( pydantic . BaseModel ): username : str password1 : str password2 : str @pydantic . root_validator def check_passwords_match ( cls , values ): # What is the type of of values? of values['password1']? # If you rename password1 field, validator will hardly be updated # You also have to test yourself that values are provided pw1 , pw2 = values . get ( 'password1' ), values . get ( 'password2' ) if pw1 is not None and pw2 is not None and pw1 != pw2 : raise ValueError ( 'passwords do not match' ) return values @dataclass class LoginForm : username : str password1 : str password2 : str @apischema . validator def check_password_match ( self ): # Typed checked, simpler, and not executed if error on password1 or password2 if self . password1 != self . password2 : raise ValueError ( 'passwords do not match' )","title":"Apischema validators are regular methods with automatic dependencies management"},{"location":"pydantic_difference/#apischema-supports-pydantic","text":"It's not a feature, is just the result of 20 lines of code .","title":"Apischema supports pydantic"},{"location":"validation/","text":"Validation \u00b6 Validation is an important part of deserialization. By default, Apischema validate types of data according to typing annotations, and schema constraints. But custom validators can also be add for a more precise validation. Deserialization and validation error \u00b6 ValidationError is raised when validation fails. This exception will contains all the information about the ill-formed part of the data. By the way this exception is also serializable and can be sent back directly to client. from dataclasses import dataclass , field from typing import NewType from pytest import raises from apischema import ValidationError , deserialize , schema , serialize Tag = NewType ( \"Tag\" , str ) schema ( min_len = 3 , pattern = r \"^\\w*$\" , examples = [ \"available\" , \"EMEA\" ])( Tag ) @dataclass class Resource : id : int tags : list [ Tag ] = field ( default_factory = list , metadata = schema ( description = \"regroup multiple resources\" , max_items = 3 , unique = True ), ) with raises ( ValidationError ) as err : # pytest check exception is raised deserialize ( Resource , { \"id\" : 42 , \"tags\" : [ \"tag\" , \"duplicate\" , \"duplicate\" , \"bad&\" , \"_\" ]} ) assert serialize ( err . value ) == [ { \"loc\" : [ \"tags\" ], \"err\" : [ \"size greater than 3 (maxItems)\" , \"duplicate items (uniqueItems)\" ], }, { \"loc\" : [ \"tags\" , 3 ], \"err\" : [ \"'^ \\\\ w*$' not matched (pattern)\" ]}, { \"loc\" : [ \"tags\" , 4 ], \"err\" : [ \"length less than 3 (minLength)\" ]}, ] As shown in the example, Apischema will not stop at the first error met but tries to validate all parts of the data. Dataclass validators \u00b6 Dataclass validation can be completed by custom validators. These are simple decorated methods which will be executed during validation, after all fields having been deserialized. from dataclasses import dataclass from pytest import raises from apischema import ValidationError , deserialize , serialize , validator @dataclass class PasswordForm : password : str confirmation : str @validator def password_match ( self ): # DO NOT use assert if self . password != self . confirmation : raise ValueError ( \"password doesn't match its confirmation\" ) with raises ( ValidationError ) as err : deserialize ( PasswordForm , { \"password\" : \"p455w0rd\" , \"confirmation\" : \"...\" }) assert serialize ( err . value ) == [ { \"loc\" : [], \"err\" : [ \"password doesn't match its confirmation\" ]} ] Warning DO NOT use assert statement to validate external data, never. In fact, this statement is made to be disabled when executed in optimized mode (see documentation ), so validation would be disabled too. This warning doesn't concern only Apischema ; assert is only for internal assertion in debug/development environment. That's why Apischema will not catch AssertionError as a validation error but reraises it, making deserialize fail. Note Validators are alawys executed in order of declaration. Automatic dependencies management \u00b6 It makes no sense to execute a validator using a field that is ill-formed. Hopefully, Apischema is able to compute validator dependencies \u2014 the fields used in validator; validator is executed only if the all its dependencies are ok. from dataclasses import dataclass from pytest import raises from apischema import ValidationError , deserialize , serialize , validator @dataclass class PasswordForm : password : str confirmation : str @validator def password_match ( self ): if self . password != self . confirmation : raise ValueError ( \"password doesn't match its confirmation\" ) with raises ( ValidationError ) as err : deserialize ( PasswordForm , { \"password\" : \"p455w0rd\" }) assert serialize ( err . value ) == [ # validator is not executed because confirmation is missing { \"loc\" : [ \"confirmation\" ], \"err\" : [ \"missing property\" ]} ] Note Despite the fact that validator use self argument, it can be called during validation even if all the fields of the class are not ok and the class not really instantiated. In fact, instance is kind of mocked for validation with only the needed field. Raise more than one error with yield \u00b6 Validation of list field can require to raise several exception, one for each bad elements. With raise , this is not possible, because you can raise only once. However, Apischema provides a way or raising as many errors as needed by using yield . Moreover, with this syntax, it is possible to add a \"path\" to the error to precise its location in the validated data. This path will be added to the loc key of the error. from dataclasses import dataclass from ipaddress import IPv4Address , IPv4Network from pytest import raises from apischema import ValidationError , deserialize , serialize , validator from apischema.fields import fields @dataclass class SubnetIps : subnet : IPv4Network ips : list [ IPv4Address ] @validator def check_ips_in_subnet ( self ): for index , ip in enumerate ( self . ips ): if ip not in self . subnet : # yield <error path>, <error message> yield ( fields ( self ) . ips , index ), \"ip not in subnet\" with raises ( ValidationError ) as err : deserialize ( SubnetIps , { \"subnet\" : \"126.42.18.0/24\" , \"ips\" : [ \"126.42.18.1\" , \"126.42.19.0\" , \"0.0.0.0\" ]}, ) assert serialize ( err . value ) == [ { \"loc\" : [ \"ips\" , 1 ], \"err\" : [ \"ip not in subnet\" ]}, { \"loc\" : [ \"ips\" , 2 ], \"err\" : [ \"ip not in subnet\" ]}, ] Error path \u00b6 In the example, validator yield a tuple of an \"error path\" and the error message. Error path can be: a string an integer (for list indices) a dataclass field (obtained with get_fields ) a tuple of this 3 components. yield can also be used with only an error message. Note For dataclass field error path, it's advised to use get_fields instead of raw string, because it will take in account potential aliasing and it will be easier to rename field with IDE refactoring. Discard \u00b6 If one of your validators fails because a field is corrupted, maybe you don't want following validators to be executed. validator decorator provides a discard parameter to discard fields of the remaining validation. All the remaining validators having discarded fields in dependencies will not be executed. from dataclasses import dataclass , field from pytest import raises from apischema import ValidationError , deserialize , serialize , validator from apischema.fields import fields @dataclass class BoundedValues : bounds : tuple [ int , int ] = field () values : list [ int ] @validator ( discard = bounds ) def bounds_are_sorted ( self ): min_bound , max_bound = self . bounds if min_bound > max_bound : yield fields ( self ) . bounds , \"bounds are not sorted\" @validator def values_dont_exceed_bounds ( self ): min_bound , max_bound = self . bounds for index , value in enumerate ( self . values ): if not min_bound <= value <= max_bound : yield ( fields ( self ) . values , index ), \"value exceeds bounds\" with raises ( ValidationError ) as err : deserialize ( BoundedValues , { \"bounds\" : [ 10 , 0 ], \"values\" : [ - 1 , 2 , 4 ]}) assert serialize ( err . value ) == [ { \"loc\" : [ \"bounds\" ], \"err\" : [ \"bounds are not sorted\" ]} # Without discard, there would have been an other error # {\"loc\": [\"values\", 1], \"err\": [\"value exceeds bounds\"]} ] Field validators \u00b6 At field level \u00b6 Fields are validated according to their types and schema. But it's also possible to add validators to fields from dataclasses import dataclass , field from pytest import raises from apischema import ValidationError , deserialize , serialize from apischema.metadata import validators def check_no_duplicate_digits ( n : int ): if len ( str ( n )) != len ( set ( str ( n ))): raise ValueError ( \"number has duplicate digits\" ) @dataclass class Foo : bar : str = field ( metadata = validators ( check_no_duplicate_digits )) with raises ( ValidationError ) as err : deserialize ( Foo , { \"bar\" : \"11\" }) assert serialize ( err . value ) == [ { \"loc\" : [ \"bar\" ], \"err\" : [ \"number has duplicate digits\" ]} ] When validation fails for a field, it is discarded and cannot be used in class validators, as it is the case when field schema validation fails. Note field_validator allow to reuse the the same validator for several fields. However in this case, using a custom type (for example a NewType ) with validators (see next section ) could often be a better solution. Using other fields \u00b6 A common pattern can be to validate a field using other fields values. This is achieved with dataclass validators seen above. However there is a shortcut for this use case: from dataclasses import dataclass , field from enum import Enum from pytest import raises from apischema import ValidationError , deserialize , serialize , validator from apischema.fields import fields class Parity ( Enum ): EVEN = \"even\" ODD = \"odd\" @dataclass class NumberWithParity : parity : Parity number : int = field () # field must be assign, even with empty `field()` @validator ( number ) def check_parity ( self ): if ( self . parity == Parity . EVEN ) != ( self . number % 2 == 0 ): yield \"number doesn't respect parity\" # using field argument adds automatically discard argument # and prefix all error paths with the field @validator ( discard = number ) def check_parity_equivalent ( self ): if ( self . parity == Parity . EVEN ) != ( self . number % 2 == 0 ): yield fields ( self ) . number , \"number doesn't respect parity\" with raises ( ValidationError ) as err : deserialize ( NumberWithParity , { \"parity\" : \"even\" , \"number\" : 1 }) assert serialize ( err . value ) == [ { \"loc\" : [ \"number\" ], \"err\" : [ \"number doesn't respect parity\" ]} ] Validators inheritance \u00b6 Validators are inherited just like other class fields. from dataclasses import dataclass from pytest import raises from apischema import ValidationError , deserialize , serialize , validator @dataclass class PasswordForm : password : str confirmation : str @validator def password_match ( self ): if self . password != self . confirmation : raise ValueError ( \"password doesn't match its confirmation\" ) @dataclass class CompleteForm ( PasswordForm ): username : str with raises ( ValidationError ) as err : deserialize ( CompleteForm , { \"username\" : \"wyfo\" , \"password\" : \"p455w0rd\" , \"confirmation\" : \"...\" }, ) assert serialize ( err . value ) == [ { \"loc\" : [], \"err\" : [ \"password doesn't match its confirmation\" ]} ] Validator with InitVar \u00b6 Dataclasses InitVar are accessible in validators by using parameters the same way __post_init__ does. Only the needed fields has to be put in parameters, they are then added to validator dependencies. from dataclasses import InitVar , dataclass , field from pytest import raises from apischema import ValidationError , deserialize , serialize , validator from apischema.metadata import init_var @dataclass class Foo : bar : InitVar [ int ] = field ( metadata = init_var ( int )) @validator ( bar ) def validate ( self , bar : int ): if bar < 0 : yield \"negative\" with raises ( ValidationError ) as err : deserialize ( Foo , { \"bar\" : - 1 }) assert serialize ( err . value ) == [{ \"loc\" : [ \"bar\" ], \"err\" : [ \"negative\" ]}] Validators are not run on default values \u00b6 If all validator dependencies are initialized with their defau lt values, they are not run; make sure your default values make sens. from dataclasses import dataclass , field from apischema import deserialize , validator validator_run = False @dataclass class Foo : bar : int = field ( default = 0 ) @validator ( bar ) def password_match ( self ): global validator_run validator_run = True if self . bar < 0 : raise ValueError ( \"negative\" ) deserialize ( Foo , {}) assert not validator_run Validators for every type \u00b6 Validators can be added to other user-defined types. When a user type is deseriarialized (even in case of conversion ), its validators are played. from typing import NewType from pytest import raises from apischema import ValidationError , deserialize , serialize from apischema.validation import add_validator Palindrome = NewType ( \"Palindrome\" , str ) @add_validator ( Palindrome ) def check_palindrome ( s : str ): for i in range ( len ( s ) // 2 ): if s [ i ] != s [ - 1 - i ]: raise ValueError ( \"Not a palindrome\" ) assert deserialize ( Palindrome , \"tacocat\" ) == \"tacocat\" with raises ( ValidationError ) as err : deserialize ( Palindrome , \"not a palindrome\" ) assert serialize ( err . value ) == [{ \"loc\" : [], \"err\" : [ \"Not a palindrome\" ]}] FAQ \u00b6 How are computed validator depedencies? \u00b6 ast.NodeVisitor and the Python black magic begins... Why only validate at deserialization and not at instantiation? \u00b6 Apischema uses type annotations, so every objects used can already be statically type-checked (with Mypy / Pycharm /etc.) at instantiation but also at modification. Why use validators for dataclasses instead of doing validation in __post_init__ ? \u00b6 Actually, validation can completly be done in __post_init__ , there is not problem with that. However, validators offers one thing that cannot be achieved with __post_init__ : they are run before __init__ , so they can validate incomplete data. Moreover, they are only run during deserialization, so they don't add overhead to normal class instantiation.","title":"Validation"},{"location":"validation/#validation","text":"Validation is an important part of deserialization. By default, Apischema validate types of data according to typing annotations, and schema constraints. But custom validators can also be add for a more precise validation.","title":"Validation"},{"location":"validation/#deserialization-and-validation-error","text":"ValidationError is raised when validation fails. This exception will contains all the information about the ill-formed part of the data. By the way this exception is also serializable and can be sent back directly to client. from dataclasses import dataclass , field from typing import NewType from pytest import raises from apischema import ValidationError , deserialize , schema , serialize Tag = NewType ( \"Tag\" , str ) schema ( min_len = 3 , pattern = r \"^\\w*$\" , examples = [ \"available\" , \"EMEA\" ])( Tag ) @dataclass class Resource : id : int tags : list [ Tag ] = field ( default_factory = list , metadata = schema ( description = \"regroup multiple resources\" , max_items = 3 , unique = True ), ) with raises ( ValidationError ) as err : # pytest check exception is raised deserialize ( Resource , { \"id\" : 42 , \"tags\" : [ \"tag\" , \"duplicate\" , \"duplicate\" , \"bad&\" , \"_\" ]} ) assert serialize ( err . value ) == [ { \"loc\" : [ \"tags\" ], \"err\" : [ \"size greater than 3 (maxItems)\" , \"duplicate items (uniqueItems)\" ], }, { \"loc\" : [ \"tags\" , 3 ], \"err\" : [ \"'^ \\\\ w*$' not matched (pattern)\" ]}, { \"loc\" : [ \"tags\" , 4 ], \"err\" : [ \"length less than 3 (minLength)\" ]}, ] As shown in the example, Apischema will not stop at the first error met but tries to validate all parts of the data.","title":"Deserialization and validation error"},{"location":"validation/#dataclass-validators","text":"Dataclass validation can be completed by custom validators. These are simple decorated methods which will be executed during validation, after all fields having been deserialized. from dataclasses import dataclass from pytest import raises from apischema import ValidationError , deserialize , serialize , validator @dataclass class PasswordForm : password : str confirmation : str @validator def password_match ( self ): # DO NOT use assert if self . password != self . confirmation : raise ValueError ( \"password doesn't match its confirmation\" ) with raises ( ValidationError ) as err : deserialize ( PasswordForm , { \"password\" : \"p455w0rd\" , \"confirmation\" : \"...\" }) assert serialize ( err . value ) == [ { \"loc\" : [], \"err\" : [ \"password doesn't match its confirmation\" ]} ] Warning DO NOT use assert statement to validate external data, never. In fact, this statement is made to be disabled when executed in optimized mode (see documentation ), so validation would be disabled too. This warning doesn't concern only Apischema ; assert is only for internal assertion in debug/development environment. That's why Apischema will not catch AssertionError as a validation error but reraises it, making deserialize fail. Note Validators are alawys executed in order of declaration.","title":"Dataclass validators"},{"location":"validation/#automatic-dependencies-management","text":"It makes no sense to execute a validator using a field that is ill-formed. Hopefully, Apischema is able to compute validator dependencies \u2014 the fields used in validator; validator is executed only if the all its dependencies are ok. from dataclasses import dataclass from pytest import raises from apischema import ValidationError , deserialize , serialize , validator @dataclass class PasswordForm : password : str confirmation : str @validator def password_match ( self ): if self . password != self . confirmation : raise ValueError ( \"password doesn't match its confirmation\" ) with raises ( ValidationError ) as err : deserialize ( PasswordForm , { \"password\" : \"p455w0rd\" }) assert serialize ( err . value ) == [ # validator is not executed because confirmation is missing { \"loc\" : [ \"confirmation\" ], \"err\" : [ \"missing property\" ]} ] Note Despite the fact that validator use self argument, it can be called during validation even if all the fields of the class are not ok and the class not really instantiated. In fact, instance is kind of mocked for validation with only the needed field.","title":"Automatic dependencies management"},{"location":"validation/#raise-more-than-one-error-with-yield","text":"Validation of list field can require to raise several exception, one for each bad elements. With raise , this is not possible, because you can raise only once. However, Apischema provides a way or raising as many errors as needed by using yield . Moreover, with this syntax, it is possible to add a \"path\" to the error to precise its location in the validated data. This path will be added to the loc key of the error. from dataclasses import dataclass from ipaddress import IPv4Address , IPv4Network from pytest import raises from apischema import ValidationError , deserialize , serialize , validator from apischema.fields import fields @dataclass class SubnetIps : subnet : IPv4Network ips : list [ IPv4Address ] @validator def check_ips_in_subnet ( self ): for index , ip in enumerate ( self . ips ): if ip not in self . subnet : # yield <error path>, <error message> yield ( fields ( self ) . ips , index ), \"ip not in subnet\" with raises ( ValidationError ) as err : deserialize ( SubnetIps , { \"subnet\" : \"126.42.18.0/24\" , \"ips\" : [ \"126.42.18.1\" , \"126.42.19.0\" , \"0.0.0.0\" ]}, ) assert serialize ( err . value ) == [ { \"loc\" : [ \"ips\" , 1 ], \"err\" : [ \"ip not in subnet\" ]}, { \"loc\" : [ \"ips\" , 2 ], \"err\" : [ \"ip not in subnet\" ]}, ]","title":"Raise more than one error with yield"},{"location":"validation/#error-path","text":"In the example, validator yield a tuple of an \"error path\" and the error message. Error path can be: a string an integer (for list indices) a dataclass field (obtained with get_fields ) a tuple of this 3 components. yield can also be used with only an error message. Note For dataclass field error path, it's advised to use get_fields instead of raw string, because it will take in account potential aliasing and it will be easier to rename field with IDE refactoring.","title":"Error path"},{"location":"validation/#discard","text":"If one of your validators fails because a field is corrupted, maybe you don't want following validators to be executed. validator decorator provides a discard parameter to discard fields of the remaining validation. All the remaining validators having discarded fields in dependencies will not be executed. from dataclasses import dataclass , field from pytest import raises from apischema import ValidationError , deserialize , serialize , validator from apischema.fields import fields @dataclass class BoundedValues : bounds : tuple [ int , int ] = field () values : list [ int ] @validator ( discard = bounds ) def bounds_are_sorted ( self ): min_bound , max_bound = self . bounds if min_bound > max_bound : yield fields ( self ) . bounds , \"bounds are not sorted\" @validator def values_dont_exceed_bounds ( self ): min_bound , max_bound = self . bounds for index , value in enumerate ( self . values ): if not min_bound <= value <= max_bound : yield ( fields ( self ) . values , index ), \"value exceeds bounds\" with raises ( ValidationError ) as err : deserialize ( BoundedValues , { \"bounds\" : [ 10 , 0 ], \"values\" : [ - 1 , 2 , 4 ]}) assert serialize ( err . value ) == [ { \"loc\" : [ \"bounds\" ], \"err\" : [ \"bounds are not sorted\" ]} # Without discard, there would have been an other error # {\"loc\": [\"values\", 1], \"err\": [\"value exceeds bounds\"]} ]","title":"Discard"},{"location":"validation/#field-validators","text":"","title":"Field validators"},{"location":"validation/#at-field-level","text":"Fields are validated according to their types and schema. But it's also possible to add validators to fields from dataclasses import dataclass , field from pytest import raises from apischema import ValidationError , deserialize , serialize from apischema.metadata import validators def check_no_duplicate_digits ( n : int ): if len ( str ( n )) != len ( set ( str ( n ))): raise ValueError ( \"number has duplicate digits\" ) @dataclass class Foo : bar : str = field ( metadata = validators ( check_no_duplicate_digits )) with raises ( ValidationError ) as err : deserialize ( Foo , { \"bar\" : \"11\" }) assert serialize ( err . value ) == [ { \"loc\" : [ \"bar\" ], \"err\" : [ \"number has duplicate digits\" ]} ] When validation fails for a field, it is discarded and cannot be used in class validators, as it is the case when field schema validation fails. Note field_validator allow to reuse the the same validator for several fields. However in this case, using a custom type (for example a NewType ) with validators (see next section ) could often be a better solution.","title":"At field level"},{"location":"validation/#using-other-fields","text":"A common pattern can be to validate a field using other fields values. This is achieved with dataclass validators seen above. However there is a shortcut for this use case: from dataclasses import dataclass , field from enum import Enum from pytest import raises from apischema import ValidationError , deserialize , serialize , validator from apischema.fields import fields class Parity ( Enum ): EVEN = \"even\" ODD = \"odd\" @dataclass class NumberWithParity : parity : Parity number : int = field () # field must be assign, even with empty `field()` @validator ( number ) def check_parity ( self ): if ( self . parity == Parity . EVEN ) != ( self . number % 2 == 0 ): yield \"number doesn't respect parity\" # using field argument adds automatically discard argument # and prefix all error paths with the field @validator ( discard = number ) def check_parity_equivalent ( self ): if ( self . parity == Parity . EVEN ) != ( self . number % 2 == 0 ): yield fields ( self ) . number , \"number doesn't respect parity\" with raises ( ValidationError ) as err : deserialize ( NumberWithParity , { \"parity\" : \"even\" , \"number\" : 1 }) assert serialize ( err . value ) == [ { \"loc\" : [ \"number\" ], \"err\" : [ \"number doesn't respect parity\" ]} ]","title":"Using other fields"},{"location":"validation/#validators-inheritance","text":"Validators are inherited just like other class fields. from dataclasses import dataclass from pytest import raises from apischema import ValidationError , deserialize , serialize , validator @dataclass class PasswordForm : password : str confirmation : str @validator def password_match ( self ): if self . password != self . confirmation : raise ValueError ( \"password doesn't match its confirmation\" ) @dataclass class CompleteForm ( PasswordForm ): username : str with raises ( ValidationError ) as err : deserialize ( CompleteForm , { \"username\" : \"wyfo\" , \"password\" : \"p455w0rd\" , \"confirmation\" : \"...\" }, ) assert serialize ( err . value ) == [ { \"loc\" : [], \"err\" : [ \"password doesn't match its confirmation\" ]} ]","title":"Validators inheritance"},{"location":"validation/#validator-with-initvar","text":"Dataclasses InitVar are accessible in validators by using parameters the same way __post_init__ does. Only the needed fields has to be put in parameters, they are then added to validator dependencies. from dataclasses import InitVar , dataclass , field from pytest import raises from apischema import ValidationError , deserialize , serialize , validator from apischema.metadata import init_var @dataclass class Foo : bar : InitVar [ int ] = field ( metadata = init_var ( int )) @validator ( bar ) def validate ( self , bar : int ): if bar < 0 : yield \"negative\" with raises ( ValidationError ) as err : deserialize ( Foo , { \"bar\" : - 1 }) assert serialize ( err . value ) == [{ \"loc\" : [ \"bar\" ], \"err\" : [ \"negative\" ]}]","title":"Validator with InitVar"},{"location":"validation/#validators-are-not-run-on-default-values","text":"If all validator dependencies are initialized with their defau lt values, they are not run; make sure your default values make sens. from dataclasses import dataclass , field from apischema import deserialize , validator validator_run = False @dataclass class Foo : bar : int = field ( default = 0 ) @validator ( bar ) def password_match ( self ): global validator_run validator_run = True if self . bar < 0 : raise ValueError ( \"negative\" ) deserialize ( Foo , {}) assert not validator_run","title":"Validators are not run on default values"},{"location":"validation/#validators-for-every-type","text":"Validators can be added to other user-defined types. When a user type is deseriarialized (even in case of conversion ), its validators are played. from typing import NewType from pytest import raises from apischema import ValidationError , deserialize , serialize from apischema.validation import add_validator Palindrome = NewType ( \"Palindrome\" , str ) @add_validator ( Palindrome ) def check_palindrome ( s : str ): for i in range ( len ( s ) // 2 ): if s [ i ] != s [ - 1 - i ]: raise ValueError ( \"Not a palindrome\" ) assert deserialize ( Palindrome , \"tacocat\" ) == \"tacocat\" with raises ( ValidationError ) as err : deserialize ( Palindrome , \"not a palindrome\" ) assert serialize ( err . value ) == [{ \"loc\" : [], \"err\" : [ \"Not a palindrome\" ]}]","title":"Validators for every type"},{"location":"validation/#faq","text":"","title":"FAQ"},{"location":"validation/#how-are-computed-validator-depedencies","text":"ast.NodeVisitor and the Python black magic begins...","title":"How are computed validator depedencies?"},{"location":"validation/#why-only-validate-at-deserialization-and-not-at-instantiation","text":"Apischema uses type annotations, so every objects used can already be statically type-checked (with Mypy / Pycharm /etc.) at instantiation but also at modification.","title":"Why only validate at deserialization and not at instantiation?"},{"location":"validation/#why-use-validators-for-dataclasses-instead-of-doing-validation-in-__post_init__","text":"Actually, validation can completly be done in __post_init__ , there is not problem with that. However, validators offers one thing that cannot be achieved with __post_init__ : they are run before __init__ , so they can validate incomplete data. Moreover, they are only run during deserialization, so they don't add overhead to normal class instantiation.","title":"Why use validators for dataclasses instead of doing validation in __post_init__?"},{"location":"examples/attrs_compatibility/","text":"Attrs compatibility \u00b6 from dataclasses import field , fields , make_dataclass from functools import lru_cache from typing import Optional import attr from apischema import deserialize , serialize , settings from apischema.conversions import Conversions , Deserialization , Serialization @lru_cache () def attrs_to_dataclass ( cls : type ) -> type : assert hasattr ( cls , \"__attrs_attrs__\" ) fields_without_default = [ ( a . name , a . type ) for a in cls . __attrs_attrs__ if a . default == attr . NOTHING ] fields_with_default = [ ( a . name , a . type , field ( default = a . default )) for a in cls . __attrs_attrs__ if a . default != attr . NOTHING ] return make_dataclass ( cls . __name__ , fields_without_default + fields_with_default ) prev_deserialization = settings . deserialization () prev_serialization = settings . serialization () @settings . deserialization def deserialization ( cls : type , conversions : Optional [ Conversions ] ) -> Optional [ Deserialization ]: result = prev_deserialization ( cls , conversions ) if result is not None : return result elif hasattr ( cls , \"__attrs_attrs__\" ): source = attrs_to_dataclass ( cls ) def converter ( source_obj ): return cls ( ** { f . name : getattr ( source_obj , f . name ) for f in fields ( source )}) return { source : ( converter , None )} else : return None @settings . serialization def serialization ( cls : type , conversions : Optional [ Conversions ] ) -> Optional [ Serialization ]: result = prev_serialization ( cls , conversions ) if result is not None : return result elif hasattr ( cls , \"__attrs_attrs__\" ): target = attrs_to_dataclass ( cls ) def converter ( obj ): return target ( ** { a . name : getattr ( obj , a . name ) for a in cls . __attrs_attrs__ }) return target , ( converter , None ) else : return None @attr . s class Foo : bar : int = attr . ib () assert deserialize ( Foo , { \"bar\" : 0 }) == Foo ( 0 ) assert serialize ( Foo ( 0 )) == { \"bar\" : 0 }","title":"Attrs compatibility"},{"location":"examples/attrs_compatibility/#attrs-compatibility","text":"from dataclasses import field , fields , make_dataclass from functools import lru_cache from typing import Optional import attr from apischema import deserialize , serialize , settings from apischema.conversions import Conversions , Deserialization , Serialization @lru_cache () def attrs_to_dataclass ( cls : type ) -> type : assert hasattr ( cls , \"__attrs_attrs__\" ) fields_without_default = [ ( a . name , a . type ) for a in cls . __attrs_attrs__ if a . default == attr . NOTHING ] fields_with_default = [ ( a . name , a . type , field ( default = a . default )) for a in cls . __attrs_attrs__ if a . default != attr . NOTHING ] return make_dataclass ( cls . __name__ , fields_without_default + fields_with_default ) prev_deserialization = settings . deserialization () prev_serialization = settings . serialization () @settings . deserialization def deserialization ( cls : type , conversions : Optional [ Conversions ] ) -> Optional [ Deserialization ]: result = prev_deserialization ( cls , conversions ) if result is not None : return result elif hasattr ( cls , \"__attrs_attrs__\" ): source = attrs_to_dataclass ( cls ) def converter ( source_obj ): return cls ( ** { f . name : getattr ( source_obj , f . name ) for f in fields ( source )}) return { source : ( converter , None )} else : return None @settings . serialization def serialization ( cls : type , conversions : Optional [ Conversions ] ) -> Optional [ Serialization ]: result = prev_serialization ( cls , conversions ) if result is not None : return result elif hasattr ( cls , \"__attrs_attrs__\" ): target = attrs_to_dataclass ( cls ) def converter ( obj ): return target ( ** { a . name : getattr ( obj , a . name ) for a in cls . __attrs_attrs__ }) return target , ( converter , None ) else : return None @attr . s class Foo : bar : int = attr . ib () assert deserialize ( Foo , { \"bar\" : 0 }) == Foo ( 0 ) assert serialize ( Foo ( 0 )) == { \"bar\" : 0 }","title":"Attrs compatibility"},{"location":"examples/open_rpc/","text":"OpenRPC \u00b6 from dataclasses import dataclass from typing import Generic , TypeVar , Union from pytest import raises from apischema import ( Undefined , UndefinedType , ValidationError , deserialize , schema , serialize , ) from apischema.json_schema import deserialization_schema T = TypeVar ( \"T\" ) @dataclass class Error ( Exception , Generic [ T ]): code : int description : str data : Union [ T , UndefinedType ] = Undefined @schema ( min_props = 1 , max_props = 1 ) @dataclass class Result ( Generic [ T ]): result : Union [ T , UndefinedType ] = Undefined error : Union [ Error , UndefinedType ] = Undefined def get ( self ) -> T : if self . error is not Undefined : raise self . error else : assert self . result is not Undefined return self . result assert deserialization_schema ( Result [ list [ int ]]) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"additionalProperties\" : False , \"maxProperties\" : 1 , \"minProperties\" : 1 , \"properties\" : { \"result\" : { \"type\" : \"array\" , \"items\" : { \"type\" : \"integer\" }}, \"error\" : { \"additionalProperties\" : False , \"properties\" : { \"code\" : { \"type\" : \"integer\" }, \"description\" : { \"type\" : \"string\" }, \"data\" : {}, }, \"required\" : [ \"code\" , \"description\" ], \"type\" : \"object\" , }, }, \"type\" : \"object\" , } data = { \"result\" : 0 } with raises ( ValidationError ): deserialize ( Result [ str ], data ) result = deserialize ( Result [ int ], data ) assert result == Result ( 0 ) assert result . get () == 0 assert serialize ( result ) == { \"result\" : 0 } error = deserialize ( Result , { \"error\" : { \"code\" : 42 , \"description\" : \"...\" }}) with raises ( Error ) as err : error . get () assert err . value == Error ( 42 , \"...\" )","title":"OpenRPC"},{"location":"examples/open_rpc/#openrpc","text":"from dataclasses import dataclass from typing import Generic , TypeVar , Union from pytest import raises from apischema import ( Undefined , UndefinedType , ValidationError , deserialize , schema , serialize , ) from apischema.json_schema import deserialization_schema T = TypeVar ( \"T\" ) @dataclass class Error ( Exception , Generic [ T ]): code : int description : str data : Union [ T , UndefinedType ] = Undefined @schema ( min_props = 1 , max_props = 1 ) @dataclass class Result ( Generic [ T ]): result : Union [ T , UndefinedType ] = Undefined error : Union [ Error , UndefinedType ] = Undefined def get ( self ) -> T : if self . error is not Undefined : raise self . error else : assert self . result is not Undefined return self . result assert deserialization_schema ( Result [ list [ int ]]) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"additionalProperties\" : False , \"maxProperties\" : 1 , \"minProperties\" : 1 , \"properties\" : { \"result\" : { \"type\" : \"array\" , \"items\" : { \"type\" : \"integer\" }}, \"error\" : { \"additionalProperties\" : False , \"properties\" : { \"code\" : { \"type\" : \"integer\" }, \"description\" : { \"type\" : \"string\" }, \"data\" : {}, }, \"required\" : [ \"code\" , \"description\" ], \"type\" : \"object\" , }, }, \"type\" : \"object\" , } data = { \"result\" : 0 } with raises ( ValidationError ): deserialize ( Result [ str ], data ) result = deserialize ( Result [ int ], data ) assert result == Result ( 0 ) assert result . get () == 0 assert serialize ( result ) == { \"result\" : 0 } error = deserialize ( Result , { \"error\" : { \"code\" : 42 , \"description\" : \"...\" }}) with raises ( Error ) as err : error . get () assert err . value == Error ( 42 , \"...\" )","title":"OpenRPC"},{"location":"examples/pydantic_compatibility/","text":"Pydantic compatibility \u00b6 It takes only 20 lines of code to support pydantic.BaseModel and all of its subclasses. You could add these lines to your project using pydantic and start to benefit of Apischema features. Note This support unfortunately doesn't include GraphQL schema feature. Note pydantic pseudo-dataclasses are de facto supported but without pydantic extra features; they could be fully supported but it would requires some additional lines of code. from collections.abc import Mapping from typing import Any , NewType import pydantic from pytest import raises from apischema import ( ValidationError , deserialize , deserializer , schema , serialize , serializer , ) from apischema.json_schema import deserialization_schema from apischema.validation.errors import LocalizedError #################### Pydantic support code starts here def add_deserializer ( cls : type [ pydantic . BaseModel ]): Data = NewType ( \"Data\" , Mapping [ str , Any ]) schema ( extra = cls . schema (), override = True )( Data ) def deserialize_pydantic ( data : Mapping [ str , Any ]) -> pydantic . BaseModel : try : return cls ( ** data ) except pydantic . ValidationError as error : raise ValidationError . deserialize ( [ LocalizedError ( err [ \"loc\" ], [ err [ \"msg\" ]]) for err in error . errors ()] ) deserializer ( deserialize_pydantic , Data , cls ) for cls in pydantic . BaseModel . __subclasses__ (): add_deserializer ( cls ) pydantic . BaseModel . __init_subclass__ = classmethod ( add_deserializer ) # type: ignore @serializer def serialize_pydantic ( obj : pydantic . BaseModel ) -> Mapping [ str , Any ]: # There is currently no mean to retrieve `serialize` parameters, # so exclude unset is set to True as it's the default apischema setting return obj . dict ( exclude_unset = True ) #################### Pydantic support code ends here class Foo ( pydantic . BaseModel ): bar : int assert deserialize ( Foo , { \"bar\" : 0 }) == Foo ( bar = 0 ) assert serialize ( Foo ( bar = 0 )) == { \"bar\" : 0 } assert deserialization_schema ( Foo ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"title\" : \"Foo\" , \"type\" : \"object\" , \"properties\" : { \"bar\" : { \"title\" : \"Bar\" , \"type\" : \"integer\" }}, \"required\" : [ \"bar\" ], } with raises ( ValidationError ) as err : deserialize ( Foo , { \"bar\" : \"not an int\" }) assert serialize ( err . value ) == [ { \"loc\" : [ \"bar\" ], \"err\" : [ \"value is not a valid integer\" ]} # pydantic error message ]","title":"Pydantic compatibility"},{"location":"examples/pydantic_compatibility/#pydantic-compatibility","text":"It takes only 20 lines of code to support pydantic.BaseModel and all of its subclasses. You could add these lines to your project using pydantic and start to benefit of Apischema features. Note This support unfortunately doesn't include GraphQL schema feature. Note pydantic pseudo-dataclasses are de facto supported but without pydantic extra features; they could be fully supported but it would requires some additional lines of code. from collections.abc import Mapping from typing import Any , NewType import pydantic from pytest import raises from apischema import ( ValidationError , deserialize , deserializer , schema , serialize , serializer , ) from apischema.json_schema import deserialization_schema from apischema.validation.errors import LocalizedError #################### Pydantic support code starts here def add_deserializer ( cls : type [ pydantic . BaseModel ]): Data = NewType ( \"Data\" , Mapping [ str , Any ]) schema ( extra = cls . schema (), override = True )( Data ) def deserialize_pydantic ( data : Mapping [ str , Any ]) -> pydantic . BaseModel : try : return cls ( ** data ) except pydantic . ValidationError as error : raise ValidationError . deserialize ( [ LocalizedError ( err [ \"loc\" ], [ err [ \"msg\" ]]) for err in error . errors ()] ) deserializer ( deserialize_pydantic , Data , cls ) for cls in pydantic . BaseModel . __subclasses__ (): add_deserializer ( cls ) pydantic . BaseModel . __init_subclass__ = classmethod ( add_deserializer ) # type: ignore @serializer def serialize_pydantic ( obj : pydantic . BaseModel ) -> Mapping [ str , Any ]: # There is currently no mean to retrieve `serialize` parameters, # so exclude unset is set to True as it's the default apischema setting return obj . dict ( exclude_unset = True ) #################### Pydantic support code ends here class Foo ( pydantic . BaseModel ): bar : int assert deserialize ( Foo , { \"bar\" : 0 }) == Foo ( bar = 0 ) assert serialize ( Foo ( bar = 0 )) == { \"bar\" : 0 } assert deserialization_schema ( Foo ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"title\" : \"Foo\" , \"type\" : \"object\" , \"properties\" : { \"bar\" : { \"title\" : \"Bar\" , \"type\" : \"integer\" }}, \"required\" : [ \"bar\" ], } with raises ( ValidationError ) as err : deserialize ( Foo , { \"bar\" : \"not an int\" }) assert serialize ( err . value ) == [ { \"loc\" : [ \"bar\" ], \"err\" : [ \"value is not a valid integer\" ]} # pydantic error message ]","title":"Pydantic compatibility"},{"location":"examples/recoverable_fields/","text":"Recoverable fields \u00b6 Inspired by https://github.com/samuelcolvin/pydantic/issues/800 from typing import Annotated , Any , Generic , TypeVar , Union from pytest import raises from apischema import deserialize , deserializer , serialize , serializer from apischema.json_schema import deserialization_schema , serialization_schema from apischema.skip import Skip class RecoverableRaw ( Exception ): def __init__ ( self , raw ): self . raw = raw deserializer ( RecoverableRaw , Any , RecoverableRaw ) T = TypeVar ( \"T\" ) class Recoverable ( Generic [ T ]): def __init__ ( self , value : T ): self . _value = value @property def value ( self ) -> T : if isinstance ( self . _value , RecoverableRaw ): raise self . _value return self . _value @value . setter def value ( self , value : T ): self . _value = value deserializer ( Recoverable , Union [ T , Annotated [ RecoverableRaw , Skip ( schema_only = True )]], Recoverable [ T ], ) @serializer def serialize_recoverable ( recoverable : Recoverable [ T ]) -> T : return recoverable . value assert deserialize ( Recoverable [ int ], 0 ) . value == 0 with raises ( RecoverableRaw ) as err : assert deserialize ( Recoverable [ int ], \"bad\" ) . value assert err . value . raw == \"bad\" assert serialize ( Recoverable ( 0 )) == 0 with raises ( RecoverableRaw ) as err : assert serialize ( Recoverable ( RecoverableRaw ( \"bad\" ))) assert err . value . raw == \"bad\" assert ( deserialization_schema ( Recoverable [ int ]) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"type\" : \"integer\" } == serialization_schema ( Recoverable [ int ]) )","title":"Recoverable fields"},{"location":"examples/recoverable_fields/#recoverable-fields","text":"Inspired by https://github.com/samuelcolvin/pydantic/issues/800 from typing import Annotated , Any , Generic , TypeVar , Union from pytest import raises from apischema import deserialize , deserializer , serialize , serializer from apischema.json_schema import deserialization_schema , serialization_schema from apischema.skip import Skip class RecoverableRaw ( Exception ): def __init__ ( self , raw ): self . raw = raw deserializer ( RecoverableRaw , Any , RecoverableRaw ) T = TypeVar ( \"T\" ) class Recoverable ( Generic [ T ]): def __init__ ( self , value : T ): self . _value = value @property def value ( self ) -> T : if isinstance ( self . _value , RecoverableRaw ): raise self . _value return self . _value @value . setter def value ( self , value : T ): self . _value = value deserializer ( Recoverable , Union [ T , Annotated [ RecoverableRaw , Skip ( schema_only = True )]], Recoverable [ T ], ) @serializer def serialize_recoverable ( recoverable : Recoverable [ T ]) -> T : return recoverable . value assert deserialize ( Recoverable [ int ], 0 ) . value == 0 with raises ( RecoverableRaw ) as err : assert deserialize ( Recoverable [ int ], \"bad\" ) . value assert err . value . raw == \"bad\" assert serialize ( Recoverable ( 0 )) == 0 with raises ( RecoverableRaw ) as err : assert serialize ( Recoverable ( RecoverableRaw ( \"bad\" ))) assert err . value . raw == \"bad\" assert ( deserialization_schema ( Recoverable [ int ]) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"type\" : \"integer\" } == serialization_schema ( Recoverable [ int ]) )","title":"Recoverable fields"},{"location":"examples/sqlalchemy/","text":"SQLAlchemy support \u00b6 This example shows a simple support of SQLAlchemy . from dataclasses import MISSING , make_dataclass from inspect import getmembers from typing import Collection from graphql import print_schema from sqlalchemy import Column , Integer from sqlalchemy.ext.declarative import as_declarative from apischema import Undefined , deserialize , serialize from apischema.conversions import dataclass_model from apischema.graphql import graphql_schema from apischema.json_schema import serialization_schema def has_default ( column : Column ) -> bool : return ( column . nullable or column . default is not None or column . server_default is not None ) # Very basic SQLAlchemy support @as_declarative () class Base : def __init_subclass__ ( cls ): columns = getmembers ( cls , lambda m : isinstance ( m , Column )) if not columns : return fields = [ ( column . name or field_name , column . type . python_type , Undefined if has_default ( column ) else MISSING , ) for field_name , column in columns ] dataclass_model ( cls )( make_dataclass ( cls . __name__ , fields )) class Foo ( Base ): __tablename__ = \"foo\" bar = Column ( Integer , primary_key = True ) foo = deserialize ( Foo , { \"bar\" : 0 }) assert isinstance ( foo , Foo ) assert foo . bar == 0 assert serialize ( foo ) == { \"bar\" : 0 } assert serialization_schema ( Foo ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"type\" : \"object\" , \"properties\" : { \"bar\" : { \"type\" : \"integer\" }}, \"required\" : [ \"bar\" ], \"additionalProperties\" : False , } def foos () -> Collection [ Foo ]: ... schema = graphql_schema ( query = [ foos ]) schema_str = \"\"\" \\ type Query { foos: [Foo!] } type Foo { bar: Int! } \"\"\" assert print_schema ( schema ) == schema_str","title":"SQLAlchemy support"},{"location":"examples/sqlalchemy/#sqlalchemy-support","text":"This example shows a simple support of SQLAlchemy . from dataclasses import MISSING , make_dataclass from inspect import getmembers from typing import Collection from graphql import print_schema from sqlalchemy import Column , Integer from sqlalchemy.ext.declarative import as_declarative from apischema import Undefined , deserialize , serialize from apischema.conversions import dataclass_model from apischema.graphql import graphql_schema from apischema.json_schema import serialization_schema def has_default ( column : Column ) -> bool : return ( column . nullable or column . default is not None or column . server_default is not None ) # Very basic SQLAlchemy support @as_declarative () class Base : def __init_subclass__ ( cls ): columns = getmembers ( cls , lambda m : isinstance ( m , Column )) if not columns : return fields = [ ( column . name or field_name , column . type . python_type , Undefined if has_default ( column ) else MISSING , ) for field_name , column in columns ] dataclass_model ( cls )( make_dataclass ( cls . __name__ , fields )) class Foo ( Base ): __tablename__ = \"foo\" bar = Column ( Integer , primary_key = True ) foo = deserialize ( Foo , { \"bar\" : 0 }) assert isinstance ( foo , Foo ) assert foo . bar == 0 assert serialize ( foo ) == { \"bar\" : 0 } assert serialization_schema ( Foo ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"type\" : \"object\" , \"properties\" : { \"bar\" : { \"type\" : \"integer\" }}, \"required\" : [ \"bar\" ], \"additionalProperties\" : False , } def foos () -> Collection [ Foo ]: ... schema = graphql_schema ( query = [ foos ]) schema_str = \"\"\" \\ type Query { foos: [Foo!] } type Foo { bar: Int! } \"\"\" assert print_schema ( schema ) == schema_str","title":"SQLAlchemy support"},{"location":"examples/subclasses_union/","text":"Class as union of its subclasses \u00b6 Inspired by https://github.com/samuelcolvin/pydantic/issues/2036 A class can easily be deserialized as an union of its subclasses using deserializers. Indeed, when more than one deserializer are registered, it results in an union. from dataclasses import dataclass from apischema import deserializer from apischema.conversions import identity from apischema.json_schema import deserialization_schema class Base : pass @dataclass class Foo ( Base ): foo : int @dataclass class Bar ( Base ): bar : str @deserializer def from_foo ( foo : Foo ) -> Base : return foo deserializer ( identity , Bar , Base ) # Roughly equivalent to # @deserializer # def from_bar(bar: Bar) -> Base: # return bar # but identity is optimized by Apischema # You can even add deserializers which are not subclass @deserializer def from_list_of_int ( ints : list [ int ]) -> Base : return Base () assert deserialization_schema ( Base ) == { \"anyOf\" : [ { \"type\" : \"object\" , \"properties\" : { \"foo\" : { \"type\" : \"integer\" }}, \"required\" : [ \"foo\" ], \"additionalProperties\" : False , }, { \"type\" : \"object\" , \"properties\" : { \"bar\" : { \"type\" : \"string\" }}, \"required\" : [ \"bar\" ], \"additionalProperties\" : False , }, { \"type\" : \"array\" , \"items\" : { \"type\" : \"integer\" }}, ], \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , }","title":"Class as union of its subclasses"},{"location":"examples/subclasses_union/#class-as-union-of-its-subclasses","text":"Inspired by https://github.com/samuelcolvin/pydantic/issues/2036 A class can easily be deserialized as an union of its subclasses using deserializers. Indeed, when more than one deserializer are registered, it results in an union. from dataclasses import dataclass from apischema import deserializer from apischema.conversions import identity from apischema.json_schema import deserialization_schema class Base : pass @dataclass class Foo ( Base ): foo : int @dataclass class Bar ( Base ): bar : str @deserializer def from_foo ( foo : Foo ) -> Base : return foo deserializer ( identity , Bar , Base ) # Roughly equivalent to # @deserializer # def from_bar(bar: Bar) -> Base: # return bar # but identity is optimized by Apischema # You can even add deserializers which are not subclass @deserializer def from_list_of_int ( ints : list [ int ]) -> Base : return Base () assert deserialization_schema ( Base ) == { \"anyOf\" : [ { \"type\" : \"object\" , \"properties\" : { \"foo\" : { \"type\" : \"integer\" }}, \"required\" : [ \"foo\" ], \"additionalProperties\" : False , }, { \"type\" : \"object\" , \"properties\" : { \"bar\" : { \"type\" : \"string\" }}, \"required\" : [ \"bar\" ], \"additionalProperties\" : False , }, { \"type\" : \"array\" , \"items\" : { \"type\" : \"integer\" }}, ], \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , }","title":"Class as union of its subclasses"},{"location":"graphql/data_model_and_resolvers/","text":"Data model and resolvers \u00b6 Almost everything of the Data model section remains valid in GraphQL integration. Restrictions \u00b6 Union \u00b6 Unions are only supported between object types, which means dataclass and NamedTuple (and some conversions / dataclass model ). More precisely, it's only supported in output schema, not in resolvers arguments. There are 2 exceptions which can be always be used in Union : None / Optional : Types are non-null (marked with an exclamation mark ! in GraphQL schema) by default; Optional types however results in normal GraphQL types (without ! ). apischema.UndefinedType : it is simply ignored. It is useful in resolvers, see following section TypedDict \u00b6 TypedDict is not supported. In fact, typed dicts are not real classes, so their type can not be checked at runtime, but this is required to disambiguate unions/interfaces. Interfaces \u00b6 Interfaces are simply classes marked with apischema.graphql.interface decorator. An object type implements an interface when its class inherits of interface-marked class, or when it has merged fields of interface-marked dataclass. from dataclasses import dataclass from graphql import print_schema from apischema.graphql import graphql_schema , interface @interface @dataclass class Bar : bar : int @dataclass class Foo ( Bar ): baz : str def foo () -> Foo : ... schema = graphql_schema ( query = [ foo ]) schema_str = \"\"\" \\ type Query { foo: Foo } type Foo implements Bar { bar: Int! baz: String! } interface Bar { bar: Int! } \"\"\" assert print_schema ( schema ) == schema_str Resolvers \u00b6 All dataclass / NamedTuple fields (excepted skipped ) are resolved with their alias in the GraphQL schema. Custom resolvers can also be added by marking methods with apischema.graphql.resolver decorator. Methods can be synchronous or asynchronous (defined with async def or returning an Awaitable ). Resolvers parameters are included in the schema with their type and their default value (except apischema.Undefined ). from dataclasses import dataclass from graphql import print_schema from apischema.graphql import graphql_schema , resolver @dataclass class Bar : baz : int @dataclass class Foo : @resolver async def bar ( self , arg : int = 0 ) -> Bar : ... async def foo () -> Foo : ... schema = graphql_schema ( query = [ foo ]) schema_str = \"\"\" \\ type Query { foo: Foo } type Foo { bar(arg: Int! = 0): Bar } type Bar { baz: Int! } \"\"\" assert print_schema ( schema ) == schema_str Undefined parameter default \u00b6 In GraphQL , non required parameters are forced to be nullable. However, Apischema allows to distinguish a null input from an input absent, by putting apischema.Undefined as parameter default of an Optional field. Thus, field will not be required, and a null value will result in a None argument whereas absent parameter will result in an apischema.Undefined argument. from typing import Optional , Union from graphql import graphql_sync from apischema import Undefined , UndefinedType from apischema.graphql import graphql_schema def arg_is_absent ( arg : Optional [ Union [ int , UndefinedType ]] = Undefined ) -> bool : return arg is Undefined schema = graphql_schema ( query = [ arg_is_absent ]) assert graphql_sync ( schema , \" {argIsAbsent} \" ) . data == { \"argIsAbsent\" : True } assert graphql_sync ( schema , \"{argIsAbsent(arg: null)}\" ) . data == { \"argIsAbsent\" : False }","title":"Data model and resolvers"},{"location":"graphql/data_model_and_resolvers/#data-model-and-resolvers","text":"Almost everything of the Data model section remains valid in GraphQL integration.","title":"Data model and resolvers"},{"location":"graphql/data_model_and_resolvers/#restrictions","text":"","title":"Restrictions"},{"location":"graphql/data_model_and_resolvers/#union","text":"Unions are only supported between object types, which means dataclass and NamedTuple (and some conversions / dataclass model ). More precisely, it's only supported in output schema, not in resolvers arguments. There are 2 exceptions which can be always be used in Union : None / Optional : Types are non-null (marked with an exclamation mark ! in GraphQL schema) by default; Optional types however results in normal GraphQL types (without ! ). apischema.UndefinedType : it is simply ignored. It is useful in resolvers, see following section","title":"Union"},{"location":"graphql/data_model_and_resolvers/#typeddict","text":"TypedDict is not supported. In fact, typed dicts are not real classes, so their type can not be checked at runtime, but this is required to disambiguate unions/interfaces.","title":"TypedDict"},{"location":"graphql/data_model_and_resolvers/#interfaces","text":"Interfaces are simply classes marked with apischema.graphql.interface decorator. An object type implements an interface when its class inherits of interface-marked class, or when it has merged fields of interface-marked dataclass. from dataclasses import dataclass from graphql import print_schema from apischema.graphql import graphql_schema , interface @interface @dataclass class Bar : bar : int @dataclass class Foo ( Bar ): baz : str def foo () -> Foo : ... schema = graphql_schema ( query = [ foo ]) schema_str = \"\"\" \\ type Query { foo: Foo } type Foo implements Bar { bar: Int! baz: String! } interface Bar { bar: Int! } \"\"\" assert print_schema ( schema ) == schema_str","title":"Interfaces"},{"location":"graphql/data_model_and_resolvers/#resolvers","text":"All dataclass / NamedTuple fields (excepted skipped ) are resolved with their alias in the GraphQL schema. Custom resolvers can also be added by marking methods with apischema.graphql.resolver decorator. Methods can be synchronous or asynchronous (defined with async def or returning an Awaitable ). Resolvers parameters are included in the schema with their type and their default value (except apischema.Undefined ). from dataclasses import dataclass from graphql import print_schema from apischema.graphql import graphql_schema , resolver @dataclass class Bar : baz : int @dataclass class Foo : @resolver async def bar ( self , arg : int = 0 ) -> Bar : ... async def foo () -> Foo : ... schema = graphql_schema ( query = [ foo ]) schema_str = \"\"\" \\ type Query { foo: Foo } type Foo { bar(arg: Int! = 0): Bar } type Bar { baz: Int! } \"\"\" assert print_schema ( schema ) == schema_str","title":"Resolvers"},{"location":"graphql/data_model_and_resolvers/#undefined-parameter-default","text":"In GraphQL , non required parameters are forced to be nullable. However, Apischema allows to distinguish a null input from an input absent, by putting apischema.Undefined as parameter default of an Optional field. Thus, field will not be required, and a null value will result in a None argument whereas absent parameter will result in an apischema.Undefined argument. from typing import Optional , Union from graphql import graphql_sync from apischema import Undefined , UndefinedType from apischema.graphql import graphql_schema def arg_is_absent ( arg : Optional [ Union [ int , UndefinedType ]] = Undefined ) -> bool : return arg is Undefined schema = graphql_schema ( query = [ arg_is_absent ]) assert graphql_sync ( schema , \" {argIsAbsent} \" ) . data == { \"argIsAbsent\" : True } assert graphql_sync ( schema , \"{argIsAbsent(arg: null)}\" ) . data == { \"argIsAbsent\" : False }","title":"Undefined parameter default"},{"location":"graphql/overview/","text":"GraphQL Overview \u00b6 Note This documentation is not complete as the feature was recently added. Apischema supports GraphQL through graphql-core library. You can install this dependency directly with Apischema using the following extra requirement: pip install apischema [ graphql ] GraphQL supports consists of generating a GraphQL schema graphql.GraphQLSchema from your data model and endpoints (queries/mutations/subscribtions), in a similar way than the JSON schema generation. This schema can then be used through graphql-core library to query/mutate/subscribe. from dataclasses import dataclass from datetime import date , datetime from typing import Collection , Optional from uuid import UUID , uuid4 from graphql import graphql_sync , print_schema from apischema.graphql import graphql_schema , resolver @dataclass class User : id : UUID username : str birthday : Optional [ date ] = None @resolver def posts ( self ) -> Collection [ \"Post\" ]: return [ post for post in POSTS if post . author . id == self . id ] @dataclass class Post : id : UUID author : User date : datetime content : str USERS = [ User ( uuid4 (), \"foo\" ), User ( uuid4 (), \"bar\" )] POSTS = [ Post ( uuid4 (), USERS [ 0 ], datetime . now (), \"Hello world!\" )] def users () -> Collection [ User ]: return USERS def posts () -> Collection [ Post ]: return POSTS def user ( username : str ) -> Optional [ User ]: for user in users (): if user . username == username : return user else : return None schema = graphql_schema ( query = [ users , user , posts ], id_types = { UUID }) schema_str = \"\"\" \\ type Query { users: [User!] user(username: String!): User posts: [Post!] } type User { id: ID! username: String! birthday: Date posts: [Post!] } scalar Date type Post { id: ID! author: User! date: Datetime! content: String! } scalar Datetime \"\"\" assert print_schema ( schema ) == schema_str query = \"\"\" { users { username posts { content } } } \"\"\" assert graphql_sync ( schema , query ) . data == { \"users\" : [ { \"username\" : \"foo\" , \"posts\" : [{ \"content\" : \"Hello world!\" }]}, { \"username\" : \"bar\" , \"posts\" : []}, ] } GraphQL is fully integrated with the rest of Apischema features, especially conversions , so it's easy to integrate ORM and other custom types in the generated schema; this concerns query results but also arguments. By the way, while GraphQL doesn't support constraints, Apischema still offers you all the power of its validation feature . In fact, Apischema deserialize and validate all the arguments passed to resolvers. FAQ \u00b6 Is it possible to use the same classes to do both GraphQL and REST-API? \u00b6 Yes it is. GraphQL has some restrictions in comparison to JSON schema (see next section ), but this taken in account, all of your code can be reused. In fact, GraphQL endpoints can also be used both by a GraphQL API and a more traditional REST or RPC API.","title":"Overview"},{"location":"graphql/overview/#graphql-overview","text":"Note This documentation is not complete as the feature was recently added. Apischema supports GraphQL through graphql-core library. You can install this dependency directly with Apischema using the following extra requirement: pip install apischema [ graphql ] GraphQL supports consists of generating a GraphQL schema graphql.GraphQLSchema from your data model and endpoints (queries/mutations/subscribtions), in a similar way than the JSON schema generation. This schema can then be used through graphql-core library to query/mutate/subscribe. from dataclasses import dataclass from datetime import date , datetime from typing import Collection , Optional from uuid import UUID , uuid4 from graphql import graphql_sync , print_schema from apischema.graphql import graphql_schema , resolver @dataclass class User : id : UUID username : str birthday : Optional [ date ] = None @resolver def posts ( self ) -> Collection [ \"Post\" ]: return [ post for post in POSTS if post . author . id == self . id ] @dataclass class Post : id : UUID author : User date : datetime content : str USERS = [ User ( uuid4 (), \"foo\" ), User ( uuid4 (), \"bar\" )] POSTS = [ Post ( uuid4 (), USERS [ 0 ], datetime . now (), \"Hello world!\" )] def users () -> Collection [ User ]: return USERS def posts () -> Collection [ Post ]: return POSTS def user ( username : str ) -> Optional [ User ]: for user in users (): if user . username == username : return user else : return None schema = graphql_schema ( query = [ users , user , posts ], id_types = { UUID }) schema_str = \"\"\" \\ type Query { users: [User!] user(username: String!): User posts: [Post!] } type User { id: ID! username: String! birthday: Date posts: [Post!] } scalar Date type Post { id: ID! author: User! date: Datetime! content: String! } scalar Datetime \"\"\" assert print_schema ( schema ) == schema_str query = \"\"\" { users { username posts { content } } } \"\"\" assert graphql_sync ( schema , query ) . data == { \"users\" : [ { \"username\" : \"foo\" , \"posts\" : [{ \"content\" : \"Hello world!\" }]}, { \"username\" : \"bar\" , \"posts\" : []}, ] } GraphQL is fully integrated with the rest of Apischema features, especially conversions , so it's easy to integrate ORM and other custom types in the generated schema; this concerns query results but also arguments. By the way, while GraphQL doesn't support constraints, Apischema still offers you all the power of its validation feature . In fact, Apischema deserialize and validate all the arguments passed to resolvers.","title":"GraphQL Overview"},{"location":"graphql/overview/#faq","text":"","title":"FAQ"},{"location":"graphql/overview/#is-it-possible-to-use-the-same-classes-to-do-both-graphql-and-rest-api","text":"Yes it is. GraphQL has some restrictions in comparison to JSON schema (see next section ), but this taken in account, all of your code can be reused. In fact, GraphQL endpoints can also be used both by a GraphQL API and a more traditional REST or RPC API.","title":"Is it possible to use the same classes to do both GraphQL and REST-API?"},{"location":"graphql/schema/","text":"Schema \u00b6 GraphQL schema is generated by passing all the operations (query/mutation/subscription) functions to apischema.graphql.graphql_schema . Functions parameters and return types are then processed by Apischema to generate the Query / Mutation / Subscription types with their resolvers/subscribers, which are then passed to graphql.graphql_schema . TO BE COMPLETED","title":"Schema"},{"location":"graphql/schema/#schema","text":"GraphQL schema is generated by passing all the operations (query/mutation/subscription) functions to apischema.graphql.graphql_schema . Functions parameters and return types are then processed by Apischema to generate the Query / Mutation / Subscription types with their resolvers/subscribers, which are then passed to graphql.graphql_schema . TO BE COMPLETED","title":"Schema"}]}