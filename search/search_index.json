{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Overview \u00b6 apischema \u00b6 Makes your life easier when it comes to python API. JSON (de)serialization, GraphQL and JSON schema generation through python typing, with a spoonful of sugar. Install \u00b6 pip install apischema It requires only Python 3.6+ (and dataclasses official backport for version 3.6 only) PyPy3 is fully supported. Why another library? \u00b6 This library fulfills the following goals: stay as close as possible to the standard library (dataclasses, typing, etc.) \u2014 as a consequence do not need plugins for editors/linters/etc.; be adaptable, provide tools to support any types (ORM, etc.); avoid dynamic things like using raw strings for attributes name - play nicely with your IDE. No known alternative achieves all of this, and apischema is also faster than all of them. On top of that, because APIs are not only JSON, apischema is also a complete GraphQL library Note Actually, apischema is even adaptable enough to enable support of competitor libraries in a few dozens of line of code ( pydantic support example using conversions feature ) Example \u00b6 from collections.abc import Collection from dataclasses import dataclass , field from typing import Optional from uuid import UUID , uuid4 from graphql import print_schema from pytest import raises from apischema import ValidationError , deserialize , serialize from apischema.graphql import graphql_schema from apischema.json_schema import deserialization_schema # Define a schema with standard dataclasses @dataclass class Resource : id : UUID name : str tags : set [ str ] = field ( default_factory = set ) # Get some data uuid = uuid4 () data = { \"id\" : str ( uuid ), \"name\" : \"wyfo\" , \"tags\" : [ \"some_tag\" ]} # Deserialize data resource = deserialize ( Resource , data ) assert resource == Resource ( uuid , \"wyfo\" , { \"some_tag\" }) # Serialize objects assert serialize ( Resource , resource ) == data # Validate during deserialization with raises ( ValidationError ) as err : # pytest checks exception is raised deserialize ( Resource , { \"id\" : \"42\" , \"name\" : \"wyfo\" }) assert serialize ( err . value ) == [ # ValidationError is serializable { \"loc\" : [ \"id\" ], \"err\" : [ \"badly formed hexadecimal UUID string\" ]} ] # Generate JSON Schema assert deserialization_schema ( Resource ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"type\" : \"object\" , \"properties\" : { \"id\" : { \"type\" : \"string\" , \"format\" : \"uuid\" }, \"name\" : { \"type\" : \"string\" }, \"tags\" : { \"type\" : \"array\" , \"items\" : { \"type\" : \"string\" }, \"uniqueItems\" : True , \"default\" : [], }, }, \"required\" : [ \"id\" , \"name\" ], \"additionalProperties\" : False , } # Define GraphQL operations def resources ( tags : Optional [ Collection [ str ]] = None ) -> Optional [ Collection [ Resource ]]: ... # Generate GraphQL schema schema = graphql_schema ( query = [ resources ], id_types = { UUID }) schema_str = \"\"\" \\ type Query { resources(tags: [String!]): [Resource!] } type Resource { id: ID! name: String! tags: [String!]! } \"\"\" assert print_schema ( schema ) == schema_str apischema works out of the box with you data model. Note This example and further ones are using pytest API because they are in fact run as tests in the library CI Run the documentation examples \u00b6 All documentation examples are written using the last Python minor \u2014 currently 3.9 \u2014 in order to provide an up-to-date documentation. Because Python 3.9 specificities (like PEP 585 ) are used, this version is \"mandatory\" to execute the examples as-is. Also, as stated above, examples are using pytest.raises as it is the most convenient way to test an exception is raised \u2014 and because it's simpler for the CI wrapping. Moreover, apischema has a graphql-core dependency when it comes to example involving GraphQL . At last, some examples of the Examples section are using third-party libraries: SQLAlchemy , attrs and pydantic . All of these dependencies can be downloaded using the examples dependencies with pip install apischema [ examples ] Once dependencies are installed, you can simply copy-paste examples and execute them, using the proper Python version. FAQ \u00b6 What is the difference between apischema and pydantic ? \u00b6 See the dedicated section , there is a lot of difference. I already have my data model with my SQLAlchemy /ORM tables, will I have to duplicate my code, making one dataclass by table? \u00b6 Why would you have to duplicate them? apischema can \"work with user own types as well as foreign libraries ones\". Some teasing of conversion feature: you can add default serialization for all your tables, or register different serializer that you can select according to your API endpoint, or both. I need more accurate validation than \"ensure this is an integer and not a string \", can I do that? \u00b6 See the validation section. You can use standard JSON schema validation ( maxItems , pattern , etc.) that will be embedded in your schema or add custom Python validators for each class/fields/ NewType you want. Let's start the apischema tour.","title":"Overview"},{"location":"#overview","text":"","title":"Overview"},{"location":"#apischema","text":"Makes your life easier when it comes to python API. JSON (de)serialization, GraphQL and JSON schema generation through python typing, with a spoonful of sugar.","title":"apischema"},{"location":"#install","text":"pip install apischema It requires only Python 3.6+ (and dataclasses official backport for version 3.6 only) PyPy3 is fully supported.","title":"Install"},{"location":"#why-another-library","text":"This library fulfills the following goals: stay as close as possible to the standard library (dataclasses, typing, etc.) \u2014 as a consequence do not need plugins for editors/linters/etc.; be adaptable, provide tools to support any types (ORM, etc.); avoid dynamic things like using raw strings for attributes name - play nicely with your IDE. No known alternative achieves all of this, and apischema is also faster than all of them. On top of that, because APIs are not only JSON, apischema is also a complete GraphQL library Note Actually, apischema is even adaptable enough to enable support of competitor libraries in a few dozens of line of code ( pydantic support example using conversions feature )","title":"Why another library?"},{"location":"#example","text":"from collections.abc import Collection from dataclasses import dataclass , field from typing import Optional from uuid import UUID , uuid4 from graphql import print_schema from pytest import raises from apischema import ValidationError , deserialize , serialize from apischema.graphql import graphql_schema from apischema.json_schema import deserialization_schema # Define a schema with standard dataclasses @dataclass class Resource : id : UUID name : str tags : set [ str ] = field ( default_factory = set ) # Get some data uuid = uuid4 () data = { \"id\" : str ( uuid ), \"name\" : \"wyfo\" , \"tags\" : [ \"some_tag\" ]} # Deserialize data resource = deserialize ( Resource , data ) assert resource == Resource ( uuid , \"wyfo\" , { \"some_tag\" }) # Serialize objects assert serialize ( Resource , resource ) == data # Validate during deserialization with raises ( ValidationError ) as err : # pytest checks exception is raised deserialize ( Resource , { \"id\" : \"42\" , \"name\" : \"wyfo\" }) assert serialize ( err . value ) == [ # ValidationError is serializable { \"loc\" : [ \"id\" ], \"err\" : [ \"badly formed hexadecimal UUID string\" ]} ] # Generate JSON Schema assert deserialization_schema ( Resource ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"type\" : \"object\" , \"properties\" : { \"id\" : { \"type\" : \"string\" , \"format\" : \"uuid\" }, \"name\" : { \"type\" : \"string\" }, \"tags\" : { \"type\" : \"array\" , \"items\" : { \"type\" : \"string\" }, \"uniqueItems\" : True , \"default\" : [], }, }, \"required\" : [ \"id\" , \"name\" ], \"additionalProperties\" : False , } # Define GraphQL operations def resources ( tags : Optional [ Collection [ str ]] = None ) -> Optional [ Collection [ Resource ]]: ... # Generate GraphQL schema schema = graphql_schema ( query = [ resources ], id_types = { UUID }) schema_str = \"\"\" \\ type Query { resources(tags: [String!]): [Resource!] } type Resource { id: ID! name: String! tags: [String!]! } \"\"\" assert print_schema ( schema ) == schema_str apischema works out of the box with you data model. Note This example and further ones are using pytest API because they are in fact run as tests in the library CI","title":"Example"},{"location":"#run-the-documentation-examples","text":"All documentation examples are written using the last Python minor \u2014 currently 3.9 \u2014 in order to provide an up-to-date documentation. Because Python 3.9 specificities (like PEP 585 ) are used, this version is \"mandatory\" to execute the examples as-is. Also, as stated above, examples are using pytest.raises as it is the most convenient way to test an exception is raised \u2014 and because it's simpler for the CI wrapping. Moreover, apischema has a graphql-core dependency when it comes to example involving GraphQL . At last, some examples of the Examples section are using third-party libraries: SQLAlchemy , attrs and pydantic . All of these dependencies can be downloaded using the examples dependencies with pip install apischema [ examples ] Once dependencies are installed, you can simply copy-paste examples and execute them, using the proper Python version.","title":"Run the documentation examples"},{"location":"#faq","text":"","title":"FAQ"},{"location":"#what-is-the-difference-between-apischema-and-pydantic","text":"See the dedicated section , there is a lot of difference.","title":"What is the difference between apischema and pydantic?"},{"location":"#i-already-have-my-data-model-with-my-sqlalchemyorm-tables-will-i-have-to-duplicate-my-code-making-one-dataclass-by-table","text":"Why would you have to duplicate them? apischema can \"work with user own types as well as foreign libraries ones\". Some teasing of conversion feature: you can add default serialization for all your tables, or register different serializer that you can select according to your API endpoint, or both.","title":"I already have my data model with my SQLAlchemy/ORM tables, will I have to duplicate my code, making one dataclass by table?"},{"location":"#i-need-more-accurate-validation-than-ensure-this-is-an-integer-and-not-a-string-can-i-do-that","text":"See the validation section. You can use standard JSON schema validation ( maxItems , pattern , etc.) that will be embedded in your schema or add custom Python validators for each class/fields/ NewType you want. Let's start the apischema tour.","title":"I need more accurate validation than \"ensure this is an integer and not a string \", can I do that?"},{"location":"benchmark/","text":"Benchmark \u00b6 Note Benchmark presented is just Pydantic benchmark where apischema has been \"inserted\" . Below are the results of crude benchmark comparing apischema to pydantic and other validation libraries. Package Version Relative Performance Mean deserialization time apischema 0.14.0 51.6\u03bcs pydantic 1.7.3 1.5x slower 77.8\u03bcs valideer 0.4.2 2.3x slower 119.4\u03bcs attrs + cattrs 20.2.0 2.4x slower 122.2\u03bcs marshmallow 3.8.0 4.0x slower 204.7\u03bcs voluptuous 0.12.0 4.9x slower 254.9\u03bcs trafaret 2.1.0 5.5x slower 281.4\u03bcs django-rest-framework 3.12.1 19.4x slower 999.2\u03bcs cerberus 1.3.2 39.5x slower 2038.5\u03bcs Package Version Relative Performance Mean serialization time apischema 0.14.0 29.5\u03bcs pydantic 1.7.3 1.6x slower 48.0\u03bcs Benchmarks were run with Python 3.8 ( CPython ) and the package versions listed above installed via pypi on macOs 11.2 Note A few precisions have to be written about these results: pydantic uses Cython to optimize its performance but apischema is still a lot faster. pydantic benchmark is biased by the implementation of datetime parsing for cattrs (see this post about it); in fact, if cattrs use a decently fast implementation, like the standard datetime.fromisoformat , cattrs becomes 3 times faster than pydantic , even faster than apischema . That being said, apischema is still claimed to be the fastest validation library of this benchmark because cattrs is not considered as a true validation library, essentially because of its fail-fast behavior. It's nevertheless a good (and fast) library, and its great performance has push apischema into optimizing its own performance a lot. By the way, the gap between them is almost filled when playing benchmark only on valid data (where fail-fast gain is minimized). pydantic benchmark mixes valid with invalid data (around 50/50), which doesn't correspond to real case. It means that error handling is very (too much?) important in this benchmark, and libraries like cattrs which raise and end simply at the first error encountered have a big advantage. FAQ \u00b6 Why not ask directly for integration to pydantic benchmark? \u00b6 Done, but rejected because \"apischema doesn't have enough usage\". Let's change that!","title":"Benchmark"},{"location":"benchmark/#benchmark","text":"Note Benchmark presented is just Pydantic benchmark where apischema has been \"inserted\" . Below are the results of crude benchmark comparing apischema to pydantic and other validation libraries. Package Version Relative Performance Mean deserialization time apischema 0.14.0 51.6\u03bcs pydantic 1.7.3 1.5x slower 77.8\u03bcs valideer 0.4.2 2.3x slower 119.4\u03bcs attrs + cattrs 20.2.0 2.4x slower 122.2\u03bcs marshmallow 3.8.0 4.0x slower 204.7\u03bcs voluptuous 0.12.0 4.9x slower 254.9\u03bcs trafaret 2.1.0 5.5x slower 281.4\u03bcs django-rest-framework 3.12.1 19.4x slower 999.2\u03bcs cerberus 1.3.2 39.5x slower 2038.5\u03bcs Package Version Relative Performance Mean serialization time apischema 0.14.0 29.5\u03bcs pydantic 1.7.3 1.6x slower 48.0\u03bcs Benchmarks were run with Python 3.8 ( CPython ) and the package versions listed above installed via pypi on macOs 11.2 Note A few precisions have to be written about these results: pydantic uses Cython to optimize its performance but apischema is still a lot faster. pydantic benchmark is biased by the implementation of datetime parsing for cattrs (see this post about it); in fact, if cattrs use a decently fast implementation, like the standard datetime.fromisoformat , cattrs becomes 3 times faster than pydantic , even faster than apischema . That being said, apischema is still claimed to be the fastest validation library of this benchmark because cattrs is not considered as a true validation library, essentially because of its fail-fast behavior. It's nevertheless a good (and fast) library, and its great performance has push apischema into optimizing its own performance a lot. By the way, the gap between them is almost filled when playing benchmark only on valid data (where fail-fast gain is minimized). pydantic benchmark mixes valid with invalid data (around 50/50), which doesn't correspond to real case. It means that error handling is very (too much?) important in this benchmark, and libraries like cattrs which raise and end simply at the first error encountered have a big advantage.","title":"Benchmark"},{"location":"benchmark/#faq","text":"","title":"FAQ"},{"location":"benchmark/#why-not-ask-directly-for-integration-to-pydantic-benchmark","text":"Done, but rejected because \"apischema doesn't have enough usage\". Let's change that!","title":"Why not ask directly for integration to pydantic benchmark?"},{"location":"conversions/","text":"Conversions \u2013 (de)serialization customization \u00b6 apischema covers majority of standard data types, but it's of course not enough, that's why it gives you the way to add support for all your classes and the libraries you use. Actually, apischema uses its own feature to provide a basic support for standard library data types like UUID/datetime/etc. (see std_types.py ) ORM support can easily be achieved with this feature (see SQLAlchemy example ). In fact, you can even add support of competitor libraries like Pydantic (see Pydantic compatibility example ) Principle - apischema conversions \u00b6 An apischema conversion is composed of a source type, let's call it Source , a target type Target and a converter function of signature (Source) -> Target . When a class (actually, a non-builtin class, so not int / list /etc.) is deserialized, apischema will look if there is a conversion where this type is the target. If found, the source type of conversion will be deserialized, then the converter will be applied to get an object of the expected type. Serialization works the same (inverted) way: look for a conversion with type as source, apply then converter, and get the target type. Conversion can only be applied on classes, not other types like NewType , etc. (see FAQ ) Conversions are also handled in schema generation: for a deserialization schema, source schema is merged to target schema, while target schema is merged to source schema for a serialization schema. Register a conversion \u00b6 Conversion is registered using apischema.deserializer / apischema.serializer for deserialization/serialization respectively. When used as function decorator, the Source / Target types are directly extracted from conversion function signature. serializer can be called on methods/properties, in which case Source type is inferred to be th owning type. from dataclasses import dataclass from apischema import deserialize , schema , serialize from apischema.conversions import deserializer , serializer from apischema.json_schema import deserialization_schema , serialization_schema @schema ( pattern = r \"^#[0-9a-fA-F] {6} $\" ) @dataclass class RGB : red : int green : int blue : int @serializer @property def hexa ( self ) -> str : return f \"# { self . red : 02x }{ self . green : 02x }{ self . blue : 02x } \" # serializer can also be called with methods/properties outside of the class # For example, `serializer(RGB.hexa)` would have the same effect as the decorator above @deserializer def from_hexa ( hexa : str ) -> RGB : return RGB ( int ( hexa [ 1 : 3 ], 16 ), int ( hexa [ 3 : 5 ], 16 ), int ( hexa [ 5 : 7 ], 16 )) assert deserialize ( RGB , \"#000000\" ) == RGB ( 0 , 0 , 0 ) assert serialize ( RGB , RGB ( 0 , 0 , 42 )) == \"#00002a\" assert ( deserialization_schema ( RGB ) == serialization_schema ( RGB ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"type\" : \"string\" , \"pattern\" : \"^#[0-9a-fA-F] {6} $\" , } ) Warning (De)serializer methods cannot be used with typing.NamedTuple ; in fact, apischema uses __set_name__ magic method but it is not called on NamedTuple subclass fields. Multiple deserializers \u00b6 Sometimes, you want to have several possibilities to deserialize a type. If it's possible to register a deserializer with an Union param, it's not very practical. That's why apischema make it possible to register several deserializers for the same type. They will be handled with an Union source type (ordered by deserializers registration), with the right serializer selected according to the matching alternative. from dataclasses import dataclass from apischema import deserialize , deserializer from apischema.json_schema import deserialization_schema @dataclass class Expression : value : int @deserializer def evaluate_expression ( expr : str ) -> Expression : return Expression ( int ( eval ( expr ))) # Could be shorten into deserializer(Expression), because class is callable too @deserializer def expression_from_value ( value : int ) -> Expression : return Expression ( value ) assert deserialization_schema ( Expression ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"type\" : [ \"string\" , \"integer\" ], } assert deserialize ( Expression , 0 ) == deserialize ( Expression , \"1 - 1\" ) == Expression ( 0 ) On the other hand, serializer registration overwrite the previous registration if any. apischema.conversions.reset_deserializers / apischema.conversions.reset_serializers can be used to reset (de)serializers (even those of the standard types embedded in apischema ) Inheritance \u00b6 All serializers are naturally inherited. In fact, with a conversion function (Source) -> Target , you can always pass a subtype of Source and get a Target in return. Moreover, when serializer is a method/property, overriding this method/property in a subclass will override the inherited serializer. from apischema import serialize , serializer class Foo : pass @serializer def serialize_foo ( foo : Foo ) -> int : return 0 class Foo2 ( Foo ): pass # Deserializer is inherited assert serialize ( Foo , Foo ()) == serialize ( Foo2 , Foo2 ()) == 0 class Bar : @serializer def serialize ( self ) -> int : return 0 class Bar2 ( Bar ): def serialize ( self ) -> int : return 1 # Deserializer is inherited and overridden assert serialize ( Bar , Bar ()) == 0 != serialize ( Bar2 , Bar2 ()) == 1 Note Inheritance can also be toggled off in specific cases, like in the Class as union of its subclasses example On the other hand, deserializers cannot be inherited, because the same Source passed to a conversion function (Source) -> Target will always give the same Target (not ensured to be the desired subtype). Note Pseudo-inheritance could be achieved by registering a conversion (using for example a classmethod ) for each subclass in __init_subclass__ method (or a metaclass), or by using __subclasses__ ; see example Generic conversions \u00b6 Generic conversions are supported out of the box. from typing import Generic , TypeVar from pytest import raises from apischema import ValidationError , deserialize , serialize from apischema.conversions import deserializer , serializer from apischema.json_schema import deserialization_schema , serialization_schema T = TypeVar ( \"T\" ) class Wrapper ( Generic [ T ]): def __init__ ( self , wrapped : T ): self . wrapped = wrapped @serializer def unwrap ( self ) -> T : return self . wrapped # Wrapper constructor can be used as a function too (so deserializer could work as decorator) deserializer ( Wrapper ) assert deserialize ( Wrapper [ list [ int ]], [ 0 , 1 ]) . wrapped == [ 0 , 1 ] with raises ( ValidationError ): deserialize ( Wrapper [ int ], \"wrapped\" ) assert serialize ( Wrapper [ str ], Wrapper ( \"wrapped\" )) == \"wrapped\" assert ( deserialization_schema ( Wrapper [ int ]) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"type\" : \"integer\" } == serialization_schema ( Wrapper [ int ]) ) However, it's not allowed to register a conversion of a specialized generic type, like Foo[int] (see FAQ ). Conversion object \u00b6 In previous example, conversions where registered using only converter functions. However, everywhere you can pass a converter, you can also pass a apischema.conversions.Conversion instance. Conversion allows adding additional metadata to conversion than a function can do ; it can also be used to precise converter source/target when annotations are not available. from base64 import b64decode from apischema import deserialize , deserializer from apischema.conversions import Conversion deserializer ( Conversion ( b64decode , source = str , target = bytes )) # Roughly equivalent to: # def decode_bytes(source: str) -> bytes: # return b64decode(source) # but saving a function call assert deserialize ( bytes , \"Zm9v\" ) == b \"foo\" Dynamic conversions \u2014 select conversions at runtime \u00b6 No matter if a conversion is registered or not for a given type, conversions can also be provided at runtime, using conversion parameter of deserialize / serialize / deserialization_schema / serialization_schema . import os import time from dataclasses import dataclass from datetime import datetime from typing import Annotated from apischema import deserialize , serialize from apischema.metadata import conversion # Set UTC timezone for example os . environ [ \"TZ\" ] = \"UTC\" time . tzset () def datetime_from_timestamp ( timestamp : int ) -> datetime : return datetime . fromtimestamp ( timestamp ) date = datetime ( 2017 , 9 , 2 ) assert deserialize ( datetime , 1504310400 , conversion = datetime_from_timestamp ) == date @dataclass class Foo : bar : int baz : int def sum ( self ) -> int : return self . bar + self . baz @property def diff ( self ) -> int : return int ( self . bar - self . baz ) assert serialize ( Foo , Foo ( 0 , 1 )) == { \"bar\" : 0 , \"baz\" : 1 } assert serialize ( Foo , Foo ( 0 , 1 ), conversion = Foo . sum ) == 1 assert serialize ( Foo , Foo ( 0 , 1 ), conversion = Foo . diff ) == - 1 # conversions can be specified using Annotated assert serialize ( Annotated [ Foo , conversion ( serialization = Foo . sum )], Foo ( 0 , 1 )) == 1 Note For definitions_schema , conversions can be added with types by using a tuple instead, for example definitions_schema(serializations=[(list[Foo], foo_to_bar)]) . conversion parameter can also take a tuple of conversions, when you have a Union , a tuple or when you want to have several deserializations for the same type. Dynamic conversions are local \u00b6 Dynamic conversions are discarded after having been applied (or after class without conversion having been encountered). For example, you can't apply directly a dynamic conversion to a dataclass field when calling serialize on an instance of this dataclass. Reasons of this design are detailed in the FAQ . import os import time from dataclasses import dataclass from datetime import datetime from apischema import serialize # Set UTC timezone for example os . environ [ \"TZ\" ] = \"UTC\" time . tzset () def to_timestamp ( d : datetime ) -> int : return int ( d . timestamp ()) @dataclass class Foo : bar : datetime # timestamp conversion is not applied on Foo field because it's discarded # when encountering Foo assert serialize ( Foo , Foo ( datetime ( 2019 , 10 , 13 )), conversion = to_timestamp ) == { \"bar\" : \"2019-10-13T00:00:00\" } # timestamp conversion is applied on every member of list assert serialize ( list [ datetime ], [ datetime ( 1970 , 1 , 1 )], conversion = to_timestamp ) == [ 0 ] Note Dynamic conversion is not discarded when the encountered type is a container ( list , dict , Collection , etc. or Union ) or a registered conversion from/to a container; the dynamic conversion can then apply to the container elements Dynamic conversions interact with type_name \u00b6 Dynamic conversions are applied before looking for a ref registered with type_name from dataclasses import dataclass from apischema import type_name from apischema.json_schema import serialization_schema @dataclass class Foo : pass @dataclass class Bar : pass def foo_to_bar ( _ : Foo ) -> Bar : return Bar () type_name ( \"Bars\" )( list [ Bar ]) assert serialization_schema ( list [ Foo ], conversion = foo_to_bar , all_refs = True ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"$ref\" : \"#/$defs/Bars\" , \"$defs\" : { # Bars is present because `list[Foo]` is dynamically converted to `list[Bar]` \"Bars\" : { \"type\" : \"array\" , \"items\" : { \"$ref\" : \"#/$defs/Bar\" }}, \"Bar\" : { \"type\" : \"object\" , \"additionalProperties\" : False }, }, } Bypass registered conversion \u00b6 Using apischema.conversion.identity as a dynamic conversion allows to bypass a registered conversion, i.e. to (de)serialize the given type as it would be without conversion registered. from dataclasses import dataclass from apischema import serialize , serializer from apischema.conversions import Conversion , identity @dataclass class RGB : red : int green : int blue : int @serializer @property def hexa ( self ) -> str : return f \"# { self . red : 02x }{ self . green : 02x }{ self . blue : 02x } \" assert serialize ( RGB , RGB ( 0 , 0 , 0 )) == \"#000000\" # dynamic conversion used to bypass the registered one assert serialize ( RGB , RGB ( 0 , 0 , 0 ), conversion = identity ) == { \"red\" : 0 , \"green\" : 0 , \"blue\" : 0 , } # Expended bypass form assert serialize ( RGB , RGB ( 0 , 0 , 0 ), conversion = Conversion ( identity , source = RGB , target = RGB ) ) == { \"red\" : 0 , \"green\" : 0 , \"blue\" : 0 } Note For a more precise selection of bypassed conversion, for tuple or Union member for example, it's possible to pass the concerned class as the source and the target of conversion with identity converter, as shown in the example. Liskov substitution principle \u00b6 LSP is taken in account when applying dynamic conversion: serializer source can be a subclass of the actual class and deserializer target can be a superclass of the actual class. from dataclasses import dataclass from apischema import deserialize , serialize @dataclass class Foo : field : int @dataclass class Bar ( Foo ): other : str def foo_to_int ( foo : Foo ) -> int : return foo . field def bar_from_int ( i : int ) -> Bar : return Bar ( i , str ( i )) assert serialize ( Bar , Bar ( 0 , \"\" ), conversion = foo_to_int ) == 0 assert deserialize ( Foo , 0 , conversion = bar_from_int ) == Bar ( 0 , \"0\" ) Generic dynamic conversions \u00b6 Generic dynamic conversions are supported out of the box. Also, contrary to registered conversions, partially specialized generics are allowed. from collections.abc import Mapping , Sequence from operator import itemgetter from typing import TypeVar from apischema import serialize from apischema.json_schema import serialization_schema T = TypeVar ( \"T\" ) Priority = int def sort_by_priority ( values_with_priority : Mapping [ T , Priority ]) -> Sequence [ T ]: return [ k for k , _ in sorted ( values_with_priority . items (), key = itemgetter ( 1 ))] assert serialize ( dict [ str , Priority ], { \"a\" : 1 , \"b\" : 0 }, conversion = sort_by_priority ) == [ \"b\" , \"a\" ] assert serialization_schema ( dict [ str , Priority ], conversion = sort_by_priority ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"type\" : \"array\" , \"items\" : { \"type\" : \"string\" }, } Field conversions \u00b6 It is possible to register a conversion for a particular dataclass field using conversion metadata. import os import time from dataclasses import dataclass , field from datetime import datetime from apischema import deserialize , serialize from apischema.conversions import Conversion from apischema.metadata import conversion # Set UTC timezone for example os . environ [ \"TZ\" ] = \"UTC\" time . tzset () from_timestamp = Conversion ( datetime . fromtimestamp , source = int , target = datetime ) def to_timestamp ( d : datetime ) -> int : return int ( d . timestamp ()) @dataclass class Foo : some_date : datetime = field ( metadata = conversion ( from_timestamp , to_timestamp )) other_date : datetime assert deserialize ( Foo , { \"some_date\" : 0 , \"other_date\" : \"2019-10-13\" }) == Foo ( datetime ( 1970 , 1 , 1 ), datetime ( 2019 , 10 , 13 ) ) assert serialize ( Foo , Foo ( datetime ( 1970 , 1 , 1 ), datetime ( 2019 , 10 , 13 ))) == { \"some_date\" : 0 , \"other_date\" : \"2019-10-13T00:00:00\" , } Note It's possible to pass a conversion only for deserialization or only for serialization Serialized method conversions \u00b6 Serialized methods can also have dedicated conversions for their return import os import time from dataclasses import dataclass from datetime import datetime from apischema import serialize , serialized # Set UTC timezone for example os . environ [ \"TZ\" ] = \"UTC\" time . tzset () def to_timestamp ( d : datetime ) -> int : return int ( d . timestamp ()) @dataclass class Foo : @serialized ( conversion = to_timestamp ) def some_date ( self ) -> datetime : return datetime ( 1970 , 1 , 1 ) assert serialize ( Foo , Foo ()) == { \"some_date\" : 0 } String conversions \u00b6 A common pattern of conversion concerns class having a string constructor and a __str__ method; standard types uuid.UUID , pathlib.Path , ipaddress.IPv4Address are concerned. Using apischema.conversions.as_str will register a string-deserializer from the constructor and a string-serializer from the __str__ method. import bson from pytest import raises from apischema import Unsupported , deserialize , serialize from apischema.conversions import as_str with raises ( Unsupported ): deserialize ( bson . ObjectId , \"0123456789ab0123456789ab\" ) with raises ( Unsupported ): serialize ( bson . ObjectId , bson . ObjectId ( \"0123456789ab0123456789ab\" )) as_str ( bson . ObjectId ) assert deserialize ( bson . ObjectId , \"0123456789ab0123456789ab\" ) == bson . ObjectId ( \"0123456789ab0123456789ab\" ) assert ( serialize ( bson . ObjectId , bson . ObjectId ( \"0123456789ab0123456789ab\" )) == \"0123456789ab0123456789ab\" ) Note Previously mentioned standard types are handled by apischema using as_str . Object deserialization \u2014 transform function into a dataclass deserializer \u00b6 apischema.objects.object_deserialization can convert a function into a new function taking a unique parameter, a dataclass whose fields are mapped from the original function parameters. It can be used for example to build a deserialization conversion from an alternative constructor. from apischema import deserialize , deserializer , type_name from apischema.json_schema import deserialization_schema from apischema.objects import object_deserialization def create_range ( start : int , stop : int , step : int = 1 ) -> range : return range ( start , stop , step ) range_conv = object_deserialization ( create_range , type_name ( \"Range\" )) # Conversion can be registered deserializer ( range_conv ) assert deserialize ( range , { \"start\" : 0 , \"stop\" : 10 }) == range ( 0 , 10 ) assert deserialization_schema ( range ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"type\" : \"object\" , \"properties\" : { \"start\" : { \"type\" : \"integer\" }, \"stop\" : { \"type\" : \"integer\" }, \"step\" : { \"type\" : \"integer\" , \"default\" : 1 }, }, \"required\" : [ \"start\" , \"stop\" ], \"additionalProperties\" : False , } Note Parameters metadata can be specified using typing.Annotated , or be passed with parameters_metadata parameter, which is a mapping of parameter names as key and mapped metadata as value. Object serialization \u2014 select only a subset of fields \u00b6 apischema.objects.object_serialization can be used to serialize only a subset of an object fields and methods. from dataclasses import dataclass from typing import Any from apischema import alias , serialize , type_name from apischema.json_schema import JsonSchemaVersion , definitions_schema from apischema.objects import get_field , object_serialization @dataclass class Data : id : int content : str @property def size ( self ) -> int : return len ( self . content ) def get_details ( self ) -> Any : ... # Serialization fields can be a str/field or a function/method/property size_only = object_serialization ( Data , [ get_field ( Data ) . id , Data . size ], type_name ( \"DataSize\" ) ) # [\"id\", Data.size] would also work def complete_data (): return [ ... , # shortcut to include all the fields Data . size , ( Data . get_details , alias ( \"details\" )), # add/override metadata using tuple ] # Serialization fields computation can be deferred in a function # The serialization name will then be defaulted to the function name complete = object_serialization ( Data , complete_data ) data = Data ( 0 , \"data\" ) assert serialize ( Data , data , conversion = size_only ) == { \"id\" : 0 , \"size\" : 4 } assert serialize ( Data , data , conversion = complete ) == { \"id\" : 0 , \"content\" : \"data\" , \"size\" : 4 , \"details\" : None , # because get_details return None in this example } assert definitions_schema ( serialization = [( Data , size_only ), ( Data , complete )], version = JsonSchemaVersion . OPEN_API_3_0 , ) == { \"DataSize\" : { \"type\" : \"object\" , \"properties\" : { \"id\" : { \"type\" : \"integer\" }, \"size\" : { \"type\" : \"integer\" }}, \"required\" : [ \"id\" , \"size\" ], \"additionalProperties\" : False , }, \"CompleteData\" : { \"type\" : \"object\" , \"properties\" : { \"id\" : { \"type\" : \"integer\" }, \"content\" : { \"type\" : \"string\" }, \"size\" : { \"type\" : \"integer\" }, \"details\" : {}, }, \"required\" : [ \"id\" , \"content\" , \"size\" , \"details\" ], \"additionalProperties\" : False , }, } Default conversions \u00b6 As almost every default behavior in apischema , default conversions can be configured using apischema.settings.deserialization.default_conversion / apischema.settings.serialization.default_conversion . The initial value of these settings are the function which retrieved conversions registered with deserializer / serializer . You can for example support attrs classes with this feature: from typing import Optional , Sequence import attr from apischema import deserialize , serialize , settings from apischema.json_schema import deserialization_schema from apischema.objects import ObjectField prev_default_object_fields = settings . default_object_fields def attrs_fields ( cls : type ) -> Optional [ Sequence [ ObjectField ]]: if hasattr ( cls , \"__attrs_attrs__\" ): return [ ObjectField ( a . name , a . type , required = a . default == attr . NOTHING , default = a . default ) for a in getattr ( cls , \"__attrs_attrs__\" ) ] else : return prev_default_object_fields ( cls ) settings . default_object_fields = attrs_fields @attr . s class Foo : bar : int = attr . ib () assert deserialize ( Foo , { \"bar\" : 0 }) == Foo ( 0 ) assert serialize ( Foo , Foo ( 0 )) == { \"bar\" : 0 } assert deserialization_schema ( Foo ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"type\" : \"object\" , \"properties\" : { \"bar\" : { \"type\" : \"integer\" }}, \"required\" : [ \"bar\" ], \"additionalProperties\" : False , } apischema functions ( deserialize / serialize / deserialization_schema / serialization_schema / definitions_schema ) also have a default_conversion parameter to dynamically modify default conversions. See FAQ for the difference between conversion and default_conversion parameters. Sub-conversions \u00b6 Sub-conversions are dynamic conversions applied on the result of a conversion. from dataclasses import dataclass from typing import Generic , Optional , TypeVar from apischema.conversions import Conversion from apischema.json_schema import serialization_schema T = TypeVar ( \"T\" ) class Query ( Generic [ T ]): ... def query_to_list ( q : Query [ T ]) -> list [ T ]: ... def query_to_scalar ( q : Query [ T ]) -> Optional [ T ]: ... @dataclass class FooModel : bar : int class Foo : def serialize ( self ) -> FooModel : ... assert serialization_schema ( Query [ Foo ], conversion = Conversion ( query_to_list , sub_conversion = Foo . serialize ) ) == { # We get an array of Foo \"type\" : \"array\" , \"items\" : { \"type\" : \"object\" , \"properties\" : { \"bar\" : { \"type\" : \"integer\" }}, \"required\" : [ \"bar\" ], \"additionalProperties\" : False , }, \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , } Sub-conversions can also be used to bypass registered conversions or to define recursive conversions . Lazy/recursive conversions \u00b6 Conversions can be defined lazily, i.e. using a function returning Conversion (single, or a tuple of it); this function must be wrap into a apischema.conversions.LazyConversion instance. It allows creating recursive conversions or using a conversion object which can be modified after its definition (for example a conversion for a base class modified by __init_subclass__ ) It is used by apischema itself for the generated JSON schema. It is indeed a recursive data, and the different versions are handled by a conversion with a lazy recursive sub-conversion. from dataclasses import dataclass from typing import Union from apischema import serialize from apischema.conversions import Conversion , LazyConversion @dataclass class Foo : elements : list [ Union [ int , \"Foo\" ]] def foo_elements ( foo : Foo ) -> list [ Union [ int , Foo ]]: return foo . elements # Recursive conversion pattern tmp = None conversion = Conversion ( foo_elements , sub_conversion = LazyConversion ( lambda : tmp )) tmp = conversion assert serialize ( Foo , Foo ([ 0 , Foo ([ 1 ])]), conversion = conversion ) == [ 0 , [ 1 ]] # Without the recursive sub-conversion, it would have been: assert serialize ( Foo , Foo ([ 0 , Foo ([ 1 ])]), conversion = foo_elements ) == [ 0 , { \"elements\" : [ 1 ]}, ] Lazy registered conversions \u00b6 Lazy conversions can also be registered, but the deserialization target/serialization source has to be passed too. from dataclasses import dataclass from apischema import deserialize , deserializer , serialize , serializer from apischema.conversions import Conversion @dataclass class Foo : bar : int deserializer ( lazy = lambda : Conversion ( lambda bar : Foo ( bar ), source = int , target = Foo ), target = Foo ) serializer ( lazy = lambda : Conversion ( lambda foo : foo . bar , source = Foo , target = int ), source = Foo ) assert deserialize ( Foo , 0 ) == Foo ( 0 ) assert serialize ( Foo , Foo ( 0 )) == 0 FAQ \u00b6 What's the difference between conversion and default_conversion parameters? \u00b6 Dynamic conversions ( conversion parameter) exists to ensure consistency and reuse of subschemas referenced (with a $ref ) in the JSON/ OpenAPI schema. In fact, different global conversions ( default_conversion parameter) could lead to have a field with different schemas depending on global conversions, so class would not be able to be referenced consistently. Because dynamic conversions are local, they cannot mess with an objet field schema. Schema generation use the same default conversions for all definitions (which can have associated dynamic conversion). default_conversion parameter allows having different (de)serialization contexts, for example to map date to string between frontend and backend, and to timestamp between backend services.","title":"Conversions"},{"location":"conversions/#conversions-deserialization-customization","text":"apischema covers majority of standard data types, but it's of course not enough, that's why it gives you the way to add support for all your classes and the libraries you use. Actually, apischema uses its own feature to provide a basic support for standard library data types like UUID/datetime/etc. (see std_types.py ) ORM support can easily be achieved with this feature (see SQLAlchemy example ). In fact, you can even add support of competitor libraries like Pydantic (see Pydantic compatibility example )","title":"Conversions \u2013 (de)serialization customization"},{"location":"conversions/#principle-apischema-conversions","text":"An apischema conversion is composed of a source type, let's call it Source , a target type Target and a converter function of signature (Source) -> Target . When a class (actually, a non-builtin class, so not int / list /etc.) is deserialized, apischema will look if there is a conversion where this type is the target. If found, the source type of conversion will be deserialized, then the converter will be applied to get an object of the expected type. Serialization works the same (inverted) way: look for a conversion with type as source, apply then converter, and get the target type. Conversion can only be applied on classes, not other types like NewType , etc. (see FAQ ) Conversions are also handled in schema generation: for a deserialization schema, source schema is merged to target schema, while target schema is merged to source schema for a serialization schema.","title":"Principle - apischema conversions"},{"location":"conversions/#register-a-conversion","text":"Conversion is registered using apischema.deserializer / apischema.serializer for deserialization/serialization respectively. When used as function decorator, the Source / Target types are directly extracted from conversion function signature. serializer can be called on methods/properties, in which case Source type is inferred to be th owning type. from dataclasses import dataclass from apischema import deserialize , schema , serialize from apischema.conversions import deserializer , serializer from apischema.json_schema import deserialization_schema , serialization_schema @schema ( pattern = r \"^#[0-9a-fA-F] {6} $\" ) @dataclass class RGB : red : int green : int blue : int @serializer @property def hexa ( self ) -> str : return f \"# { self . red : 02x }{ self . green : 02x }{ self . blue : 02x } \" # serializer can also be called with methods/properties outside of the class # For example, `serializer(RGB.hexa)` would have the same effect as the decorator above @deserializer def from_hexa ( hexa : str ) -> RGB : return RGB ( int ( hexa [ 1 : 3 ], 16 ), int ( hexa [ 3 : 5 ], 16 ), int ( hexa [ 5 : 7 ], 16 )) assert deserialize ( RGB , \"#000000\" ) == RGB ( 0 , 0 , 0 ) assert serialize ( RGB , RGB ( 0 , 0 , 42 )) == \"#00002a\" assert ( deserialization_schema ( RGB ) == serialization_schema ( RGB ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"type\" : \"string\" , \"pattern\" : \"^#[0-9a-fA-F] {6} $\" , } ) Warning (De)serializer methods cannot be used with typing.NamedTuple ; in fact, apischema uses __set_name__ magic method but it is not called on NamedTuple subclass fields.","title":"Register a conversion"},{"location":"conversions/#multiple-deserializers","text":"Sometimes, you want to have several possibilities to deserialize a type. If it's possible to register a deserializer with an Union param, it's not very practical. That's why apischema make it possible to register several deserializers for the same type. They will be handled with an Union source type (ordered by deserializers registration), with the right serializer selected according to the matching alternative. from dataclasses import dataclass from apischema import deserialize , deserializer from apischema.json_schema import deserialization_schema @dataclass class Expression : value : int @deserializer def evaluate_expression ( expr : str ) -> Expression : return Expression ( int ( eval ( expr ))) # Could be shorten into deserializer(Expression), because class is callable too @deserializer def expression_from_value ( value : int ) -> Expression : return Expression ( value ) assert deserialization_schema ( Expression ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"type\" : [ \"string\" , \"integer\" ], } assert deserialize ( Expression , 0 ) == deserialize ( Expression , \"1 - 1\" ) == Expression ( 0 ) On the other hand, serializer registration overwrite the previous registration if any. apischema.conversions.reset_deserializers / apischema.conversions.reset_serializers can be used to reset (de)serializers (even those of the standard types embedded in apischema )","title":"Multiple deserializers"},{"location":"conversions/#inheritance","text":"All serializers are naturally inherited. In fact, with a conversion function (Source) -> Target , you can always pass a subtype of Source and get a Target in return. Moreover, when serializer is a method/property, overriding this method/property in a subclass will override the inherited serializer. from apischema import serialize , serializer class Foo : pass @serializer def serialize_foo ( foo : Foo ) -> int : return 0 class Foo2 ( Foo ): pass # Deserializer is inherited assert serialize ( Foo , Foo ()) == serialize ( Foo2 , Foo2 ()) == 0 class Bar : @serializer def serialize ( self ) -> int : return 0 class Bar2 ( Bar ): def serialize ( self ) -> int : return 1 # Deserializer is inherited and overridden assert serialize ( Bar , Bar ()) == 0 != serialize ( Bar2 , Bar2 ()) == 1 Note Inheritance can also be toggled off in specific cases, like in the Class as union of its subclasses example On the other hand, deserializers cannot be inherited, because the same Source passed to a conversion function (Source) -> Target will always give the same Target (not ensured to be the desired subtype). Note Pseudo-inheritance could be achieved by registering a conversion (using for example a classmethod ) for each subclass in __init_subclass__ method (or a metaclass), or by using __subclasses__ ; see example","title":"Inheritance"},{"location":"conversions/#generic-conversions","text":"Generic conversions are supported out of the box. from typing import Generic , TypeVar from pytest import raises from apischema import ValidationError , deserialize , serialize from apischema.conversions import deserializer , serializer from apischema.json_schema import deserialization_schema , serialization_schema T = TypeVar ( \"T\" ) class Wrapper ( Generic [ T ]): def __init__ ( self , wrapped : T ): self . wrapped = wrapped @serializer def unwrap ( self ) -> T : return self . wrapped # Wrapper constructor can be used as a function too (so deserializer could work as decorator) deserializer ( Wrapper ) assert deserialize ( Wrapper [ list [ int ]], [ 0 , 1 ]) . wrapped == [ 0 , 1 ] with raises ( ValidationError ): deserialize ( Wrapper [ int ], \"wrapped\" ) assert serialize ( Wrapper [ str ], Wrapper ( \"wrapped\" )) == \"wrapped\" assert ( deserialization_schema ( Wrapper [ int ]) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"type\" : \"integer\" } == serialization_schema ( Wrapper [ int ]) ) However, it's not allowed to register a conversion of a specialized generic type, like Foo[int] (see FAQ ).","title":"Generic conversions"},{"location":"conversions/#conversion-object","text":"In previous example, conversions where registered using only converter functions. However, everywhere you can pass a converter, you can also pass a apischema.conversions.Conversion instance. Conversion allows adding additional metadata to conversion than a function can do ; it can also be used to precise converter source/target when annotations are not available. from base64 import b64decode from apischema import deserialize , deserializer from apischema.conversions import Conversion deserializer ( Conversion ( b64decode , source = str , target = bytes )) # Roughly equivalent to: # def decode_bytes(source: str) -> bytes: # return b64decode(source) # but saving a function call assert deserialize ( bytes , \"Zm9v\" ) == b \"foo\"","title":"Conversion object"},{"location":"conversions/#dynamic-conversions-select-conversions-at-runtime","text":"No matter if a conversion is registered or not for a given type, conversions can also be provided at runtime, using conversion parameter of deserialize / serialize / deserialization_schema / serialization_schema . import os import time from dataclasses import dataclass from datetime import datetime from typing import Annotated from apischema import deserialize , serialize from apischema.metadata import conversion # Set UTC timezone for example os . environ [ \"TZ\" ] = \"UTC\" time . tzset () def datetime_from_timestamp ( timestamp : int ) -> datetime : return datetime . fromtimestamp ( timestamp ) date = datetime ( 2017 , 9 , 2 ) assert deserialize ( datetime , 1504310400 , conversion = datetime_from_timestamp ) == date @dataclass class Foo : bar : int baz : int def sum ( self ) -> int : return self . bar + self . baz @property def diff ( self ) -> int : return int ( self . bar - self . baz ) assert serialize ( Foo , Foo ( 0 , 1 )) == { \"bar\" : 0 , \"baz\" : 1 } assert serialize ( Foo , Foo ( 0 , 1 ), conversion = Foo . sum ) == 1 assert serialize ( Foo , Foo ( 0 , 1 ), conversion = Foo . diff ) == - 1 # conversions can be specified using Annotated assert serialize ( Annotated [ Foo , conversion ( serialization = Foo . sum )], Foo ( 0 , 1 )) == 1 Note For definitions_schema , conversions can be added with types by using a tuple instead, for example definitions_schema(serializations=[(list[Foo], foo_to_bar)]) . conversion parameter can also take a tuple of conversions, when you have a Union , a tuple or when you want to have several deserializations for the same type.","title":"Dynamic conversions \u2014 select conversions at runtime"},{"location":"conversions/#dynamic-conversions-are-local","text":"Dynamic conversions are discarded after having been applied (or after class without conversion having been encountered). For example, you can't apply directly a dynamic conversion to a dataclass field when calling serialize on an instance of this dataclass. Reasons of this design are detailed in the FAQ . import os import time from dataclasses import dataclass from datetime import datetime from apischema import serialize # Set UTC timezone for example os . environ [ \"TZ\" ] = \"UTC\" time . tzset () def to_timestamp ( d : datetime ) -> int : return int ( d . timestamp ()) @dataclass class Foo : bar : datetime # timestamp conversion is not applied on Foo field because it's discarded # when encountering Foo assert serialize ( Foo , Foo ( datetime ( 2019 , 10 , 13 )), conversion = to_timestamp ) == { \"bar\" : \"2019-10-13T00:00:00\" } # timestamp conversion is applied on every member of list assert serialize ( list [ datetime ], [ datetime ( 1970 , 1 , 1 )], conversion = to_timestamp ) == [ 0 ] Note Dynamic conversion is not discarded when the encountered type is a container ( list , dict , Collection , etc. or Union ) or a registered conversion from/to a container; the dynamic conversion can then apply to the container elements","title":"Dynamic conversions are local"},{"location":"conversions/#dynamic-conversions-interact-with-type_name","text":"Dynamic conversions are applied before looking for a ref registered with type_name from dataclasses import dataclass from apischema import type_name from apischema.json_schema import serialization_schema @dataclass class Foo : pass @dataclass class Bar : pass def foo_to_bar ( _ : Foo ) -> Bar : return Bar () type_name ( \"Bars\" )( list [ Bar ]) assert serialization_schema ( list [ Foo ], conversion = foo_to_bar , all_refs = True ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"$ref\" : \"#/$defs/Bars\" , \"$defs\" : { # Bars is present because `list[Foo]` is dynamically converted to `list[Bar]` \"Bars\" : { \"type\" : \"array\" , \"items\" : { \"$ref\" : \"#/$defs/Bar\" }}, \"Bar\" : { \"type\" : \"object\" , \"additionalProperties\" : False }, }, }","title":"Dynamic conversions interact with type_name"},{"location":"conversions/#bypass-registered-conversion","text":"Using apischema.conversion.identity as a dynamic conversion allows to bypass a registered conversion, i.e. to (de)serialize the given type as it would be without conversion registered. from dataclasses import dataclass from apischema import serialize , serializer from apischema.conversions import Conversion , identity @dataclass class RGB : red : int green : int blue : int @serializer @property def hexa ( self ) -> str : return f \"# { self . red : 02x }{ self . green : 02x }{ self . blue : 02x } \" assert serialize ( RGB , RGB ( 0 , 0 , 0 )) == \"#000000\" # dynamic conversion used to bypass the registered one assert serialize ( RGB , RGB ( 0 , 0 , 0 ), conversion = identity ) == { \"red\" : 0 , \"green\" : 0 , \"blue\" : 0 , } # Expended bypass form assert serialize ( RGB , RGB ( 0 , 0 , 0 ), conversion = Conversion ( identity , source = RGB , target = RGB ) ) == { \"red\" : 0 , \"green\" : 0 , \"blue\" : 0 } Note For a more precise selection of bypassed conversion, for tuple or Union member for example, it's possible to pass the concerned class as the source and the target of conversion with identity converter, as shown in the example.","title":"Bypass registered conversion"},{"location":"conversions/#liskov-substitution-principle","text":"LSP is taken in account when applying dynamic conversion: serializer source can be a subclass of the actual class and deserializer target can be a superclass of the actual class. from dataclasses import dataclass from apischema import deserialize , serialize @dataclass class Foo : field : int @dataclass class Bar ( Foo ): other : str def foo_to_int ( foo : Foo ) -> int : return foo . field def bar_from_int ( i : int ) -> Bar : return Bar ( i , str ( i )) assert serialize ( Bar , Bar ( 0 , \"\" ), conversion = foo_to_int ) == 0 assert deserialize ( Foo , 0 , conversion = bar_from_int ) == Bar ( 0 , \"0\" )","title":"Liskov substitution principle"},{"location":"conversions/#generic-dynamic-conversions","text":"Generic dynamic conversions are supported out of the box. Also, contrary to registered conversions, partially specialized generics are allowed. from collections.abc import Mapping , Sequence from operator import itemgetter from typing import TypeVar from apischema import serialize from apischema.json_schema import serialization_schema T = TypeVar ( \"T\" ) Priority = int def sort_by_priority ( values_with_priority : Mapping [ T , Priority ]) -> Sequence [ T ]: return [ k for k , _ in sorted ( values_with_priority . items (), key = itemgetter ( 1 ))] assert serialize ( dict [ str , Priority ], { \"a\" : 1 , \"b\" : 0 }, conversion = sort_by_priority ) == [ \"b\" , \"a\" ] assert serialization_schema ( dict [ str , Priority ], conversion = sort_by_priority ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"type\" : \"array\" , \"items\" : { \"type\" : \"string\" }, }","title":"Generic dynamic conversions"},{"location":"conversions/#field-conversions","text":"It is possible to register a conversion for a particular dataclass field using conversion metadata. import os import time from dataclasses import dataclass , field from datetime import datetime from apischema import deserialize , serialize from apischema.conversions import Conversion from apischema.metadata import conversion # Set UTC timezone for example os . environ [ \"TZ\" ] = \"UTC\" time . tzset () from_timestamp = Conversion ( datetime . fromtimestamp , source = int , target = datetime ) def to_timestamp ( d : datetime ) -> int : return int ( d . timestamp ()) @dataclass class Foo : some_date : datetime = field ( metadata = conversion ( from_timestamp , to_timestamp )) other_date : datetime assert deserialize ( Foo , { \"some_date\" : 0 , \"other_date\" : \"2019-10-13\" }) == Foo ( datetime ( 1970 , 1 , 1 ), datetime ( 2019 , 10 , 13 ) ) assert serialize ( Foo , Foo ( datetime ( 1970 , 1 , 1 ), datetime ( 2019 , 10 , 13 ))) == { \"some_date\" : 0 , \"other_date\" : \"2019-10-13T00:00:00\" , } Note It's possible to pass a conversion only for deserialization or only for serialization","title":"Field conversions"},{"location":"conversions/#serialized-method-conversions","text":"Serialized methods can also have dedicated conversions for their return import os import time from dataclasses import dataclass from datetime import datetime from apischema import serialize , serialized # Set UTC timezone for example os . environ [ \"TZ\" ] = \"UTC\" time . tzset () def to_timestamp ( d : datetime ) -> int : return int ( d . timestamp ()) @dataclass class Foo : @serialized ( conversion = to_timestamp ) def some_date ( self ) -> datetime : return datetime ( 1970 , 1 , 1 ) assert serialize ( Foo , Foo ()) == { \"some_date\" : 0 }","title":"Serialized method conversions"},{"location":"conversions/#string-conversions","text":"A common pattern of conversion concerns class having a string constructor and a __str__ method; standard types uuid.UUID , pathlib.Path , ipaddress.IPv4Address are concerned. Using apischema.conversions.as_str will register a string-deserializer from the constructor and a string-serializer from the __str__ method. import bson from pytest import raises from apischema import Unsupported , deserialize , serialize from apischema.conversions import as_str with raises ( Unsupported ): deserialize ( bson . ObjectId , \"0123456789ab0123456789ab\" ) with raises ( Unsupported ): serialize ( bson . ObjectId , bson . ObjectId ( \"0123456789ab0123456789ab\" )) as_str ( bson . ObjectId ) assert deserialize ( bson . ObjectId , \"0123456789ab0123456789ab\" ) == bson . ObjectId ( \"0123456789ab0123456789ab\" ) assert ( serialize ( bson . ObjectId , bson . ObjectId ( \"0123456789ab0123456789ab\" )) == \"0123456789ab0123456789ab\" ) Note Previously mentioned standard types are handled by apischema using as_str .","title":"String conversions"},{"location":"conversions/#object-deserialization-transform-function-into-a-dataclass-deserializer","text":"apischema.objects.object_deserialization can convert a function into a new function taking a unique parameter, a dataclass whose fields are mapped from the original function parameters. It can be used for example to build a deserialization conversion from an alternative constructor. from apischema import deserialize , deserializer , type_name from apischema.json_schema import deserialization_schema from apischema.objects import object_deserialization def create_range ( start : int , stop : int , step : int = 1 ) -> range : return range ( start , stop , step ) range_conv = object_deserialization ( create_range , type_name ( \"Range\" )) # Conversion can be registered deserializer ( range_conv ) assert deserialize ( range , { \"start\" : 0 , \"stop\" : 10 }) == range ( 0 , 10 ) assert deserialization_schema ( range ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"type\" : \"object\" , \"properties\" : { \"start\" : { \"type\" : \"integer\" }, \"stop\" : { \"type\" : \"integer\" }, \"step\" : { \"type\" : \"integer\" , \"default\" : 1 }, }, \"required\" : [ \"start\" , \"stop\" ], \"additionalProperties\" : False , } Note Parameters metadata can be specified using typing.Annotated , or be passed with parameters_metadata parameter, which is a mapping of parameter names as key and mapped metadata as value.","title":"Object deserialization \u2014 transform function into a dataclass deserializer"},{"location":"conversions/#object-serialization-select-only-a-subset-of-fields","text":"apischema.objects.object_serialization can be used to serialize only a subset of an object fields and methods. from dataclasses import dataclass from typing import Any from apischema import alias , serialize , type_name from apischema.json_schema import JsonSchemaVersion , definitions_schema from apischema.objects import get_field , object_serialization @dataclass class Data : id : int content : str @property def size ( self ) -> int : return len ( self . content ) def get_details ( self ) -> Any : ... # Serialization fields can be a str/field or a function/method/property size_only = object_serialization ( Data , [ get_field ( Data ) . id , Data . size ], type_name ( \"DataSize\" ) ) # [\"id\", Data.size] would also work def complete_data (): return [ ... , # shortcut to include all the fields Data . size , ( Data . get_details , alias ( \"details\" )), # add/override metadata using tuple ] # Serialization fields computation can be deferred in a function # The serialization name will then be defaulted to the function name complete = object_serialization ( Data , complete_data ) data = Data ( 0 , \"data\" ) assert serialize ( Data , data , conversion = size_only ) == { \"id\" : 0 , \"size\" : 4 } assert serialize ( Data , data , conversion = complete ) == { \"id\" : 0 , \"content\" : \"data\" , \"size\" : 4 , \"details\" : None , # because get_details return None in this example } assert definitions_schema ( serialization = [( Data , size_only ), ( Data , complete )], version = JsonSchemaVersion . OPEN_API_3_0 , ) == { \"DataSize\" : { \"type\" : \"object\" , \"properties\" : { \"id\" : { \"type\" : \"integer\" }, \"size\" : { \"type\" : \"integer\" }}, \"required\" : [ \"id\" , \"size\" ], \"additionalProperties\" : False , }, \"CompleteData\" : { \"type\" : \"object\" , \"properties\" : { \"id\" : { \"type\" : \"integer\" }, \"content\" : { \"type\" : \"string\" }, \"size\" : { \"type\" : \"integer\" }, \"details\" : {}, }, \"required\" : [ \"id\" , \"content\" , \"size\" , \"details\" ], \"additionalProperties\" : False , }, }","title":"Object serialization \u2014 select only a subset of fields"},{"location":"conversions/#default-conversions","text":"As almost every default behavior in apischema , default conversions can be configured using apischema.settings.deserialization.default_conversion / apischema.settings.serialization.default_conversion . The initial value of these settings are the function which retrieved conversions registered with deserializer / serializer . You can for example support attrs classes with this feature: from typing import Optional , Sequence import attr from apischema import deserialize , serialize , settings from apischema.json_schema import deserialization_schema from apischema.objects import ObjectField prev_default_object_fields = settings . default_object_fields def attrs_fields ( cls : type ) -> Optional [ Sequence [ ObjectField ]]: if hasattr ( cls , \"__attrs_attrs__\" ): return [ ObjectField ( a . name , a . type , required = a . default == attr . NOTHING , default = a . default ) for a in getattr ( cls , \"__attrs_attrs__\" ) ] else : return prev_default_object_fields ( cls ) settings . default_object_fields = attrs_fields @attr . s class Foo : bar : int = attr . ib () assert deserialize ( Foo , { \"bar\" : 0 }) == Foo ( 0 ) assert serialize ( Foo , Foo ( 0 )) == { \"bar\" : 0 } assert deserialization_schema ( Foo ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"type\" : \"object\" , \"properties\" : { \"bar\" : { \"type\" : \"integer\" }}, \"required\" : [ \"bar\" ], \"additionalProperties\" : False , } apischema functions ( deserialize / serialize / deserialization_schema / serialization_schema / definitions_schema ) also have a default_conversion parameter to dynamically modify default conversions. See FAQ for the difference between conversion and default_conversion parameters.","title":"Default conversions"},{"location":"conversions/#sub-conversions","text":"Sub-conversions are dynamic conversions applied on the result of a conversion. from dataclasses import dataclass from typing import Generic , Optional , TypeVar from apischema.conversions import Conversion from apischema.json_schema import serialization_schema T = TypeVar ( \"T\" ) class Query ( Generic [ T ]): ... def query_to_list ( q : Query [ T ]) -> list [ T ]: ... def query_to_scalar ( q : Query [ T ]) -> Optional [ T ]: ... @dataclass class FooModel : bar : int class Foo : def serialize ( self ) -> FooModel : ... assert serialization_schema ( Query [ Foo ], conversion = Conversion ( query_to_list , sub_conversion = Foo . serialize ) ) == { # We get an array of Foo \"type\" : \"array\" , \"items\" : { \"type\" : \"object\" , \"properties\" : { \"bar\" : { \"type\" : \"integer\" }}, \"required\" : [ \"bar\" ], \"additionalProperties\" : False , }, \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , } Sub-conversions can also be used to bypass registered conversions or to define recursive conversions .","title":"Sub-conversions"},{"location":"conversions/#lazyrecursive-conversions","text":"Conversions can be defined lazily, i.e. using a function returning Conversion (single, or a tuple of it); this function must be wrap into a apischema.conversions.LazyConversion instance. It allows creating recursive conversions or using a conversion object which can be modified after its definition (for example a conversion for a base class modified by __init_subclass__ ) It is used by apischema itself for the generated JSON schema. It is indeed a recursive data, and the different versions are handled by a conversion with a lazy recursive sub-conversion. from dataclasses import dataclass from typing import Union from apischema import serialize from apischema.conversions import Conversion , LazyConversion @dataclass class Foo : elements : list [ Union [ int , \"Foo\" ]] def foo_elements ( foo : Foo ) -> list [ Union [ int , Foo ]]: return foo . elements # Recursive conversion pattern tmp = None conversion = Conversion ( foo_elements , sub_conversion = LazyConversion ( lambda : tmp )) tmp = conversion assert serialize ( Foo , Foo ([ 0 , Foo ([ 1 ])]), conversion = conversion ) == [ 0 , [ 1 ]] # Without the recursive sub-conversion, it would have been: assert serialize ( Foo , Foo ([ 0 , Foo ([ 1 ])]), conversion = foo_elements ) == [ 0 , { \"elements\" : [ 1 ]}, ]","title":"Lazy/recursive conversions"},{"location":"conversions/#lazy-registered-conversions","text":"Lazy conversions can also be registered, but the deserialization target/serialization source has to be passed too. from dataclasses import dataclass from apischema import deserialize , deserializer , serialize , serializer from apischema.conversions import Conversion @dataclass class Foo : bar : int deserializer ( lazy = lambda : Conversion ( lambda bar : Foo ( bar ), source = int , target = Foo ), target = Foo ) serializer ( lazy = lambda : Conversion ( lambda foo : foo . bar , source = Foo , target = int ), source = Foo ) assert deserialize ( Foo , 0 ) == Foo ( 0 ) assert serialize ( Foo , Foo ( 0 )) == 0","title":"Lazy registered conversions"},{"location":"conversions/#faq","text":"","title":"FAQ"},{"location":"conversions/#whats-the-difference-between-conversion-and-default_conversion-parameters","text":"Dynamic conversions ( conversion parameter) exists to ensure consistency and reuse of subschemas referenced (with a $ref ) in the JSON/ OpenAPI schema. In fact, different global conversions ( default_conversion parameter) could lead to have a field with different schemas depending on global conversions, so class would not be able to be referenced consistently. Because dynamic conversions are local, they cannot mess with an objet field schema. Schema generation use the same default conversions for all definitions (which can have associated dynamic conversion). default_conversion parameter allows having different (de)serialization contexts, for example to map date to string between frontend and backend, and to timestamp between backend services.","title":"What's the difference between conversion and default_conversion parameters?"},{"location":"data_model/","text":"Data model \u00b6 apischema handle every classes/types you need. By the way, it's done in an additive way, meaning that it doesn't affect your types. PEP 585 \u00b6 With Python 3.9 and PEP 585 , typing is substantially shaken up; all container types of typing module are now deprecated. apischema fully support 3.9 and PEP 585, as shown in the different examples. However, typing containers can still be used, especially/necessarily when using an older version. Dataclasses \u00b6 Because the library aims to bring the minimum boilerplate, it's build on the top of standard library. Dataclasses are thus the core structure of the data model. Dataclasses bring the possibility of field customization, with more than just a default value. In addition to the common parameters of dataclasses.field , customization is done with metadata parameter; metadata can also be passed using PEP 593 typing.Annotated . With some teasing of features presented later: from dataclasses import dataclass , field from typing import Annotated from apischema import alias , schema from apischema.metadata import required @dataclass class Foo : bar : int = field ( default = 0 , metadata = alias ( \"foo_bar\" ) | schema ( title = \"foo! bar!\" , min = 0 , max = 42 ) | required , ) baz : Annotated [ int , alias ( \"foo_baz\" ), schema ( title = \"foo! baz!\" , min = 0 , max = 32 ), required ] = 0 # pipe `|` operator can also be used in Annotated Note Field's metadata are just an ordinary dict ; apischema provides some functions to enrich these metadata with it's own keys ( alias(\"foo_bar) is roughly equivalent to `{\"_apischema_alias\": \"foo_bar\"}) and use them when the time comes, but metadata are not reserved to apischema and other keys can be added. Because PEP 584 is painfully missing before Python 3.9, apischema metadata use their own subclass of dict just to add | operator for convenience in all Python versions. Dataclasses __post_init__ and field(init=False) are fully supported. Implication of this feature usage is documented in the relative sections. Warning Before 3.8, InitVar is doing type erasure , that's why it's not possible for apischema to retrieve type information of init variables. To fix this behavior, a field metadata init_var can be used to put back the type of the field ( init_var also accepts stringified type annotations). Dataclass-like types ( attrs / SQLAlchemy /etc.) can also get support with a few lines of code, see next section Standard library types \u00b6 apischema handle natively most of the types provided by the standard library. They are sorted in the following categories: Primitive \u00b6 str , int , float , bool , None , subclasses of them They correspond to JSON primitive types. Collection \u00b6 collection.abc.Collection ( typing.Collection ) collection.abc.Sequence ( typing.Sequence ) tuple ( typing.Tuple ) collection.abc.MutableSequence ( typing.MutableSequence ) list ( typing.List ) collection.abc.Set ( typing.AbstractSet ) collection.abc.MutableSet ( typing.MutableSet ) frozenset ( typing.FrozenSet ) set ( typing.Set ) They correspond to JSON array and are serialized to list . Some of them are abstract; deserialization will instantiate a concrete child class. For example collection.abc.Sequence will be instantiated with tuple while collection.MutableSequence will be instantiated with list . Mapping \u00b6 collection.abc.Mapping ( typing.Mapping ) collection.abc.MutableMapping ( typing.MutableMapping ) dict ( typing.Dict ) They correpond to JSON object and are serialized to dict . Enumeration \u00b6 enum.Enum subclasses, typing.Literal For Enum , this is the value and not the attribute name that is serialized Typing facilities \u00b6 typing.Optional / typing.Union ( Optional[T] is strictly equivalent to Union[T, None] ) Deserialization select the first matching alternative (see below how to skip some union member ) tuple ( typing.Tuple ) Can be used as collection as well as true tuple, like tuple[str, int] typing.NewType Serialized according to its base type typing.NamedTuple Handled as an object type, roughly like a dataclass; fields metadata can be passed using Annotated typing.TypedDict Hanlded as an object type, but with a dictionary shape; fields metadata can be passed using Annotated typing.Any Untouched by deserialization Other standard library types \u00b6 bytes with str (de)serialization using base64 encoding datetime.datetime datetime.date datetime.time Supported only in 3.7+ with fromisoformat / isoformat Decimal With float (de)serialization ipaddress.IPv4Address ipaddress.IPv4Interface ipaddress.IPv4Network ipaddress.IPv6Address ipaddress.IPv6Interface ipaddress.IPv6Network pathlib.Path re.Pattern ( typing.Pattern ) uuid.UUID With str (de)serialization Generic \u00b6 typing.Generic can be used out of the box like in the following example: from dataclasses import dataclass from typing import Generic , TypeVar from pytest import raises from apischema import ValidationError , deserialize T = TypeVar ( \"T\" ) @dataclass class Box ( Generic [ T ]): content : T assert deserialize ( Box [ str ], { \"content\" : \"void\" }) == Box ( \"void\" ) with raises ( ValidationError ): deserialize ( Box [ str ], { \"content\" : 42 }) Recursive types, string annotations and PEP 563 \u00b6 Recursive classes can be typed as they usually do, with or without PEP 563 . Here with string annotations: from dataclasses import dataclass from typing import Optional from apischema import deserialize @dataclass class Node : value : int child : Optional [ \"Node\" ] = None assert deserialize ( Node , { \"value\" : 0 , \"child\" : { \"value\" : 1 }}) == Node ( 0 , Node ( 1 )) Here with PEP 563 (requires 3.7+) from __future__ import annotations from dataclasses import dataclass from typing import Optional from apischema import deserialize @dataclass class Node : value : int child : Optional [ Node ] = None assert deserialize ( Node , { \"value\" : 0 , \"child\" : { \"value\" : 1 }}) == Node ( 0 , Node ( 1 )) Warning To resolve annotations, apischema uses typing.get_type_hints ; this doesn't work really well when used on objects defined outside of global scope. Warning (minor) Currently, PEP 585 can have surprising behavior when used outside the box, see bpo-41370 null vs undefined \u00b6 Contrary to Javascript, Python doesn't have an undefined equivalent (if we consider None to be null equivalent). But it can be useful to distinguish (especially when thinkinn about HTTP PATCH method) between a null field and an undefined /absent field. That's why apischema provides an Undefined constant (a single instance of UndefinedType class) which can be used as a default value everywhere where this distinction is needed. In fact, default values are used when field are absent, thus a default Undefined will mark the field as absent. Dataclass/ NamedTuple fields are ignored by serialization when Undefined . from dataclasses import dataclass from typing import Union from apischema import Undefined , UndefinedType , deserialize , serialize from apischema.json_schema import deserialization_schema @dataclass class Foo : bar : Union [ int , UndefinedType ] = Undefined baz : Union [ int , UndefinedType , None ] = Undefined assert deserialize ( Foo , { \"bar\" : 0 , \"baz\" : None }) == Foo ( 0 , None ) assert deserialize ( Foo , {}) == Foo ( Undefined , Undefined ) assert serialize ( Foo , Foo ( Undefined , 42 )) == { \"baz\" : 42 } # Foo.bar and Foo.baz are not required assert deserialization_schema ( Foo ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"type\" : \"object\" , \"properties\" : { \"bar\" : { \"type\" : \"integer\" }, \"baz\" : { \"type\" : [ \"integer\" , \"null\" ]}}, \"additionalProperties\" : False , } Note UndefinedType must only be used inside an Union , as it has no sense as a standalone type. By the way, no suitable name was found to shorten Union[T, UndefinedType] but propositions are welcomed. Note Undefined is a falsy constant, i.e. bool(Undefined) is False . Annotated - PEP 593 \u00b6 PEP 593 is fully supported; annotations stranger to apischema are simply ignored. Skip Union member \u00b6 Sometimes, one of the Union members has not to be taken in account during validation; it can just be here to match the type of the default value of the field. This member can be marked as to be skipped with PEP 593 Annotated and apischema.skip.Skip from dataclasses import dataclass from typing import Annotated , Union from pytest import raises from apischema import ValidationError , deserialize , serialize from apischema.skip import Skip @dataclass class Foo : bar : Union [ int , Annotated [ None , Skip ]] = None with raises ( ValidationError ) as err : deserialize ( Foo , { \"bar\" : None }) assert serialize ( err . value ) == [ { \"loc\" : [ \"bar\" ], \"err\" : [ \"expected type integer, found null\" ]} ] Optional vs. NotNull \u00b6 Optional type is not always appropriate, because it allows deserialized value to be null , but sometimes, you just want None as a default value for unset fields, not an authorized one. That's why apischema defines a NotNull type; in fact, NotNull = Union[T, Annotated[None, Skip]] . from dataclasses import dataclass from pytest import raises from apischema import ValidationError , deserialize , serialize from apischema.skip import NotNull @dataclass class Foo : # NotNull is exactly like Optional for type checkers, # it's only interpreted differently by apischema bar : NotNull [ int ] = None with raises ( ValidationError ) as err : deserialize ( Foo , { \"bar\" : None }) assert serialize ( err . value ) == [ { \"loc\" : [ \"bar\" ], \"err\" : [ \"expected type integer, found null\" ]} ] Note You can also use Undefined , but it can be more convenient to directly manipulate an Optional field, especially in the rest of the code unrelated to (de)serialization. Custom types \u00b6 apischema can support almost all of your custom types in a few lines of code, using conversion feature . However, it also provides a simple and direct way to support dataclass-like types, as presented below . Otherwise, when apischema encounters a type that it doesn't support, apischema.Unsupported exception will be raised. Dataclass-like types, aka object types \u00b6 Internally, apischema handle standard object types \u2014 dataclasses, named tuple and typed dictionary \u2014 the same way by mapping them to a set of apischema.objects.ObjectField , which has the following definition: @dataclass ( frozen = True ) class ObjectField : name : str # field's name type : Any # field's type required : bool = True # if the field is required metadata : Mapping [ str , Any ] = field ( default_factory = dict ) # field's metadata default : InitVar [ Any ] = ... # field's default value default_factory : Optional [ Callable [[], Any ]] = None # field's default factory kind : FieldKind = FieldKind . NORMAL # NORMAL/READ_ONLY/WRITE_ONLY Thus, support of dataclass-like types ( attrs , SQLAlchemy traditional mappers, etc.) can be achieved by mapping the concerned class to its own list of ObjectField s; this is done using apischema.objects.set_object_fields . from apischema import deserialize , serialize from apischema.json_schema import deserialization_schema from apischema.objects import ObjectField , set_object_fields class Foo : def __init__ ( self , bar ): self . bar = bar set_object_fields ( Foo , [ ObjectField ( \"bar\" , int )]) # Fields can also be passed in a factory set_object_fields ( Foo , lambda : [ ObjectField ( \"bar\" , int )]) foo = deserialize ( Foo , { \"bar\" : 0 }) assert isinstance ( foo , Foo ) and foo . bar == 0 assert serialize ( Foo , Foo ( 0 )) == { \"bar\" : 0 } assert deserialization_schema ( Foo ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"type\" : \"object\" , \"properties\" : { \"bar\" : { \"type\" : \"integer\" }}, \"required\" : [ \"bar\" ], \"additionalProperties\" : False , } Another way to set object fields is to directly modify apischema default behavior, using apischema.settings.default_object_fields . from collections.abc import Sequence from typing import Optional from apischema import settings from apischema.objects import ObjectField previous_default_object_fields = settings . default_object_field def default_object_fields ( cls ) -> Optional [ Sequence [ ObjectField ]]: return [ ... ] if ... else previous_default_object_fields ( cls ) settings . default_object_fields = default_object_fields Note Almost every default behavior of apischema can be customized using apischema.settings . Examples of SQLAlchemy support and attrs support illustrate both methods (which could also be combined). Skip field \u00b6 Dataclass fields can be excluded from apischema processing by using apischema.metadata.skip in the field metadata from dataclasses import dataclass , field from apischema.json_schema import deserialization_schema from apischema.metadata import skip @dataclass class Foo : bar : int baz : str = field ( default = \"baz\" , metadata = skip ) assert deserialization_schema ( Foo ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"type\" : \"object\" , \"properties\" : { \"bar\" : { \"type\" : \"integer\" }}, \"required\" : [ \"bar\" ], \"additionalProperties\" : False , } Composition over inheritance - composed dataclasses flattening \u00b6 Dataclass fields which are themselves dataclass can be \"flattened\" into the owning one by using flattened metadata. Then, when the class will be (de)serialized, \"flattened\" fields will be (de)serialized at the same level than the owning class. from dataclasses import dataclass , field from typing import Union from apischema import Undefined , UndefinedType , alias , deserialize , serialize from apischema.fields import with_fields_set from apischema.json_schema import deserialization_schema from apischema.metadata import flattened @dataclass class JsonSchema : title : Union [ str , UndefinedType ] = Undefined description : Union [ str , UndefinedType ] = Undefined format : Union [ str , UndefinedType ] = Undefined ... @with_fields_set @dataclass class RootJsonSchema : schema : Union [ str , UndefinedType ] = field ( default = Undefined , metadata = alias ( \"$schema\" ) ) defs : list [ JsonSchema ] = field ( default_factory = list , metadata = alias ( \"$defs\" )) # This field schema is flattened inside the owning one json_schema : JsonSchema = field ( default = JsonSchema (), metadata = flattened ) data = { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"title\" : \"flattened example\" , } root_schema = RootJsonSchema ( schema = \"http://json-schema.org/draft/2019-09/schema#\" , json_schema = JsonSchema ( title = \"flattened example\" ), ) assert deserialize ( RootJsonSchema , data ) == root_schema assert serialize ( RootJsonSchema , root_schema ) == data assert deserialization_schema ( RootJsonSchema ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"$defs\" : { \"JsonSchema\" : { \"type\" : \"object\" , \"properties\" : { \"title\" : { \"type\" : \"string\" }, \"description\" : { \"type\" : \"string\" }, \"format\" : { \"type\" : \"string\" }, }, \"additionalProperties\" : False , } }, \"type\" : \"object\" , # It results in allOf + unevaluatedProperties=False \"allOf\" : [ # RootJsonSchema (without JsonSchema) { \"type\" : \"object\" , \"properties\" : { \"$schema\" : { \"type\" : \"string\" }, \"$defs\" : { \"type\" : \"array\" , \"items\" : { \"$ref\" : \"#/$defs/JsonSchema\" }, \"default\" : [], }, }, \"additionalProperties\" : False , }, # JonsSchema { \"$ref\" : \"#/$defs/JsonSchema\" }, ], \"unevaluatedProperties\" : False , } Note This feature use JSON schema draft 2019-09 unevaluatedProperties keyword . However, this keyword is removed when JSON schema is converted in a version that doesn't support it, like OpenAPI 3.0. This feature is very convenient for building model by composing smaller components. If some kind of reuse could also be achieved with inheritance, it can be less practical when it comes to use it in code, because there is no easy way to build an inherited class when you have an instance of the super class ; you have to copy all the fields by hand. On the other hand, using composition (of flattened fields), it's easy to instantiate the class when the smaller component is just a field of it. FAQ \u00b6 Why Iterable is not handled with other collection type? \u00b6 Iterable could be handled (actually, it was at the beginning), however, this doesn't really make sense from a data point of view. Iterable are computation objects, they can be infinite, etc. They don't correspond to a serialized data; Collection is way more appropriate in this context. What happens if I override dataclass __init__ ? \u00b6 apischema always assumes that dataclass __init__ can be called with all its fields as kwargs parameters. If that's no more the case after a modification of __init__ (what means if an exception is thrown when the constructor is called because of bad parameters), apischema treats then the class as not supported .","title":"Data model"},{"location":"data_model/#data-model","text":"apischema handle every classes/types you need. By the way, it's done in an additive way, meaning that it doesn't affect your types.","title":"Data model"},{"location":"data_model/#pep-585","text":"With Python 3.9 and PEP 585 , typing is substantially shaken up; all container types of typing module are now deprecated. apischema fully support 3.9 and PEP 585, as shown in the different examples. However, typing containers can still be used, especially/necessarily when using an older version.","title":"PEP 585"},{"location":"data_model/#dataclasses","text":"Because the library aims to bring the minimum boilerplate, it's build on the top of standard library. Dataclasses are thus the core structure of the data model. Dataclasses bring the possibility of field customization, with more than just a default value. In addition to the common parameters of dataclasses.field , customization is done with metadata parameter; metadata can also be passed using PEP 593 typing.Annotated . With some teasing of features presented later: from dataclasses import dataclass , field from typing import Annotated from apischema import alias , schema from apischema.metadata import required @dataclass class Foo : bar : int = field ( default = 0 , metadata = alias ( \"foo_bar\" ) | schema ( title = \"foo! bar!\" , min = 0 , max = 42 ) | required , ) baz : Annotated [ int , alias ( \"foo_baz\" ), schema ( title = \"foo! baz!\" , min = 0 , max = 32 ), required ] = 0 # pipe `|` operator can also be used in Annotated Note Field's metadata are just an ordinary dict ; apischema provides some functions to enrich these metadata with it's own keys ( alias(\"foo_bar) is roughly equivalent to `{\"_apischema_alias\": \"foo_bar\"}) and use them when the time comes, but metadata are not reserved to apischema and other keys can be added. Because PEP 584 is painfully missing before Python 3.9, apischema metadata use their own subclass of dict just to add | operator for convenience in all Python versions. Dataclasses __post_init__ and field(init=False) are fully supported. Implication of this feature usage is documented in the relative sections. Warning Before 3.8, InitVar is doing type erasure , that's why it's not possible for apischema to retrieve type information of init variables. To fix this behavior, a field metadata init_var can be used to put back the type of the field ( init_var also accepts stringified type annotations). Dataclass-like types ( attrs / SQLAlchemy /etc.) can also get support with a few lines of code, see next section","title":"Dataclasses"},{"location":"data_model/#standard-library-types","text":"apischema handle natively most of the types provided by the standard library. They are sorted in the following categories:","title":"Standard library types"},{"location":"data_model/#primitive","text":"str , int , float , bool , None , subclasses of them They correspond to JSON primitive types.","title":"Primitive"},{"location":"data_model/#collection","text":"collection.abc.Collection ( typing.Collection ) collection.abc.Sequence ( typing.Sequence ) tuple ( typing.Tuple ) collection.abc.MutableSequence ( typing.MutableSequence ) list ( typing.List ) collection.abc.Set ( typing.AbstractSet ) collection.abc.MutableSet ( typing.MutableSet ) frozenset ( typing.FrozenSet ) set ( typing.Set ) They correspond to JSON array and are serialized to list . Some of them are abstract; deserialization will instantiate a concrete child class. For example collection.abc.Sequence will be instantiated with tuple while collection.MutableSequence will be instantiated with list .","title":"Collection"},{"location":"data_model/#mapping","text":"collection.abc.Mapping ( typing.Mapping ) collection.abc.MutableMapping ( typing.MutableMapping ) dict ( typing.Dict ) They correpond to JSON object and are serialized to dict .","title":"Mapping"},{"location":"data_model/#enumeration","text":"enum.Enum subclasses, typing.Literal For Enum , this is the value and not the attribute name that is serialized","title":"Enumeration"},{"location":"data_model/#typing-facilities","text":"typing.Optional / typing.Union ( Optional[T] is strictly equivalent to Union[T, None] ) Deserialization select the first matching alternative (see below how to skip some union member ) tuple ( typing.Tuple ) Can be used as collection as well as true tuple, like tuple[str, int] typing.NewType Serialized according to its base type typing.NamedTuple Handled as an object type, roughly like a dataclass; fields metadata can be passed using Annotated typing.TypedDict Hanlded as an object type, but with a dictionary shape; fields metadata can be passed using Annotated typing.Any Untouched by deserialization","title":"Typing facilities"},{"location":"data_model/#other-standard-library-types","text":"bytes with str (de)serialization using base64 encoding datetime.datetime datetime.date datetime.time Supported only in 3.7+ with fromisoformat / isoformat Decimal With float (de)serialization ipaddress.IPv4Address ipaddress.IPv4Interface ipaddress.IPv4Network ipaddress.IPv6Address ipaddress.IPv6Interface ipaddress.IPv6Network pathlib.Path re.Pattern ( typing.Pattern ) uuid.UUID With str (de)serialization","title":"Other standard library types"},{"location":"data_model/#generic","text":"typing.Generic can be used out of the box like in the following example: from dataclasses import dataclass from typing import Generic , TypeVar from pytest import raises from apischema import ValidationError , deserialize T = TypeVar ( \"T\" ) @dataclass class Box ( Generic [ T ]): content : T assert deserialize ( Box [ str ], { \"content\" : \"void\" }) == Box ( \"void\" ) with raises ( ValidationError ): deserialize ( Box [ str ], { \"content\" : 42 })","title":"Generic"},{"location":"data_model/#recursive-types-string-annotations-and-pep-563","text":"Recursive classes can be typed as they usually do, with or without PEP 563 . Here with string annotations: from dataclasses import dataclass from typing import Optional from apischema import deserialize @dataclass class Node : value : int child : Optional [ \"Node\" ] = None assert deserialize ( Node , { \"value\" : 0 , \"child\" : { \"value\" : 1 }}) == Node ( 0 , Node ( 1 )) Here with PEP 563 (requires 3.7+) from __future__ import annotations from dataclasses import dataclass from typing import Optional from apischema import deserialize @dataclass class Node : value : int child : Optional [ Node ] = None assert deserialize ( Node , { \"value\" : 0 , \"child\" : { \"value\" : 1 }}) == Node ( 0 , Node ( 1 )) Warning To resolve annotations, apischema uses typing.get_type_hints ; this doesn't work really well when used on objects defined outside of global scope. Warning (minor) Currently, PEP 585 can have surprising behavior when used outside the box, see bpo-41370","title":"Recursive types, string annotations and PEP 563"},{"location":"data_model/#null-vs-undefined","text":"Contrary to Javascript, Python doesn't have an undefined equivalent (if we consider None to be null equivalent). But it can be useful to distinguish (especially when thinkinn about HTTP PATCH method) between a null field and an undefined /absent field. That's why apischema provides an Undefined constant (a single instance of UndefinedType class) which can be used as a default value everywhere where this distinction is needed. In fact, default values are used when field are absent, thus a default Undefined will mark the field as absent. Dataclass/ NamedTuple fields are ignored by serialization when Undefined . from dataclasses import dataclass from typing import Union from apischema import Undefined , UndefinedType , deserialize , serialize from apischema.json_schema import deserialization_schema @dataclass class Foo : bar : Union [ int , UndefinedType ] = Undefined baz : Union [ int , UndefinedType , None ] = Undefined assert deserialize ( Foo , { \"bar\" : 0 , \"baz\" : None }) == Foo ( 0 , None ) assert deserialize ( Foo , {}) == Foo ( Undefined , Undefined ) assert serialize ( Foo , Foo ( Undefined , 42 )) == { \"baz\" : 42 } # Foo.bar and Foo.baz are not required assert deserialization_schema ( Foo ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"type\" : \"object\" , \"properties\" : { \"bar\" : { \"type\" : \"integer\" }, \"baz\" : { \"type\" : [ \"integer\" , \"null\" ]}}, \"additionalProperties\" : False , } Note UndefinedType must only be used inside an Union , as it has no sense as a standalone type. By the way, no suitable name was found to shorten Union[T, UndefinedType] but propositions are welcomed. Note Undefined is a falsy constant, i.e. bool(Undefined) is False .","title":"null vs undefined"},{"location":"data_model/#annotated-pep-593","text":"PEP 593 is fully supported; annotations stranger to apischema are simply ignored.","title":"Annotated - PEP 593"},{"location":"data_model/#skip-union-member","text":"Sometimes, one of the Union members has not to be taken in account during validation; it can just be here to match the type of the default value of the field. This member can be marked as to be skipped with PEP 593 Annotated and apischema.skip.Skip from dataclasses import dataclass from typing import Annotated , Union from pytest import raises from apischema import ValidationError , deserialize , serialize from apischema.skip import Skip @dataclass class Foo : bar : Union [ int , Annotated [ None , Skip ]] = None with raises ( ValidationError ) as err : deserialize ( Foo , { \"bar\" : None }) assert serialize ( err . value ) == [ { \"loc\" : [ \"bar\" ], \"err\" : [ \"expected type integer, found null\" ]} ]","title":"Skip Union member"},{"location":"data_model/#optional-vs-notnull","text":"Optional type is not always appropriate, because it allows deserialized value to be null , but sometimes, you just want None as a default value for unset fields, not an authorized one. That's why apischema defines a NotNull type; in fact, NotNull = Union[T, Annotated[None, Skip]] . from dataclasses import dataclass from pytest import raises from apischema import ValidationError , deserialize , serialize from apischema.skip import NotNull @dataclass class Foo : # NotNull is exactly like Optional for type checkers, # it's only interpreted differently by apischema bar : NotNull [ int ] = None with raises ( ValidationError ) as err : deserialize ( Foo , { \"bar\" : None }) assert serialize ( err . value ) == [ { \"loc\" : [ \"bar\" ], \"err\" : [ \"expected type integer, found null\" ]} ] Note You can also use Undefined , but it can be more convenient to directly manipulate an Optional field, especially in the rest of the code unrelated to (de)serialization.","title":"Optional vs. NotNull"},{"location":"data_model/#custom-types","text":"apischema can support almost all of your custom types in a few lines of code, using conversion feature . However, it also provides a simple and direct way to support dataclass-like types, as presented below . Otherwise, when apischema encounters a type that it doesn't support, apischema.Unsupported exception will be raised.","title":"Custom types"},{"location":"data_model/#dataclass-like-types-aka-object-types","text":"Internally, apischema handle standard object types \u2014 dataclasses, named tuple and typed dictionary \u2014 the same way by mapping them to a set of apischema.objects.ObjectField , which has the following definition: @dataclass ( frozen = True ) class ObjectField : name : str # field's name type : Any # field's type required : bool = True # if the field is required metadata : Mapping [ str , Any ] = field ( default_factory = dict ) # field's metadata default : InitVar [ Any ] = ... # field's default value default_factory : Optional [ Callable [[], Any ]] = None # field's default factory kind : FieldKind = FieldKind . NORMAL # NORMAL/READ_ONLY/WRITE_ONLY Thus, support of dataclass-like types ( attrs , SQLAlchemy traditional mappers, etc.) can be achieved by mapping the concerned class to its own list of ObjectField s; this is done using apischema.objects.set_object_fields . from apischema import deserialize , serialize from apischema.json_schema import deserialization_schema from apischema.objects import ObjectField , set_object_fields class Foo : def __init__ ( self , bar ): self . bar = bar set_object_fields ( Foo , [ ObjectField ( \"bar\" , int )]) # Fields can also be passed in a factory set_object_fields ( Foo , lambda : [ ObjectField ( \"bar\" , int )]) foo = deserialize ( Foo , { \"bar\" : 0 }) assert isinstance ( foo , Foo ) and foo . bar == 0 assert serialize ( Foo , Foo ( 0 )) == { \"bar\" : 0 } assert deserialization_schema ( Foo ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"type\" : \"object\" , \"properties\" : { \"bar\" : { \"type\" : \"integer\" }}, \"required\" : [ \"bar\" ], \"additionalProperties\" : False , } Another way to set object fields is to directly modify apischema default behavior, using apischema.settings.default_object_fields . from collections.abc import Sequence from typing import Optional from apischema import settings from apischema.objects import ObjectField previous_default_object_fields = settings . default_object_field def default_object_fields ( cls ) -> Optional [ Sequence [ ObjectField ]]: return [ ... ] if ... else previous_default_object_fields ( cls ) settings . default_object_fields = default_object_fields Note Almost every default behavior of apischema can be customized using apischema.settings . Examples of SQLAlchemy support and attrs support illustrate both methods (which could also be combined).","title":"Dataclass-like types, aka object types"},{"location":"data_model/#skip-field","text":"Dataclass fields can be excluded from apischema processing by using apischema.metadata.skip in the field metadata from dataclasses import dataclass , field from apischema.json_schema import deserialization_schema from apischema.metadata import skip @dataclass class Foo : bar : int baz : str = field ( default = \"baz\" , metadata = skip ) assert deserialization_schema ( Foo ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"type\" : \"object\" , \"properties\" : { \"bar\" : { \"type\" : \"integer\" }}, \"required\" : [ \"bar\" ], \"additionalProperties\" : False , }","title":"Skip field"},{"location":"data_model/#composition-over-inheritance-composed-dataclasses-flattening","text":"Dataclass fields which are themselves dataclass can be \"flattened\" into the owning one by using flattened metadata. Then, when the class will be (de)serialized, \"flattened\" fields will be (de)serialized at the same level than the owning class. from dataclasses import dataclass , field from typing import Union from apischema import Undefined , UndefinedType , alias , deserialize , serialize from apischema.fields import with_fields_set from apischema.json_schema import deserialization_schema from apischema.metadata import flattened @dataclass class JsonSchema : title : Union [ str , UndefinedType ] = Undefined description : Union [ str , UndefinedType ] = Undefined format : Union [ str , UndefinedType ] = Undefined ... @with_fields_set @dataclass class RootJsonSchema : schema : Union [ str , UndefinedType ] = field ( default = Undefined , metadata = alias ( \"$schema\" ) ) defs : list [ JsonSchema ] = field ( default_factory = list , metadata = alias ( \"$defs\" )) # This field schema is flattened inside the owning one json_schema : JsonSchema = field ( default = JsonSchema (), metadata = flattened ) data = { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"title\" : \"flattened example\" , } root_schema = RootJsonSchema ( schema = \"http://json-schema.org/draft/2019-09/schema#\" , json_schema = JsonSchema ( title = \"flattened example\" ), ) assert deserialize ( RootJsonSchema , data ) == root_schema assert serialize ( RootJsonSchema , root_schema ) == data assert deserialization_schema ( RootJsonSchema ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"$defs\" : { \"JsonSchema\" : { \"type\" : \"object\" , \"properties\" : { \"title\" : { \"type\" : \"string\" }, \"description\" : { \"type\" : \"string\" }, \"format\" : { \"type\" : \"string\" }, }, \"additionalProperties\" : False , } }, \"type\" : \"object\" , # It results in allOf + unevaluatedProperties=False \"allOf\" : [ # RootJsonSchema (without JsonSchema) { \"type\" : \"object\" , \"properties\" : { \"$schema\" : { \"type\" : \"string\" }, \"$defs\" : { \"type\" : \"array\" , \"items\" : { \"$ref\" : \"#/$defs/JsonSchema\" }, \"default\" : [], }, }, \"additionalProperties\" : False , }, # JonsSchema { \"$ref\" : \"#/$defs/JsonSchema\" }, ], \"unevaluatedProperties\" : False , } Note This feature use JSON schema draft 2019-09 unevaluatedProperties keyword . However, this keyword is removed when JSON schema is converted in a version that doesn't support it, like OpenAPI 3.0. This feature is very convenient for building model by composing smaller components. If some kind of reuse could also be achieved with inheritance, it can be less practical when it comes to use it in code, because there is no easy way to build an inherited class when you have an instance of the super class ; you have to copy all the fields by hand. On the other hand, using composition (of flattened fields), it's easy to instantiate the class when the smaller component is just a field of it.","title":"Composition over inheritance - composed dataclasses flattening"},{"location":"data_model/#faq","text":"","title":"FAQ"},{"location":"data_model/#why-iterable-is-not-handled-with-other-collection-type","text":"Iterable could be handled (actually, it was at the beginning), however, this doesn't really make sense from a data point of view. Iterable are computation objects, they can be infinite, etc. They don't correspond to a serialized data; Collection is way more appropriate in this context.","title":"Why Iterable is not handled with other collection type?"},{"location":"data_model/#what-happens-if-i-override-dataclass-__init__","text":"apischema always assumes that dataclass __init__ can be called with all its fields as kwargs parameters. If that's no more the case after a modification of __init__ (what means if an exception is thrown when the constructor is called because of bad parameters), apischema treats then the class as not supported .","title":"What happens if I override dataclass __init__?"},{"location":"de_serialization/","text":"(De)serialization \u00b6 apischema aims to help API with deserialization/serialization of data, mostly JSON. Let start again with the overview example from collections.abc import Collection from dataclasses import dataclass , field from typing import Optional from uuid import UUID , uuid4 from graphql import print_schema from pytest import raises from apischema import ValidationError , deserialize , serialize from apischema.graphql import graphql_schema from apischema.json_schema import deserialization_schema # Define a schema with standard dataclasses @dataclass class Resource : id : UUID name : str tags : set [ str ] = field ( default_factory = set ) # Get some data uuid = uuid4 () data = { \"id\" : str ( uuid ), \"name\" : \"wyfo\" , \"tags\" : [ \"some_tag\" ]} # Deserialize data resource = deserialize ( Resource , data ) assert resource == Resource ( uuid , \"wyfo\" , { \"some_tag\" }) # Serialize objects assert serialize ( Resource , resource ) == data # Validate during deserialization with raises ( ValidationError ) as err : # pytest checks exception is raised deserialize ( Resource , { \"id\" : \"42\" , \"name\" : \"wyfo\" }) assert serialize ( err . value ) == [ # ValidationError is serializable { \"loc\" : [ \"id\" ], \"err\" : [ \"badly formed hexadecimal UUID string\" ]} ] # Generate JSON Schema assert deserialization_schema ( Resource ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"type\" : \"object\" , \"properties\" : { \"id\" : { \"type\" : \"string\" , \"format\" : \"uuid\" }, \"name\" : { \"type\" : \"string\" }, \"tags\" : { \"type\" : \"array\" , \"items\" : { \"type\" : \"string\" }, \"uniqueItems\" : True , \"default\" : [], }, }, \"required\" : [ \"id\" , \"name\" ], \"additionalProperties\" : False , } # Define GraphQL operations def resources ( tags : Optional [ Collection [ str ]] = None ) -> Optional [ Collection [ Resource ]]: ... # Generate GraphQL schema schema = graphql_schema ( query = [ resources ], id_types = { UUID }) schema_str = \"\"\" \\ type Query { resources(tags: [String!]): [Resource!] } type Resource { id: ID! name: String! tags: [String!]! } \"\"\" assert print_schema ( schema ) == schema_str Deserialization \u00b6 apischema.deserialize deserializes Python types from JSON-like data: dict / list / str / int / float / bool / None \u2014 in short, what you get when you execute json.loads . Types can be dataclasses as well as list[int] , NewType s, or whatever you want (see conversions to extend deserialization support to every type you want). from collections.abc import Collection, Mapping from dataclasses import dataclass from typing import NewType from apischema import deserialize @dataclass class Foo: bar: str MyInt = NewType(\"MyInt\", int) assert deserialize(Foo, {\"bar\": \"bar\"}) == Foo(\"bar\") assert deserialize(MyInt, 0) == MyInt(0) == 0 assert deserialize(Mapping[str, Collection[Foo]], {\"key\": [{\"bar\": \"42\"}]}) == { \"key\": (Foo(\"42\"),) } Deserialization performs a validation of data, based on typing annotations and other information (see schema and validation ). Strictness \u00b6 Coercion \u00b6 apischema is strict by default. You ask for an integer, you have to receive an integer. However, in some cases, data has to be be coerced, for example when parsing aconfiguration file. That can be done using coerce parameter; when set to True , all primitive types will be coerce to the expected type of the data model like the following: from pytest import raises from apischema import ValidationError , deserialize with raises ( ValidationError ): deserialize ( bool , \"ok\" ) assert deserialize ( bool , \"ok\" , coerce = True ) bool can be coerced from str with the following case-insensitive mapping: False True 0 1 f t n y no yes false true off on ko ok Note bool coercion from str is just a global dict[str, bool] named apischema.data.coercion.STR_TO_BOOL and it can be customized according to your need (but keys have to be lower cased). There is also a global set[str] named apischema.data.coercion.STR_NONE_VALUES for None coercion. coerce parameter can also receive a coercion function which will then be used instead of default one. from typing import TypeVar , cast from pytest import raises from apischema import ValidationError , deserialize T = TypeVar ( \"T\" ) def coerce ( cls : type [ T ], data ) -> T : \"\"\"Only coerce int to bool\"\"\" if cls is bool and isinstance ( data , int ): return cast ( T , bool ( data )) else : return data with raises ( ValidationError ): deserialize ( bool , 0 ) with raises ( ValidationError ): assert deserialize ( bool , \"ok\" , coerce = coerce ) assert deserialize ( bool , 1 , coerce = coerce ) Note If coercer result is not an instance of class passed in argument, a ValidationError will be raised with an appropriate error message Warning Coercer first argument is a primitive json type str / bool / int / float / list / dict / type(None) ; it can be type(None) , so returning cls(data) will fail in this case. Additional properties \u00b6 apischema is strict too about number of fields received for an object . In JSON schema terms, apischema put \"additionalProperties\": false by default (this can be configured by class with properties field ). This behavior can be controlled by additional_properties parameter. When set to True , it prevents the reject of unexpected properties. from dataclasses import dataclass from pytest import raises from apischema import ValidationError , deserialize @dataclass class Foo : bar : str data = { \"bar\" : \"bar\" , \"other\" : 42 } with raises ( ValidationError ): deserialize ( Foo , data ) assert deserialize ( Foo , data , additional_properties = True ) == Foo ( \"bar\" ) Fall back on default \u00b6 Validation error can happen when deserializing an ill-formed field. However, if this field has a default value/factory, deserialization can fall back on this default; this is enabled by fall_back_on_default parameter. This behavior can also be configured for each field using metadata. from dataclasses import dataclass , field from pytest import raises from apischema import ValidationError , deserialize from apischema.metadata import fall_back_on_default @dataclass class Foo : bar : str = \"bar\" baz : str = field ( default = \"baz\" , metadata = fall_back_on_default ) with raises ( ValidationError ): deserialize ( Foo , { \"bar\" : 0 }) assert deserialize ( Foo , { \"bar\" : 0 }, fall_back_on_default = True ) == Foo () assert deserialize ( Foo , { \"baz\" : 0 }) == Foo () Strictness configuration \u00b6 apischema global configuration is managed through apischema.settings object. It has, among other, three global variables settings.deserializaton.additional_properties , settings.deserialization.coerce and settings.deserialization.fall_back_on_default whose values are used as default parameter values for the deserialize ; by default, additional_properties=False , coerce=False and fall_back_on_default=False . Global coercion function can be set with settings.coercer following this example: import json from apischema import ValidationError , settings prev_coercer = settings . coercer def coercer ( cls , data ): \"\"\"In case of coercion failures, try to deserialize json data\"\"\" try : return prev_coercer ( cls , data ) except ValidationError as err : if not isinstance ( data , str ): raise try : return json . loads ( data ) except json . JSONDecodeError : raise err settings . coercer = coercer Fields set \u00b6 Sometimes, it can be useful to know which field has been set by the deserialization, for example in the case of a PATCH requests, to know which field has been updated. Moreover, it is also used in serialization to limit the fields serialized (see next section ) Because apischema use vanilla dataclasses, this feature is not enabled by default and must be set explicitly on a per-class basis. apischema provides a simple API to get/set this metadata. from dataclasses import dataclass from typing import Optional from apischema import deserialize from apischema.fields import ( fields_set , is_set , set_fields , unset_fields , with_fields_set , ) # This decorator enable the feature @with_fields_set @dataclass class Foo : bar : int baz : Optional [ str ] = None # Retrieve fields set foo1 = Foo ( 0 , None ) assert fields_set ( foo1 ) == { \"bar\" , \"baz\" } foo2 = Foo ( 0 ) assert fields_set ( foo2 ) == { \"bar\" } # Test fields individually (with autocompletion and refactoring) assert is_set ( foo1 ) . baz assert not is_set ( foo2 ) . baz # Mark fields as set/unset set_fields ( foo2 , \"baz\" ) assert fields_set ( foo2 ) == { \"bar\" , \"baz\" } unset_fields ( foo2 , \"baz\" ) assert fields_set ( foo2 ) == { \"bar\" } set_fields ( foo2 , \"baz\" , overwrite = True ) assert fields_set ( foo2 ) == { \"baz\" } # Fields modification are taken in account foo2 . bar = 0 assert fields_set ( foo2 ) == { \"bar\" , \"baz\" } # Because deserialization use normal constructor, it works with the feature foo3 = deserialize ( Foo , { \"bar\" : 0 }) assert fields_set ( foo3 ) == { \"bar\" } Warning with_fields_set decorator MUST be put above dataclass one. This is because both of them modify __init__ method, but only the first is built to take the second in account. Warning dataclasses.replace works by setting all the fields of the replaced object. Because of this issue, apischema provides a little wrapper apischema.dataclasses.replace . Serialization \u00b6 apischema.serialize is used to serialize Python objects to JSON-like data. Contrary to apischema.deserialize , Python type can be omitted; in this case, the object will be serialized with an typing.Any type, i.e. the class of the serialized object will be used. from dataclasses import dataclass from typing import Any from apischema import serialize @dataclass class Foo : bar : str assert serialize ( Foo , Foo ( \"baz\" )) == { \"bar\" : \"baz\" } assert serialize ( tuple [ int , int ], ( 0 , 1 )) == [ 0 , 1 ] assert ( serialize ( Any , { \"key\" : ( \"value\" , 42 )}) == serialize ({ \"key\" : ( \"value\" , 42 )}) == { \"key\" : [ \"value\" , 42 ]} ) assert serialize ( Foo ( \"baz\" )) == { \"bar\" : \"baz\" } Note Omitting type with serialize can have unwanted side effects, as it makes loose any type annotations of the serialized object. In fact, generic specialization as well as PEP 593 annotations cannot be retrieved from an object instance; conversions can also be impacted That's why it's advisable to pass the type when it is available. Type checking \u00b6 Serialization can be configured using check_type (default to False ) and fall_back_on_any (default to False ) parameters. If check_type is True , serialized object type will be checked to match the serialized type. If it doesn't, fall_back_on_any allows bypassing serialized type to use typing.Any instead, i.e. to use the serialized object class. The default values of these parameters can be modified through apischema.settings.serialization.check_type and apischema.settings.serialization.fall_back_on_any . Note apischema relies on typing annotations, and assumes that the code is well statically type-checked. That's why it doesn't add the overhead of type checking by default (it's more than 10% performance impact). Serialized methods/properties \u00b6 apischema can execute methods/properties during serialization and add the computed values with the other fields values; just put apischema.serialized decorator on top of methods/properties you want to be serialized. The function name is used unless an alias is given in decorator argument. from dataclasses import dataclass from apischema import serialize , serialized from apischema.json_schema import serialization_schema @dataclass class Foo : @serialized @property def bar ( self ) -> int : return 0 # Serialized method can have default argument @serialized def baz ( self , some_arg_with_default : int = 1 ) -> int : return some_arg_with_default @serialized ( \"aliased\" ) @property def with_alias ( self ) -> int : return 2 # Serialized method can also be defined outside class, # but first parameter must be annotated @serialized def function ( foo : Foo ) -> int : return 3 assert serialize ( Foo , Foo ()) == { \"bar\" : 0 , \"baz\" : 1 , \"aliased\" : 2 , \"function\" : 3 } assert serialization_schema ( Foo ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"type\" : \"object\" , \"properties\" : { \"aliased\" : { \"type\" : \"integer\" }, \"bar\" : { \"type\" : \"integer\" }, \"baz\" : { \"type\" : \"integer\" }, \"function\" : { \"type\" : \"integer\" }, }, \"required\" : [ \"aliased\" , \"bar\" , \"baz\" , \"function\" ], \"additionalProperties\" : False , } Note Serialized methods must not have parameters without default, as apischema need to execute them without arguments Note Overriding of a serialized method in a subclass will also override the serialization of the subclass. Error handling \u00b6 Errors occurring in serialized methods can be caught in a dedicated error handler registered with error_handler parameter. This function takes in parameters the exception, the object and the alias of the serialized method; it can return a new value or raise the current or another exception \u2014 it can for example be used to log errors without throwing the complete serialization. The resulting serialization type will be a Union of the normal type and the error handling type ; if the error handler always raises, use typing.NoReturn annotation. error_handler=None correspond to a default handler which only return None \u2014 exception is thus discarded and serialization type becomes Optional . The error handler is only executed by apischema serialization process, it's not added to the function, so this one can be executed normally and raise an exception in the rest of your code. from dataclasses import dataclass from logging import getLogger from typing import Any from apischema import serialize , serialized from apischema.json_schema import serialization_schema logger = getLogger ( __name__ ) def log_error ( error : Exception , obj : Any , alias : str ) -> None : logger . error ( \"Serialization error in %s . %s \" , type ( obj ) . __name__ , alias , exc_info = error ) return None @dataclass class Foo : @serialized ( error_handler = log_error ) def bar ( self ) -> int : raise RuntimeError ( \"Some error\" ) assert serialize ( Foo , Foo ()) == { \"bar\" : None } # Logs \"Serialization error in Foo.bar\" assert serialization_schema ( Foo ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"type\" : \"object\" , \"properties\" : { \"bar\" : { \"type\" : [ \"integer\" , \"null\" ]}}, \"required\" : [ \"bar\" ], \"additionalProperties\" : False , } Non-required serialized methods \u00b6 Serialized methods (or their error handler) can return apischema.Undefined , in which case the property will not be included into the serialization; accordingly, the property loose the required qualification in the JSON schema. from dataclasses import dataclass from typing import Union from apischema import Undefined , UndefinedType , serialize , serialized from apischema.json_schema import serialization_schema @dataclass class Foo : @serialized def bar ( self ) -> Union [ int , UndefinedType ]: return Undefined assert serialize ( Foo , Foo ()) == {} assert serialization_schema ( Foo ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"type\" : \"object\" , \"properties\" : { \"bar\" : { \"type\" : \"integer\" }}, \"additionalProperties\" : False , } Generic serialized methods \u00b6 Serialized methods of generic classes get the right type when their owning class is specialized. from dataclasses import dataclass from typing import Generic , TypeVar from apischema import serialized from apischema.json_schema import serialization_schema T = TypeVar ( \"T\" ) U = TypeVar ( \"U\" ) @dataclass class Foo ( Generic [ T ]): # serialized decorator for methods of generic class is not supported in Python 3.6 def bar ( self ) -> T : ... serialized ( Foo . bar ) @serialized def baz ( foo : Foo [ U ]) -> U : ... @dataclass class FooInt ( Foo [ int ]): ... assert ( serialization_schema ( Foo [ int ]) == serialization_schema ( FooInt ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"type\" : \"object\" , \"properties\" : { \"bar\" : { \"type\" : \"integer\" }, \"baz\" : { \"type\" : \"integer\" }}, \"required\" : [ \"bar\" , \"baz\" ], \"additionalProperties\" : False , } ) Warning serialized cannot decorate methods of Generic classes in Python 3.6, it has to be used outside of class. Exclude unset fields \u00b6 When a class has a lot of optional fields, it can be convenient to not include all of them, to avoid a bunch of useless fields in your serialized data. Using the previous feature of fields set tracking , serialize can exclude unset fields using its exclude_unset parameter or settings.serialization.exclude_unset (default is True ). from dataclasses import dataclass from typing import Optional from apischema import serialize from apischema.fields import with_fields_set # Decorator needed to benefit from the feature @with_fields_set @dataclass class Foo : bar : int baz : Optional [ str ] = None assert serialize ( Foo , Foo ( 0 )) == { \"bar\" : 0 } assert serialize ( Foo , Foo ( 0 ), exclude_unset = False ) == { \"bar\" : 0 , \"baz\" : None } Note As written in comment in the example, with_fields_set is necessary to benefit from the feature. If the dataclass don't use it, the feature will have no effect. Sometimes, some fields must be serialized, even with their default value; this behavior can be enforced using field metadata. With it, field will be marked as set even if its default value is used at initialization. from dataclasses import dataclass , field from typing import Optional from apischema import serialize from apischema.fields import with_fields_set from apischema.metadata import default_as_set # Decorator needed to benefit from the feature @with_fields_set @dataclass class Foo : bar : Optional [ int ] = field ( default = None , metadata = default_as_set ) assert serialize ( Foo , Foo ()) == { \"bar\" : None } assert serialize ( Foo , Foo ( 0 )) == { \"bar\" : 0 } Note This metadata has effect only in combination with with_fields_set decorator. Performances \u00b6 apischema is among the fastest (if not the fastest) Python library in its domain. These performances are achieved by pre-computing (de)serialization methods depending on the (de)serialized type (and other parameters): all the type annotations processing is done in this pre-computation. The methods are then cached using functools.lru_cache , so deserialize and serialize don't recompute them every time. However, if lru_cache is fast, using the methods directly is faster, so apischema provides apischema.deserialization_method and apischema.serialization_method . These functions share the same parameters than deserialize / serialize , except the data/object parameter to (de)serialize. Using the computed methods directly can increase performances by 10%. from dataclasses import dataclass from apischema import deserialization_method , serialization_method @dataclass class Foo : bar : int deserialize_foo = deserialization_method ( Foo ) serialize_foo = serialization_method ( Foo ) assert deserialize_foo ({ \"bar\" : 0 }) == Foo ( 0 ) assert serialize_foo ( Foo ( 0 )) == { \"bar\" : 0 } Also, apischema cache size can be modified using apischema.cache.set_size , and it can be reset using apischema.cache.reset (it happens automatically when apischema.settings is modified), but you should not need it. FAQ \u00b6 Why coercion is not default behavior? \u00b6 Because ill-formed data can be symptomatic of deeper issues, it has been decided that highlighting them would be better than hiding them. By the way, this is easily globally configurable. Why with_fields_set feature is not enable by default? \u00b6 It's true that this feature has the little cost of adding a decorator everywhere. However, keeping dataclass decorator allows IDEs/linters/type checkers/etc. to handle the class as such, so there is no need to develop a plugin for them. Standard compliance can be worth the additional decorator. (And little overhead can be avoided when not useful)","title":"(De)serialization"},{"location":"de_serialization/#deserialization","text":"apischema aims to help API with deserialization/serialization of data, mostly JSON. Let start again with the overview example from collections.abc import Collection from dataclasses import dataclass , field from typing import Optional from uuid import UUID , uuid4 from graphql import print_schema from pytest import raises from apischema import ValidationError , deserialize , serialize from apischema.graphql import graphql_schema from apischema.json_schema import deserialization_schema # Define a schema with standard dataclasses @dataclass class Resource : id : UUID name : str tags : set [ str ] = field ( default_factory = set ) # Get some data uuid = uuid4 () data = { \"id\" : str ( uuid ), \"name\" : \"wyfo\" , \"tags\" : [ \"some_tag\" ]} # Deserialize data resource = deserialize ( Resource , data ) assert resource == Resource ( uuid , \"wyfo\" , { \"some_tag\" }) # Serialize objects assert serialize ( Resource , resource ) == data # Validate during deserialization with raises ( ValidationError ) as err : # pytest checks exception is raised deserialize ( Resource , { \"id\" : \"42\" , \"name\" : \"wyfo\" }) assert serialize ( err . value ) == [ # ValidationError is serializable { \"loc\" : [ \"id\" ], \"err\" : [ \"badly formed hexadecimal UUID string\" ]} ] # Generate JSON Schema assert deserialization_schema ( Resource ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"type\" : \"object\" , \"properties\" : { \"id\" : { \"type\" : \"string\" , \"format\" : \"uuid\" }, \"name\" : { \"type\" : \"string\" }, \"tags\" : { \"type\" : \"array\" , \"items\" : { \"type\" : \"string\" }, \"uniqueItems\" : True , \"default\" : [], }, }, \"required\" : [ \"id\" , \"name\" ], \"additionalProperties\" : False , } # Define GraphQL operations def resources ( tags : Optional [ Collection [ str ]] = None ) -> Optional [ Collection [ Resource ]]: ... # Generate GraphQL schema schema = graphql_schema ( query = [ resources ], id_types = { UUID }) schema_str = \"\"\" \\ type Query { resources(tags: [String!]): [Resource!] } type Resource { id: ID! name: String! tags: [String!]! } \"\"\" assert print_schema ( schema ) == schema_str","title":"(De)serialization"},{"location":"de_serialization/#deserialization_1","text":"apischema.deserialize deserializes Python types from JSON-like data: dict / list / str / int / float / bool / None \u2014 in short, what you get when you execute json.loads . Types can be dataclasses as well as list[int] , NewType s, or whatever you want (see conversions to extend deserialization support to every type you want). from collections.abc import Collection, Mapping from dataclasses import dataclass from typing import NewType from apischema import deserialize @dataclass class Foo: bar: str MyInt = NewType(\"MyInt\", int) assert deserialize(Foo, {\"bar\": \"bar\"}) == Foo(\"bar\") assert deserialize(MyInt, 0) == MyInt(0) == 0 assert deserialize(Mapping[str, Collection[Foo]], {\"key\": [{\"bar\": \"42\"}]}) == { \"key\": (Foo(\"42\"),) } Deserialization performs a validation of data, based on typing annotations and other information (see schema and validation ).","title":"Deserialization"},{"location":"de_serialization/#strictness","text":"","title":"Strictness"},{"location":"de_serialization/#coercion","text":"apischema is strict by default. You ask for an integer, you have to receive an integer. However, in some cases, data has to be be coerced, for example when parsing aconfiguration file. That can be done using coerce parameter; when set to True , all primitive types will be coerce to the expected type of the data model like the following: from pytest import raises from apischema import ValidationError , deserialize with raises ( ValidationError ): deserialize ( bool , \"ok\" ) assert deserialize ( bool , \"ok\" , coerce = True ) bool can be coerced from str with the following case-insensitive mapping: False True 0 1 f t n y no yes false true off on ko ok Note bool coercion from str is just a global dict[str, bool] named apischema.data.coercion.STR_TO_BOOL and it can be customized according to your need (but keys have to be lower cased). There is also a global set[str] named apischema.data.coercion.STR_NONE_VALUES for None coercion. coerce parameter can also receive a coercion function which will then be used instead of default one. from typing import TypeVar , cast from pytest import raises from apischema import ValidationError , deserialize T = TypeVar ( \"T\" ) def coerce ( cls : type [ T ], data ) -> T : \"\"\"Only coerce int to bool\"\"\" if cls is bool and isinstance ( data , int ): return cast ( T , bool ( data )) else : return data with raises ( ValidationError ): deserialize ( bool , 0 ) with raises ( ValidationError ): assert deserialize ( bool , \"ok\" , coerce = coerce ) assert deserialize ( bool , 1 , coerce = coerce ) Note If coercer result is not an instance of class passed in argument, a ValidationError will be raised with an appropriate error message Warning Coercer first argument is a primitive json type str / bool / int / float / list / dict / type(None) ; it can be type(None) , so returning cls(data) will fail in this case.","title":"Coercion"},{"location":"de_serialization/#additional-properties","text":"apischema is strict too about number of fields received for an object . In JSON schema terms, apischema put \"additionalProperties\": false by default (this can be configured by class with properties field ). This behavior can be controlled by additional_properties parameter. When set to True , it prevents the reject of unexpected properties. from dataclasses import dataclass from pytest import raises from apischema import ValidationError , deserialize @dataclass class Foo : bar : str data = { \"bar\" : \"bar\" , \"other\" : 42 } with raises ( ValidationError ): deserialize ( Foo , data ) assert deserialize ( Foo , data , additional_properties = True ) == Foo ( \"bar\" )","title":"Additional properties"},{"location":"de_serialization/#fall-back-on-default","text":"Validation error can happen when deserializing an ill-formed field. However, if this field has a default value/factory, deserialization can fall back on this default; this is enabled by fall_back_on_default parameter. This behavior can also be configured for each field using metadata. from dataclasses import dataclass , field from pytest import raises from apischema import ValidationError , deserialize from apischema.metadata import fall_back_on_default @dataclass class Foo : bar : str = \"bar\" baz : str = field ( default = \"baz\" , metadata = fall_back_on_default ) with raises ( ValidationError ): deserialize ( Foo , { \"bar\" : 0 }) assert deserialize ( Foo , { \"bar\" : 0 }, fall_back_on_default = True ) == Foo () assert deserialize ( Foo , { \"baz\" : 0 }) == Foo ()","title":"Fall back on default"},{"location":"de_serialization/#strictness-configuration","text":"apischema global configuration is managed through apischema.settings object. It has, among other, three global variables settings.deserializaton.additional_properties , settings.deserialization.coerce and settings.deserialization.fall_back_on_default whose values are used as default parameter values for the deserialize ; by default, additional_properties=False , coerce=False and fall_back_on_default=False . Global coercion function can be set with settings.coercer following this example: import json from apischema import ValidationError , settings prev_coercer = settings . coercer def coercer ( cls , data ): \"\"\"In case of coercion failures, try to deserialize json data\"\"\" try : return prev_coercer ( cls , data ) except ValidationError as err : if not isinstance ( data , str ): raise try : return json . loads ( data ) except json . JSONDecodeError : raise err settings . coercer = coercer","title":"Strictness configuration"},{"location":"de_serialization/#fields-set","text":"Sometimes, it can be useful to know which field has been set by the deserialization, for example in the case of a PATCH requests, to know which field has been updated. Moreover, it is also used in serialization to limit the fields serialized (see next section ) Because apischema use vanilla dataclasses, this feature is not enabled by default and must be set explicitly on a per-class basis. apischema provides a simple API to get/set this metadata. from dataclasses import dataclass from typing import Optional from apischema import deserialize from apischema.fields import ( fields_set , is_set , set_fields , unset_fields , with_fields_set , ) # This decorator enable the feature @with_fields_set @dataclass class Foo : bar : int baz : Optional [ str ] = None # Retrieve fields set foo1 = Foo ( 0 , None ) assert fields_set ( foo1 ) == { \"bar\" , \"baz\" } foo2 = Foo ( 0 ) assert fields_set ( foo2 ) == { \"bar\" } # Test fields individually (with autocompletion and refactoring) assert is_set ( foo1 ) . baz assert not is_set ( foo2 ) . baz # Mark fields as set/unset set_fields ( foo2 , \"baz\" ) assert fields_set ( foo2 ) == { \"bar\" , \"baz\" } unset_fields ( foo2 , \"baz\" ) assert fields_set ( foo2 ) == { \"bar\" } set_fields ( foo2 , \"baz\" , overwrite = True ) assert fields_set ( foo2 ) == { \"baz\" } # Fields modification are taken in account foo2 . bar = 0 assert fields_set ( foo2 ) == { \"bar\" , \"baz\" } # Because deserialization use normal constructor, it works with the feature foo3 = deserialize ( Foo , { \"bar\" : 0 }) assert fields_set ( foo3 ) == { \"bar\" } Warning with_fields_set decorator MUST be put above dataclass one. This is because both of them modify __init__ method, but only the first is built to take the second in account. Warning dataclasses.replace works by setting all the fields of the replaced object. Because of this issue, apischema provides a little wrapper apischema.dataclasses.replace .","title":"Fields set"},{"location":"de_serialization/#serialization","text":"apischema.serialize is used to serialize Python objects to JSON-like data. Contrary to apischema.deserialize , Python type can be omitted; in this case, the object will be serialized with an typing.Any type, i.e. the class of the serialized object will be used. from dataclasses import dataclass from typing import Any from apischema import serialize @dataclass class Foo : bar : str assert serialize ( Foo , Foo ( \"baz\" )) == { \"bar\" : \"baz\" } assert serialize ( tuple [ int , int ], ( 0 , 1 )) == [ 0 , 1 ] assert ( serialize ( Any , { \"key\" : ( \"value\" , 42 )}) == serialize ({ \"key\" : ( \"value\" , 42 )}) == { \"key\" : [ \"value\" , 42 ]} ) assert serialize ( Foo ( \"baz\" )) == { \"bar\" : \"baz\" } Note Omitting type with serialize can have unwanted side effects, as it makes loose any type annotations of the serialized object. In fact, generic specialization as well as PEP 593 annotations cannot be retrieved from an object instance; conversions can also be impacted That's why it's advisable to pass the type when it is available.","title":"Serialization"},{"location":"de_serialization/#type-checking","text":"Serialization can be configured using check_type (default to False ) and fall_back_on_any (default to False ) parameters. If check_type is True , serialized object type will be checked to match the serialized type. If it doesn't, fall_back_on_any allows bypassing serialized type to use typing.Any instead, i.e. to use the serialized object class. The default values of these parameters can be modified through apischema.settings.serialization.check_type and apischema.settings.serialization.fall_back_on_any . Note apischema relies on typing annotations, and assumes that the code is well statically type-checked. That's why it doesn't add the overhead of type checking by default (it's more than 10% performance impact).","title":"Type checking"},{"location":"de_serialization/#serialized-methodsproperties","text":"apischema can execute methods/properties during serialization and add the computed values with the other fields values; just put apischema.serialized decorator on top of methods/properties you want to be serialized. The function name is used unless an alias is given in decorator argument. from dataclasses import dataclass from apischema import serialize , serialized from apischema.json_schema import serialization_schema @dataclass class Foo : @serialized @property def bar ( self ) -> int : return 0 # Serialized method can have default argument @serialized def baz ( self , some_arg_with_default : int = 1 ) -> int : return some_arg_with_default @serialized ( \"aliased\" ) @property def with_alias ( self ) -> int : return 2 # Serialized method can also be defined outside class, # but first parameter must be annotated @serialized def function ( foo : Foo ) -> int : return 3 assert serialize ( Foo , Foo ()) == { \"bar\" : 0 , \"baz\" : 1 , \"aliased\" : 2 , \"function\" : 3 } assert serialization_schema ( Foo ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"type\" : \"object\" , \"properties\" : { \"aliased\" : { \"type\" : \"integer\" }, \"bar\" : { \"type\" : \"integer\" }, \"baz\" : { \"type\" : \"integer\" }, \"function\" : { \"type\" : \"integer\" }, }, \"required\" : [ \"aliased\" , \"bar\" , \"baz\" , \"function\" ], \"additionalProperties\" : False , } Note Serialized methods must not have parameters without default, as apischema need to execute them without arguments Note Overriding of a serialized method in a subclass will also override the serialization of the subclass.","title":"Serialized methods/properties"},{"location":"de_serialization/#error-handling","text":"Errors occurring in serialized methods can be caught in a dedicated error handler registered with error_handler parameter. This function takes in parameters the exception, the object and the alias of the serialized method; it can return a new value or raise the current or another exception \u2014 it can for example be used to log errors without throwing the complete serialization. The resulting serialization type will be a Union of the normal type and the error handling type ; if the error handler always raises, use typing.NoReturn annotation. error_handler=None correspond to a default handler which only return None \u2014 exception is thus discarded and serialization type becomes Optional . The error handler is only executed by apischema serialization process, it's not added to the function, so this one can be executed normally and raise an exception in the rest of your code. from dataclasses import dataclass from logging import getLogger from typing import Any from apischema import serialize , serialized from apischema.json_schema import serialization_schema logger = getLogger ( __name__ ) def log_error ( error : Exception , obj : Any , alias : str ) -> None : logger . error ( \"Serialization error in %s . %s \" , type ( obj ) . __name__ , alias , exc_info = error ) return None @dataclass class Foo : @serialized ( error_handler = log_error ) def bar ( self ) -> int : raise RuntimeError ( \"Some error\" ) assert serialize ( Foo , Foo ()) == { \"bar\" : None } # Logs \"Serialization error in Foo.bar\" assert serialization_schema ( Foo ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"type\" : \"object\" , \"properties\" : { \"bar\" : { \"type\" : [ \"integer\" , \"null\" ]}}, \"required\" : [ \"bar\" ], \"additionalProperties\" : False , }","title":"Error handling"},{"location":"de_serialization/#non-required-serialized-methods","text":"Serialized methods (or their error handler) can return apischema.Undefined , in which case the property will not be included into the serialization; accordingly, the property loose the required qualification in the JSON schema. from dataclasses import dataclass from typing import Union from apischema import Undefined , UndefinedType , serialize , serialized from apischema.json_schema import serialization_schema @dataclass class Foo : @serialized def bar ( self ) -> Union [ int , UndefinedType ]: return Undefined assert serialize ( Foo , Foo ()) == {} assert serialization_schema ( Foo ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"type\" : \"object\" , \"properties\" : { \"bar\" : { \"type\" : \"integer\" }}, \"additionalProperties\" : False , }","title":"Non-required serialized methods"},{"location":"de_serialization/#generic-serialized-methods","text":"Serialized methods of generic classes get the right type when their owning class is specialized. from dataclasses import dataclass from typing import Generic , TypeVar from apischema import serialized from apischema.json_schema import serialization_schema T = TypeVar ( \"T\" ) U = TypeVar ( \"U\" ) @dataclass class Foo ( Generic [ T ]): # serialized decorator for methods of generic class is not supported in Python 3.6 def bar ( self ) -> T : ... serialized ( Foo . bar ) @serialized def baz ( foo : Foo [ U ]) -> U : ... @dataclass class FooInt ( Foo [ int ]): ... assert ( serialization_schema ( Foo [ int ]) == serialization_schema ( FooInt ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"type\" : \"object\" , \"properties\" : { \"bar\" : { \"type\" : \"integer\" }, \"baz\" : { \"type\" : \"integer\" }}, \"required\" : [ \"bar\" , \"baz\" ], \"additionalProperties\" : False , } ) Warning serialized cannot decorate methods of Generic classes in Python 3.6, it has to be used outside of class.","title":"Generic serialized methods"},{"location":"de_serialization/#exclude-unset-fields","text":"When a class has a lot of optional fields, it can be convenient to not include all of them, to avoid a bunch of useless fields in your serialized data. Using the previous feature of fields set tracking , serialize can exclude unset fields using its exclude_unset parameter or settings.serialization.exclude_unset (default is True ). from dataclasses import dataclass from typing import Optional from apischema import serialize from apischema.fields import with_fields_set # Decorator needed to benefit from the feature @with_fields_set @dataclass class Foo : bar : int baz : Optional [ str ] = None assert serialize ( Foo , Foo ( 0 )) == { \"bar\" : 0 } assert serialize ( Foo , Foo ( 0 ), exclude_unset = False ) == { \"bar\" : 0 , \"baz\" : None } Note As written in comment in the example, with_fields_set is necessary to benefit from the feature. If the dataclass don't use it, the feature will have no effect. Sometimes, some fields must be serialized, even with their default value; this behavior can be enforced using field metadata. With it, field will be marked as set even if its default value is used at initialization. from dataclasses import dataclass , field from typing import Optional from apischema import serialize from apischema.fields import with_fields_set from apischema.metadata import default_as_set # Decorator needed to benefit from the feature @with_fields_set @dataclass class Foo : bar : Optional [ int ] = field ( default = None , metadata = default_as_set ) assert serialize ( Foo , Foo ()) == { \"bar\" : None } assert serialize ( Foo , Foo ( 0 )) == { \"bar\" : 0 } Note This metadata has effect only in combination with with_fields_set decorator.","title":"Exclude unset fields"},{"location":"de_serialization/#performances","text":"apischema is among the fastest (if not the fastest) Python library in its domain. These performances are achieved by pre-computing (de)serialization methods depending on the (de)serialized type (and other parameters): all the type annotations processing is done in this pre-computation. The methods are then cached using functools.lru_cache , so deserialize and serialize don't recompute them every time. However, if lru_cache is fast, using the methods directly is faster, so apischema provides apischema.deserialization_method and apischema.serialization_method . These functions share the same parameters than deserialize / serialize , except the data/object parameter to (de)serialize. Using the computed methods directly can increase performances by 10%. from dataclasses import dataclass from apischema import deserialization_method , serialization_method @dataclass class Foo : bar : int deserialize_foo = deserialization_method ( Foo ) serialize_foo = serialization_method ( Foo ) assert deserialize_foo ({ \"bar\" : 0 }) == Foo ( 0 ) assert serialize_foo ( Foo ( 0 )) == { \"bar\" : 0 } Also, apischema cache size can be modified using apischema.cache.set_size , and it can be reset using apischema.cache.reset (it happens automatically when apischema.settings is modified), but you should not need it.","title":"Performances"},{"location":"de_serialization/#faq","text":"","title":"FAQ"},{"location":"de_serialization/#why-coercion-is-not-default-behavior","text":"Because ill-formed data can be symptomatic of deeper issues, it has been decided that highlighting them would be better than hiding them. By the way, this is easily globally configurable.","title":"Why coercion is not default behavior?"},{"location":"de_serialization/#why-with_fields_set-feature-is-not-enable-by-default","text":"It's true that this feature has the little cost of adding a decorator everywhere. However, keeping dataclass decorator allows IDEs/linters/type checkers/etc. to handle the class as such, so there is no need to develop a plugin for them. Standard compliance can be worth the additional decorator. (And little overhead can be avoided when not useful)","title":"Why with_fields_set feature is not enable by default?"},{"location":"difference_with_pydantic/","text":"Difference with pydantic \u00b6 As the question is often asked, it is answered in a dedicated section. Here are some the key differences between apischema and pydantic : apischema is faster \u00b6 pydantic uses Cython to improve its performance; apischema doesn't need it and is still 1.5x faster according to pydantic benchmark \u2014 more than 2x when pydantic is not compiled with Cython. Better performance, but not at the cost of fewer functionalities; that's rather the opposite: dynamic aliasing , conversions , flattened fields , etc. apischema can generate GraphQL schema from your resolvers \u00b6 Not just a simple printable schema but a complete graphql.GraphQLSchema (using graphql-core library) which can be used to execute your queries/mutations/subscriptions through your resolvers, powered by apischema (de)serialization and conversions features. Types and resolvers can be used both in traditional JSON-oriented API and GraphQL API apischema uses standard dataclasses and types \u00b6 pydantic uses its own BaseModel class, or its own pseudo- dataclass , so you are forced to tie all your code to the library, and you cannot easily reuse code written in a more standard way or in external libraries. By the way, Pydantic use expressions in typing annotations ( conint , etc.), while it's not recommended and treated as an error by tools like Mypy apischema doesn't require external plugins for editors, linters, etc. \u00b6 pydantic requires a plugin to allow Mypy to type checked BaseModel and others pydantic singularities (and to not raise errors on it); plugin are also needed for editors. apischema doesn't mix up (de)serialization with your code \u00b6 While pydantic mix up model constructor with deserializer, apischema use dedicated functions for its features, meaning your dataclasses are instantiated normally with type checking. In your code, you manipulate objects; (de)serialization is for input/output. apischema also doesn't mix up validation of external data with your statically checked code; there is no runtime validation in constructors. apischema truly works out-of-the-box with forward type references (especially for recursive model) \u00b6 pydantic requires calling update_forward_refs method on recursive types, while apischema \"just works\". apischema supports Generic in Python 3.6 and without requiring additional stuff \u00b6 pydantic BaseModel cannot be used with generic model, you have to use GenericModel , and it's not supported in Python 3.6. With apischema , you just write your generic classes normally. apischema conversions feature allows to support any type defined in your code, but also in external libraries \u00b6 pydantic doesn't make it easy to support external types, like bson.ObjectId ; see this issue on the subject. You could dynamically add a __get_validators__ method to foreign classes, but that doesn't work with builtin types like collection.deque and other types written in C. Serialization customization is harder, with definition of encoding function by model; it cannot be done at the same place as deserialization. There is also no correlation done between (de)serialization customization and model JSON schema; you could have to overwrite the generated schema if you don't want to get an inconsistency. apischema only requires a few lines of code to support any type you want, from bson.ObjectId to SQLAlchemy models by way of builtin and generic like collection.deque , and even pydantic . Conversions are also integrated in JSON schema this one is generated according to the source/target of the conversion Here is a comparison of a custom type support: import re from typing import NamedTuple , NewType import pydantic.validators import apischema # Serialization can only be customized into the enclosing models class RGB ( NamedTuple ): red : int green : int blue : int # If you don't put this method, RGB schema will be: # {'title': 'Rgb', 'type': 'array', 'items': {}} @classmethod def __modify_schema__ ( cls , field_schema ) -> None : field_schema . update ({ \"type\" : \"string\" , \"pattern\" : r \"#[0-9A-Fa-f] {6} \" }) field_schema . pop ( \"items\" , ... ) @classmethod def __get_validators__ ( cls ): yield pydantic . validators . str_validator yield cls . validate @classmethod def validate ( cls , value ) -> \"RGB\" : if ( not isinstance ( value , str ) or re . fullmatch ( r \"#[0-9A-Fa-f] {6} \" , value ) is None ): raise ValueError ( \"Invalid RGB\" ) return RGB ( red = int ( value [ 1 : 3 ], 16 ), green = int ( value [ 3 : 5 ], 16 ), blue = int ( value [ 5 : 7 ], 16 ) ) # Simpler with apischema class RGB ( NamedTuple ): red : int green : int blue : int # NewType can be used to add schema to conversion source/target # but Annotated[str, apischema.schema(pattern=r\"#[0-9A-Fa-f]{6}\")] would have worked too HexaRGB = NewType ( \"HexaRGB\" , str ) # pattern is used in JSON schema and in deserialization validation apischema . schema ( pattern = r \"#[0-9A-Fa-f] {6} \" )( HexaRGB ) @apischema . deserializer # could be declared as a staticmethod of RGB class def from_hexa ( hexa : HexaRGB ) -> RGB : return RGB ( int ( hexa [ 1 : 3 ], 16 ), int ( hexa [ 3 : 5 ], 16 ), int ( hexa [ 5 : 7 ], 16 )) @apischema . serializer # could be declared as a method/property of RGB class def to_hexa ( rgb : RGB ) -> HexaRGB : return HexaRGB ( f \"# { rgb . red : 02x }{ rgb . green : 02x }{ rgb . blue : 02x } \" ) assert ( # schema is inherited from deserialized type apischema . json_schema . deserialization_schema ( RGB ) == apischema . json_schema . deserialization_schema ( HexaRGB ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"type\" : \"string\" , \"pattern\" : \"#[0-9A-Fa-f] {6} \" , } ) apischema can also customize serialization with computed fields \u00b6 Serialized methods/properties are regular methods/properties which are included in serialization effortlessly. apischema allows you to use composition over inheritance \u00b6 Flattened fields is a distinctive apischema feature that is very handy to build complex model from smaller fragments; you don't have to merge yourself the fields of your fragments in a complex class with a lot of fields, apischema deal with it for you, and your code is kept simple. apischema has a functional approach, pydantic has an object one \u00b6 pydantic features are based on BaseModel methods. You have to have a BaseModel instance to do anything, even if you manipulate only an integer. Complex pydantic stuff like __root__ model or deserialization customization come from this approach. apischema is functional, it doesn't use method but simple functions, which works with all types. You can also register conversions for any types similarly you would implement a type class in a functional language. And your class namespace don't mix up with a mandatory base class' one. apischema can use both camelCase and snake_case with the same types \u00b6 While pydantic field aliases are fixed at model creation, apischema let you choose which aliasing you want at (de)serialization time. It can be convenient if you need to juggle with cases for the same models between frontends and other backend services for example. apischema doesn't coerce by default \u00b6 Your API respects its schema. It can also coerce, for example to parse configuration file, and coercion can be adjusted (for example coercing list from comma-separated string). apischema has a better integration of JSON schema/ OpenAPI \u00b6 With pydantic , if you want to have a nullable field in the generated schema, you have to put nullable into schema extra keywords. apischema is bound to the last JSON schema version but offers conversion to other version like OpenAPI 3.0 and nullable is added for Optional types. apischema also support more advanced features like dependentRequired or unevaluatedProperties . Reference handling is also more flexible apischema can add validators and JSON schema to NewType \u00b6 So it will be used in deserialization validation. You can use NewType everywhere, to gain a better type checking, self-documented code. apischema validators are regular methods with automatic dependencies management \u00b6 Using regular methods allows benefiting of type checking of fields, where pydantic validators use dynamic stuffs (name of the fields as strings) and are not type-checked or have to get redundant type annotations. apischema validators also have automatic dependency management. And apischema directly supports JSON schema property dependencies . Comparison is simple with an example (validator is taken from pydantic documentation : from dataclasses import dataclass import apischema import pydantic class UserModel ( pydantic . BaseModel ): username : str password1 : str password2 : str @pydantic . root_validator def check_passwords_match ( cls , values ): # This is a classmethod (it needs a plugin to not raise a warning in your IDE) # What is the type of of values? of values['password1']? # If you rename password1 field, validator will hardly be updated # You also have to test yourself that values are provided pw1 , pw2 = values . get ( \"password1\" ), values . get ( \"password2\" ) if pw1 is not None and pw2 is not None and pw1 != pw2 : raise ValueError ( \"passwords do not match\" ) return values @dataclass class LoginForm : username : str password1 : str password2 : str @apischema . validator def check_password_match ( self ): # Typed checked, simpler, and not executed if password1 or password2 # are missing/invalid if self . password1 != self . password2 : raise ValueError ( \"passwords do not match\" ) apischema supports pydantic \u00b6 It's not a feature, is just the result of 30 lines of code .","title":"Difference with pydantic"},{"location":"difference_with_pydantic/#difference-with-pydantic","text":"As the question is often asked, it is answered in a dedicated section. Here are some the key differences between apischema and pydantic :","title":"Difference with pydantic"},{"location":"difference_with_pydantic/#apischema-is-faster","text":"pydantic uses Cython to improve its performance; apischema doesn't need it and is still 1.5x faster according to pydantic benchmark \u2014 more than 2x when pydantic is not compiled with Cython. Better performance, but not at the cost of fewer functionalities; that's rather the opposite: dynamic aliasing , conversions , flattened fields , etc.","title":"apischema is faster"},{"location":"difference_with_pydantic/#apischema-can-generate-graphql-schema-from-your-resolvers","text":"Not just a simple printable schema but a complete graphql.GraphQLSchema (using graphql-core library) which can be used to execute your queries/mutations/subscriptions through your resolvers, powered by apischema (de)serialization and conversions features. Types and resolvers can be used both in traditional JSON-oriented API and GraphQL API","title":"apischema can generate GraphQL schema from your resolvers"},{"location":"difference_with_pydantic/#apischema-uses-standard-dataclasses-and-types","text":"pydantic uses its own BaseModel class, or its own pseudo- dataclass , so you are forced to tie all your code to the library, and you cannot easily reuse code written in a more standard way or in external libraries. By the way, Pydantic use expressions in typing annotations ( conint , etc.), while it's not recommended and treated as an error by tools like Mypy","title":"apischema uses standard dataclasses and types"},{"location":"difference_with_pydantic/#apischema-doesnt-require-external-plugins-for-editors-linters-etc","text":"pydantic requires a plugin to allow Mypy to type checked BaseModel and others pydantic singularities (and to not raise errors on it); plugin are also needed for editors.","title":"apischema doesn't require external plugins for editors, linters, etc."},{"location":"difference_with_pydantic/#apischema-doesnt-mix-up-deserialization-with-your-code","text":"While pydantic mix up model constructor with deserializer, apischema use dedicated functions for its features, meaning your dataclasses are instantiated normally with type checking. In your code, you manipulate objects; (de)serialization is for input/output. apischema also doesn't mix up validation of external data with your statically checked code; there is no runtime validation in constructors.","title":"apischema doesn't mix up (de)serialization with your code"},{"location":"difference_with_pydantic/#apischema-truly-works-out-of-the-box-with-forward-type-references-especially-for-recursive-model","text":"pydantic requires calling update_forward_refs method on recursive types, while apischema \"just works\".","title":"apischema truly works out-of-the-box with forward type references (especially for recursive model)"},{"location":"difference_with_pydantic/#apischema-supports-generic-in-python-36-and-without-requiring-additional-stuff","text":"pydantic BaseModel cannot be used with generic model, you have to use GenericModel , and it's not supported in Python 3.6. With apischema , you just write your generic classes normally.","title":"apischema supports Generic in Python 3.6 and without requiring additional stuff"},{"location":"difference_with_pydantic/#apischema-conversions-feature-allows-to-support-any-type-defined-in-your-code-but-also-in-external-libraries","text":"pydantic doesn't make it easy to support external types, like bson.ObjectId ; see this issue on the subject. You could dynamically add a __get_validators__ method to foreign classes, but that doesn't work with builtin types like collection.deque and other types written in C. Serialization customization is harder, with definition of encoding function by model; it cannot be done at the same place as deserialization. There is also no correlation done between (de)serialization customization and model JSON schema; you could have to overwrite the generated schema if you don't want to get an inconsistency. apischema only requires a few lines of code to support any type you want, from bson.ObjectId to SQLAlchemy models by way of builtin and generic like collection.deque , and even pydantic . Conversions are also integrated in JSON schema this one is generated according to the source/target of the conversion Here is a comparison of a custom type support: import re from typing import NamedTuple , NewType import pydantic.validators import apischema # Serialization can only be customized into the enclosing models class RGB ( NamedTuple ): red : int green : int blue : int # If you don't put this method, RGB schema will be: # {'title': 'Rgb', 'type': 'array', 'items': {}} @classmethod def __modify_schema__ ( cls , field_schema ) -> None : field_schema . update ({ \"type\" : \"string\" , \"pattern\" : r \"#[0-9A-Fa-f] {6} \" }) field_schema . pop ( \"items\" , ... ) @classmethod def __get_validators__ ( cls ): yield pydantic . validators . str_validator yield cls . validate @classmethod def validate ( cls , value ) -> \"RGB\" : if ( not isinstance ( value , str ) or re . fullmatch ( r \"#[0-9A-Fa-f] {6} \" , value ) is None ): raise ValueError ( \"Invalid RGB\" ) return RGB ( red = int ( value [ 1 : 3 ], 16 ), green = int ( value [ 3 : 5 ], 16 ), blue = int ( value [ 5 : 7 ], 16 ) ) # Simpler with apischema class RGB ( NamedTuple ): red : int green : int blue : int # NewType can be used to add schema to conversion source/target # but Annotated[str, apischema.schema(pattern=r\"#[0-9A-Fa-f]{6}\")] would have worked too HexaRGB = NewType ( \"HexaRGB\" , str ) # pattern is used in JSON schema and in deserialization validation apischema . schema ( pattern = r \"#[0-9A-Fa-f] {6} \" )( HexaRGB ) @apischema . deserializer # could be declared as a staticmethod of RGB class def from_hexa ( hexa : HexaRGB ) -> RGB : return RGB ( int ( hexa [ 1 : 3 ], 16 ), int ( hexa [ 3 : 5 ], 16 ), int ( hexa [ 5 : 7 ], 16 )) @apischema . serializer # could be declared as a method/property of RGB class def to_hexa ( rgb : RGB ) -> HexaRGB : return HexaRGB ( f \"# { rgb . red : 02x }{ rgb . green : 02x }{ rgb . blue : 02x } \" ) assert ( # schema is inherited from deserialized type apischema . json_schema . deserialization_schema ( RGB ) == apischema . json_schema . deserialization_schema ( HexaRGB ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"type\" : \"string\" , \"pattern\" : \"#[0-9A-Fa-f] {6} \" , } )","title":"apischema conversions feature allows to support any type defined in your code, but also in external libraries"},{"location":"difference_with_pydantic/#apischema-can-also-customize-serialization-with-computed-fields","text":"Serialized methods/properties are regular methods/properties which are included in serialization effortlessly.","title":"apischema can also customize serialization with computed fields"},{"location":"difference_with_pydantic/#apischema-allows-you-to-use-composition-over-inheritance","text":"Flattened fields is a distinctive apischema feature that is very handy to build complex model from smaller fragments; you don't have to merge yourself the fields of your fragments in a complex class with a lot of fields, apischema deal with it for you, and your code is kept simple.","title":"apischema allows you to use composition over inheritance"},{"location":"difference_with_pydantic/#apischema-has-a-functional-approach-pydantic-has-an-object-one","text":"pydantic features are based on BaseModel methods. You have to have a BaseModel instance to do anything, even if you manipulate only an integer. Complex pydantic stuff like __root__ model or deserialization customization come from this approach. apischema is functional, it doesn't use method but simple functions, which works with all types. You can also register conversions for any types similarly you would implement a type class in a functional language. And your class namespace don't mix up with a mandatory base class' one.","title":"apischema has a functional approach, pydantic has an object one"},{"location":"difference_with_pydantic/#apischema-can-use-both-camelcase-and-snake_case-with-the-same-types","text":"While pydantic field aliases are fixed at model creation, apischema let you choose which aliasing you want at (de)serialization time. It can be convenient if you need to juggle with cases for the same models between frontends and other backend services for example.","title":"apischema can use both camelCase and snake_case with the same types"},{"location":"difference_with_pydantic/#apischema-doesnt-coerce-by-default","text":"Your API respects its schema. It can also coerce, for example to parse configuration file, and coercion can be adjusted (for example coercing list from comma-separated string).","title":"apischema doesn't coerce by default"},{"location":"difference_with_pydantic/#apischema-has-a-better-integration-of-json-schemaopenapi","text":"With pydantic , if you want to have a nullable field in the generated schema, you have to put nullable into schema extra keywords. apischema is bound to the last JSON schema version but offers conversion to other version like OpenAPI 3.0 and nullable is added for Optional types. apischema also support more advanced features like dependentRequired or unevaluatedProperties . Reference handling is also more flexible","title":"apischema has a better integration of JSON schema/OpenAPI"},{"location":"difference_with_pydantic/#apischema-can-add-validators-and-json-schema-to-newtype","text":"So it will be used in deserialization validation. You can use NewType everywhere, to gain a better type checking, self-documented code.","title":"apischema can add validators and JSON schema to NewType"},{"location":"difference_with_pydantic/#apischema-validators-are-regular-methods-with-automatic-dependencies-management","text":"Using regular methods allows benefiting of type checking of fields, where pydantic validators use dynamic stuffs (name of the fields as strings) and are not type-checked or have to get redundant type annotations. apischema validators also have automatic dependency management. And apischema directly supports JSON schema property dependencies . Comparison is simple with an example (validator is taken from pydantic documentation : from dataclasses import dataclass import apischema import pydantic class UserModel ( pydantic . BaseModel ): username : str password1 : str password2 : str @pydantic . root_validator def check_passwords_match ( cls , values ): # This is a classmethod (it needs a plugin to not raise a warning in your IDE) # What is the type of of values? of values['password1']? # If you rename password1 field, validator will hardly be updated # You also have to test yourself that values are provided pw1 , pw2 = values . get ( \"password1\" ), values . get ( \"password2\" ) if pw1 is not None and pw2 is not None and pw1 != pw2 : raise ValueError ( \"passwords do not match\" ) return values @dataclass class LoginForm : username : str password1 : str password2 : str @apischema . validator def check_password_match ( self ): # Typed checked, simpler, and not executed if password1 or password2 # are missing/invalid if self . password1 != self . password2 : raise ValueError ( \"passwords do not match\" )","title":"apischema validators are regular methods with automatic dependencies management"},{"location":"difference_with_pydantic/#apischema-supports-pydantic","text":"It's not a feature, is just the result of 30 lines of code .","title":"apischema supports pydantic"},{"location":"json_schema/","text":"JSON schema \u00b6 JSON schema generation \u00b6 JSON schema can be generated from data model. However, because of all possible customizations , schema can be differ between deserilialization and serialization. In common cases, deserialization_schema and serialization_schema will give the same result. from dataclasses import dataclass from apischema.json_schema import deserialization_schema , serialization_schema @dataclass class Foo : bar : str assert deserialization_schema ( Foo ) == serialization_schema ( Foo ) assert deserialization_schema ( Foo ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"additionalProperties\" : False , \"properties\" : { \"bar\" : { \"type\" : \"string\" }}, \"required\" : [ \"bar\" ], \"type\" : \"object\" , } Field alias \u00b6 Sometimes dataclass field names can clash with language keyword, sometimes the property name is not convenient. Hopefully, field can define an alias which will be used in schema and deserialization/serialization. from dataclasses import dataclass , field from apischema import alias , deserialize , serialize from apischema.json_schema import deserialization_schema @dataclass class Foo : class_ : str = field ( metadata = alias ( \"class\" )) assert deserialization_schema ( Foo ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"additionalProperties\" : False , \"properties\" : { \"class\" : { \"type\" : \"string\" }}, \"required\" : [ \"class\" ], \"type\" : \"object\" , } assert deserialize ( Foo , { \"class\" : \"bar\" }) == Foo ( \"bar\" ) assert serialize ( Foo , Foo ( \"bar\" )) == { \"class\" : \"bar\" } Alias all fields \u00b6 Field aliasing can also be done at class level by specifying an aliasing function. This aliaser is applied to field alias if defined or field name, or not applied if override=False is specified. from dataclasses import dataclass , field from typing import Any from apischema import alias from apischema.json_schema import deserialization_schema @alias ( lambda s : f \"foo_ { s } \" ) @dataclass class Foo : field1 : Any field2 : Any = field ( metadata = alias ( override = False )) field3 : Any = field ( metadata = alias ( \"field03\" )) field4 : Any = field ( metadata = alias ( \"field04\" , override = False )) assert deserialization_schema ( Foo ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"additionalProperties\" : False , \"properties\" : { \"foo_field1\" : {}, \"field2\" : {}, \"foo_field03\" : {}, \"field04\" : {}}, \"required\" : [ \"foo_field1\" , \"field2\" , \"foo_field03\" , \"field04\" ], \"type\" : \"object\" , } Class-level aliasing can be used to define a camelCase API. Dynamic aliasing and default aliaser \u00b6 apischema operations deserialize / serialize / deserialization_schema / serialization_schema provide an aliaser parameter which will be applied on every fields being processed in this operation. Similar to strictness configuration , this parameter has a default value controlled by apischema.settings.aliaser . It can be used for example to make all an application use camelCase . Actually, there is a shortcut for that: Otherwise, it's used the same way than settings.coercer . from apischema import settings settings . camel_case = True Note Dynamic aliaser ignores override=False Schema annotations \u00b6 Type annotations are not enough to express a complete schema, but apischema has a function for that; schema can be used both as type decorator or field metadata. from dataclasses import dataclass , field from typing import NewType from apischema import schema from apischema.json_schema import deserialization_schema Tag = NewType ( \"Tag\" , str ) schema ( min_len = 3 , pattern = r \"^\\w*$\" , examples = [ \"available\" , \"EMEA\" ])( Tag ) @dataclass class Resource : id : int tags : list [ Tag ] = field ( default_factory = list , metadata = schema ( description = \"regroup multiple resources\" , max_items = 3 , unique = True ), ) assert deserialization_schema ( Resource ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"additionalProperties\" : False , \"properties\" : { \"id\" : { \"type\" : \"integer\" }, \"tags\" : { \"description\" : \"regroup multiple resources\" , \"items\" : { \"examples\" : [ \"available\" , \"EMEA\" ], \"minLength\" : 3 , \"pattern\" : \"^ \\\\ w*$\" , \"type\" : \"string\" , }, \"maxItems\" : 3 , \"type\" : \"array\" , \"uniqueItems\" : True , \"default\" : [], }, }, \"required\" : [ \"id\" ], \"type\" : \"object\" , } Note Schema are particularly useful with NewType . For example, if you use prefixed ids, you can use a NewType with a pattern schema to validate them, and benefit of more precise type checking. The following keys are available (they are sometimes shorten compared to JSON schema original for code concision and snake_case): Key JSON schema keyword type restriction title / / description / / default / / examples / / min minimum int max maximum int exc_min exclusiveMinimum int exc_max exclusiveMaximum int mult_of multipleOf int format / str media_type contentMediaType str encoding contentEncoding str min_len minLength str max_len maxLength str pattern / str min_items minItems list max_items maxItems list unique / list min_props minProperties dict max_props maxProperties dict Note In case of field schema, field default value will be serialized (if possible) to add default keyword to the schema. Constraints validation \u00b6 JSON schema constrains the data deserialized; this constraints are naturally used for validation. from dataclasses import dataclass , field from typing import NewType from pytest import raises from apischema import ValidationError , deserialize , schema , serialize Tag = NewType ( \"Tag\" , str ) schema ( min_len = 3 , pattern = r \"^\\w*$\" , examples = [ \"available\" , \"EMEA\" ])( Tag ) @dataclass class Resource : id : int tags : list [ Tag ] = field ( default_factory = list , metadata = schema ( description = \"regroup multiple resources\" , max_items = 3 , unique = True ), ) with raises ( ValidationError ) as err : # pytest check exception is raised deserialize ( Resource , { \"id\" : 42 , \"tags\" : [ \"tag\" , \"duplicate\" , \"duplicate\" , \"bad&\" , \"_\" ]} ) assert serialize ( err . value ) == [ { \"loc\" : [ \"tags\" ], \"err\" : [ \"item count greater than 3 (maxItems)\" , \"duplicate items (uniqueItems)\" , ], }, { \"loc\" : [ \"tags\" , 3 ], \"err\" : [ \"not matching '^ \\\\ w*$' (pattern)\" ]}, { \"loc\" : [ \"tags\" , 4 ], \"err\" : [ \"string length lower than 3 (minLength)\" ]}, ] Default schema \u00b6 When no schema are defined, a default schema can be computed using settings.default_schema like this: from typing import Optional from apischema import schema , settings from apischema.schemas import Schema def default_schema ( cls ) -> Optional [ Schema ]: return schema ( ... ) if ... else None settings . default_schema = default_schema Default implementation returns None for every types. Extra schema \u00b6 schema has two other arguments: extra and override , which give a finer control of the JSON schema generated: extra and override . It can be used for example to build \"strict\" unions (using oneOf instead of anyOf ) from dataclasses import dataclass from typing import Annotated , Any , Union from apischema import schema from apischema.json_schema import deserialization_schema # schema extra can be callable to modify the schema in place def to_one_of ( schema : dict [ str , Any ]): if \"anyOf\" in schema : schema [ \"oneOf\" ] = schema . pop ( \"anyOf\" ) OneOf = schema ( extra = to_one_of ) # or extra can be a dictionary which will update the schema @schema ( extra = { \"$ref\" : \"http://some-domain.org/path/to/schema.json#/$defs/Foo\" }, override = True , # override apischema generated schema, using only extra ) @dataclass class Foo : bar : int # Use Annotated with OneOf to make a \"strict\" Union assert deserialization_schema ( Annotated [ Union [ Foo , int ], OneOf ]) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"oneOf\" : [ # oneOf instead of anyOf { \"$ref\" : \"http://some-domain.org/path/to/schema.json#/$defs/Foo\" }, { \"type\" : \"integer\" }, ], } Required field with default value \u00b6 By default, a dataclass/namedtuple field will be tagged required if it doesn't have a default value. However, you may want to have a default value for a field in order to be more convenient in your code, but still make the field required. One could think about some schema model where version is fixed but is required, for example JSON-RPC with \"jsonrpc\": \"2.0\" . That's done with field metadata required . from dataclasses import dataclass , field from typing import Optional from pytest import raises from apischema import ValidationError , deserialize , serialize from apischema.metadata import required @dataclass class Foo : bar : Optional [ int ] = field ( default = None , metadata = required ) with raises ( ValidationError ) as err : deserialize ( Foo , {}) assert serialize ( err . value ) == [{ \"loc\" : [ \"bar\" ], \"err\" : [ \"missing property\" ]}] Additional properties / pattern properties \u00b6 With Mapping \u00b6 Schema of a Mapping / dict type is naturally translated to \"additionalProperties\": <schema of the value type> . However when the schema of the key has a pattern , it will give a \"patternProperties\": {<key pattern>: <schema of the value type>} With dataclass \u00b6 additionalProperties / patternProperties can be added to dataclasses by using fields annotated with properties metadata. Properties not mapped on regular fields will be deserialized into this fields; they must have a Mapping type, or be deserializable from a Mapping , because they are instantiated with a mapping. from collections.abc import Mapping from dataclasses import dataclass , field from typing import Annotated from apischema import deserialize , properties , schema from apischema.json_schema import deserialization_schema @dataclass class Config : active : bool = True server_options : Mapping [ str , bool ] = field ( default_factory = dict , metadata = properties ( pattern = r \"^server_\" ) ) client_options : Mapping [ Annotated [ str , schema ( pattern = r \"^client_\" )], bool # noqa: F722 ] = field ( default_factory = dict , metadata = properties ( ... )) options : Mapping [ str , bool ] = field ( default_factory = dict , metadata = properties ) assert deserialize ( Config , { \"use_lightsaber\" : True , \"server_auto_restart\" : False , \"client_timeout\" : False }, ) == Config ( True , { \"server_auto_restart\" : False }, { \"client_timeout\" : False }, { \"use_lightsaber\" : True }, ) assert deserialization_schema ( Config ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"type\" : \"object\" , \"properties\" : { \"active\" : { \"type\" : \"boolean\" , \"default\" : True }}, \"additionalProperties\" : { \"type\" : \"boolean\" }, \"patternProperties\" : { \"^server_\" : { \"type\" : \"boolean\" }, \"^client_\" : { \"type\" : \"boolean\" }, }, } Note Of course, a dataclass can only have a single properties field without pattern, because it makes no sens to have several additionalProperties . Property dependencies \u00b6 apischema support property dependencies for dataclass through a class member. Dependencies are also used in validation. Note JSON schema draft 2019-09 renames properties dependencies dependentRequired to disambiguate with schema dependencies from dataclasses import dataclass , field from pytest import raises from apischema import ValidationError , dependent_required , deserialize , serialize from apischema.json_schema import deserialization_schema from apischema.skip import NotNull @dataclass class Billing : name : str # Fields used in dependencies MUST be declared with `field` credit_card : NotNull [ int ] = field ( default = None ) billing_address : NotNull [ str ] = field ( default = None ) dependencies = dependent_required ({ credit_card : [ billing_address ]}) # it can also be done outside the class with # dependent_required({\"credit_card\": [\"billing_address\"]}, owner=Billing) assert deserialization_schema ( Billing ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"additionalProperties\" : False , \"dependentRequired\" : { \"credit_card\" : [ \"billing_address\" ]}, \"properties\" : { \"name\" : { \"type\" : \"string\" }, \"credit_card\" : { \"type\" : \"integer\" }, \"billing_address\" : { \"type\" : \"string\" }, }, \"required\" : [ \"name\" ], \"type\" : \"object\" , } with raises ( ValidationError ) as err : deserialize ( Billing , { \"name\" : \"Anonymous\" , \"credit_card\" : 1234_5678_9012_3456 }) assert serialize ( err . value ) == [ { \"loc\" : [ \"billing_address\" ], \"err\" : [ \"missing property (required by ['credit_card'])\" ], } ] Because bidirectional dependencies are a common idiom, apischema provides a shortcut notation; it's indeed possible to write dependent_required([credit_card, billing_adress]) . JSON schema reference \u00b6 For complex schema with type reuse, it's convenient to extract definitions of schema components in order to reuse them \u2014 it's even mandatory for recursive types; JSON schema use JSON pointers \"$ref\" to refer to the definitions. apischema handles this feature natively. from dataclasses import dataclass from typing import Optional from apischema.json_schema import deserialization_schema @dataclass class Node : value : int child : Optional [ \"Node\" ] = None assert deserialization_schema ( Node ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"$ref\" : \"#/$defs/Node\" , \"$defs\" : { \"Node\" : { \"type\" : \"object\" , \"properties\" : { \"value\" : { \"type\" : \"integer\" }, \"child\" : { \"anyOf\" : [{ \"$ref\" : \"#/$defs/Node\" }, { \"type\" : \"null\" }], \"default\" : None , }, }, \"required\" : [ \"value\" ], \"additionalProperties\" : False , } }, } Use reference only for reused types \u00b6 apischema can control the reference use through the boolean all_ref parameter of deserialization_schema / serialization_schema : all_refs=True -> all types with a reference will be put in the definitions and referenced with $ref ; all_refs=False -> only types which are reused in the schema are put in definitions all_refs default value depends on the JSON schema version : it's False for JSON schema drafts but True for OpenAPI . from dataclasses import dataclass from apischema.json_schema import deserialization_schema @dataclass class Bar : baz : str @dataclass class Foo : bar1 : Bar bar2 : Bar assert deserialization_schema ( Foo , all_refs = False ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"$defs\" : { \"Bar\" : { \"additionalProperties\" : False , \"properties\" : { \"baz\" : { \"type\" : \"string\" }}, \"required\" : [ \"baz\" ], \"type\" : \"object\" , } }, \"additionalProperties\" : False , \"properties\" : { \"bar1\" : { \"$ref\" : \"#/$defs/Bar\" }, \"bar2\" : { \"$ref\" : \"#/$defs/Bar\" }}, \"required\" : [ \"bar1\" , \"bar2\" ], \"type\" : \"object\" , } assert deserialization_schema ( Foo , all_refs = True ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"$defs\" : { \"Bar\" : { \"additionalProperties\" : False , \"properties\" : { \"baz\" : { \"type\" : \"string\" }}, \"required\" : [ \"baz\" ], \"type\" : \"object\" , }, \"Foo\" : { \"additionalProperties\" : False , \"properties\" : { \"bar1\" : { \"$ref\" : \"#/$defs/Bar\" }, \"bar2\" : { \"$ref\" : \"#/$defs/Bar\" }, }, \"required\" : [ \"bar1\" , \"bar2\" ], \"type\" : \"object\" , }, }, \"$ref\" : \"#/$defs/Foo\" , } Set reference name \u00b6 In the previous examples, types were referenced using their name. This is indeed the default behavior for every classes/ NewType s (except primitive int / str / bool / float ). It's possible to override the default reference name using apischema.type_name ; passing None instead of a string will remove the reference, making the type unable to be referenced as a separate definition in the schema. from dataclasses import dataclass from typing import Annotated from apischema import type_name from apischema.json_schema import deserialization_schema # Type name can be added as a decorator @type_name ( \"Resource\" ) @dataclass class BaseResource : id : int # or using typing.Annotated tags : Annotated [ set [ str ], type_name ( \"ResourceTags\" )] assert deserialization_schema ( BaseResource , all_refs = True ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"$defs\" : { \"Resource\" : { \"type\" : \"object\" , \"properties\" : { \"id\" : { \"type\" : \"integer\" }, \"tags\" : { \"$ref\" : \"#/$defs/ResourceTags\" }, }, \"required\" : [ \"id\" , \"tags\" ], \"additionalProperties\" : False , }, \"ResourceTags\" : { \"type\" : \"array\" , \"items\" : { \"type\" : \"string\" }, \"uniqueItems\" : True , }, }, \"$ref\" : \"#/$defs/Resource\" , } Note Builtin collections are interchangeable when a type_name is registered. For example, if a name is registered for list[Foo] , this name will also be used for Sequence[Foo] or Collection[Foo] . Generic aliases can have a type name, but they need to be specialized; Foo[T, int] cannot have a type name but Foo[str, int] can. However, generic classes can get a dynamic type name depending on their generic argument, passing a name factory to type_name : from dataclasses import dataclass , field from typing import Generic , TypeVar from apischema import type_name from apischema.json_schema import deserialization_schema from apischema.metadata import flattened T = TypeVar ( \"T\" ) # Type name factory takes the type and its arguments as (positional) parameters @type_name ( lambda tp , arg : f \" { arg . __name__ } Resource\" ) @dataclass class Resource ( Generic [ T ]): id : int content : T = field ( metadata = flattened ) ... @dataclass class Foo : bar : str assert deserialization_schema ( Resource [ Foo ], all_refs = True ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"$ref\" : \"#/$defs/FooResource\" , \"$defs\" : { \"FooResource\" : { \"type\" : \"object\" , \"allOf\" : [ { \"type\" : \"object\" , \"properties\" : { \"id\" : { \"type\" : \"integer\" }}, \"required\" : [ \"id\" ], \"additionalProperties\" : False , }, { \"$ref\" : \"#/$defs/Foo\" }, ], \"unevaluatedProperties\" : False , }, \"Foo\" : { \"type\" : \"object\" , \"properties\" : { \"bar\" : { \"type\" : \"string\" }}, \"required\" : [ \"bar\" ], \"additionalProperties\" : False , }, }, } The default behavior can also be customized using apischema.settings.default_type_name : Reference factory \u00b6 In JSON schema, $ref looks like #/$defs/Foo , not just Foo . In fact, schema generation use the ref given by type_name / default_type_name and pass it to a ref_factory function (a parameter of schema generation functions) which will convert it to its final form. JSON schema version comes with its default ref_factory , for draft 2019-09, it prefixes the ref with #/$defs/ , while it prefixes with #/components/schema in case of OpenAPI . from dataclasses import dataclass from apischema.json_schema import deserialization_schema @dataclass class Foo : bar : int def ref_factory ( ref : str ) -> str : return f \"http://some-domain.org/path/to/ { ref } .json#\" assert deserialization_schema ( Foo , all_refs = True , ref_factory = ref_factory ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"$ref\" : \"http://some-domain.org/path/to/Foo.json#\" , } Note When ref_factory is passed in arguments, definitions are not added to the generated schema. That's because ref_factory would surely change definitions location, so there would be no interest to add them with a wrong location. These definitions can of course be generated separately with definitions_schema . Definitions schema \u00b6 Definitions schemas can also be extracted using apischema.json_schema.definitions_schema . It takes two lists deserialization / serialization of types (or tuple of type + dynamic conversion ) and returns a dictionary of all referenced schemas. Note This is especially useful when it comes to OpenAPI schema to generate the components section. from dataclasses import dataclass from apischema.json_schema import definitions_schema @dataclass class Bar : baz : int = 0 @dataclass class Foo : bar : Bar assert definitions_schema ( deserialization = [ list [ Foo ]], all_refs = True ) == { \"Foo\" : { \"type\" : \"object\" , \"properties\" : { \"bar\" : { \"$ref\" : \"#/$defs/Bar\" }}, \"required\" : [ \"bar\" ], \"additionalProperties\" : False , }, \"Bar\" : { \"type\" : \"object\" , \"properties\" : { \"baz\" : { \"type\" : \"integer\" , \"default\" : 0 }}, \"additionalProperties\" : False , }, } JSON schema / OpenAPI version \u00b6 JSON schema has several versions \u2014 OpenAPI is treated as a JSON schema version. If apischema natively use the last one: draft 2019-09, it is possible to specify a schema version which will be used for the generation. from dataclasses import dataclass from typing import Literal , Optional from apischema.json_schema import ( JsonSchemaVersion , definitions_schema , deserialization_schema , ) @dataclass class Bar : baz : Optional [ int ] constant : Literal [ 0 ] = 0 @dataclass class Foo : bar : Bar assert deserialization_schema ( Foo , all_refs = True ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"$ref\" : \"#/$defs/Foo\" , \"$defs\" : { \"Foo\" : { \"type\" : \"object\" , \"properties\" : { \"bar\" : { \"$ref\" : \"#/$defs/Bar\" }}, \"required\" : [ \"bar\" ], \"additionalProperties\" : False , }, \"Bar\" : { \"type\" : \"object\" , \"properties\" : { \"baz\" : { \"type\" : [ \"integer\" , \"null\" ]}, \"constant\" : { \"type\" : \"integer\" , \"const\" : 0 , \"default\" : 0 }, }, \"required\" : [ \"baz\" ], \"additionalProperties\" : False , }, }, } assert deserialization_schema ( Foo , all_refs = True , version = JsonSchemaVersion . DRAFT_7 ) == { \"$schema\" : \"http://json-schema.org/draft-07/schema#\" , # $ref is isolated in allOf + draft 7 prefix \"allOf\" : [{ \"$ref\" : \"#/definitions/Foo\" }], \"definitions\" : { # not \"$defs\" \"Foo\" : { \"type\" : \"object\" , \"properties\" : { \"bar\" : { \"$ref\" : \"#/definitions/Bar\" }}, \"required\" : [ \"bar\" ], \"additionalProperties\" : False , }, \"Bar\" : { \"type\" : \"object\" , \"properties\" : { \"baz\" : { \"type\" : [ \"integer\" , \"null\" ]}, \"constant\" : { \"type\" : \"integer\" , \"const\" : 0 , \"default\" : 0 }, }, \"required\" : [ \"baz\" ], \"additionalProperties\" : False , }, }, } assert deserialization_schema ( Foo , version = JsonSchemaVersion . OPEN_API_3_0 ) == { # No definitions for OpenAPI, use definitions_schema for it \"$ref\" : \"#/components/schemas/Foo\" # OpenAPI prefix } assert definitions_schema ( deserialization = [ Foo ], version = JsonSchemaVersion . OPEN_API_3_0 ) == { \"Foo\" : { \"type\" : \"object\" , \"properties\" : { \"bar\" : { \"$ref\" : \"#/components/schemas/Bar\" }}, \"required\" : [ \"bar\" ], \"additionalProperties\" : False , }, \"Bar\" : { \"type\" : \"object\" , # \"nullable\" instead of \"type\": \"null\" \"properties\" : { \"baz\" : { \"type\" : \"integer\" , \"nullable\" : True }, \"constant\" : { \"type\" : \"integer\" , \"enum\" : [ 0 ], \"default\" : 0 }, }, \"required\" : [ \"baz\" ], \"additionalProperties\" : False , }, } readOnly / writeOnly \u00b6 Dataclasses InitVar and field(init=False) fields will be flagged respectively with \"writeOnly\": true and \"readOnly\": true in the generated schema. In definitions schema , if a type appears both in deserialization and serialization, properties are merged and the resulting schema contains then readOnly and writeOnly properties. By the way, the required is not merged because it can't (it would mess up validation if some not-init field was required), so deserialization required is kept because it's more important as it can be used in validation ( OpenAPI 3.0 semantic which allows the merge has been dropped in 3.1, so it has not been judged useful to be supported)","title":"JSON schema"},{"location":"json_schema/#json-schema","text":"","title":"JSON schema"},{"location":"json_schema/#json-schema-generation","text":"JSON schema can be generated from data model. However, because of all possible customizations , schema can be differ between deserilialization and serialization. In common cases, deserialization_schema and serialization_schema will give the same result. from dataclasses import dataclass from apischema.json_schema import deserialization_schema , serialization_schema @dataclass class Foo : bar : str assert deserialization_schema ( Foo ) == serialization_schema ( Foo ) assert deserialization_schema ( Foo ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"additionalProperties\" : False , \"properties\" : { \"bar\" : { \"type\" : \"string\" }}, \"required\" : [ \"bar\" ], \"type\" : \"object\" , }","title":"JSON schema generation"},{"location":"json_schema/#field-alias","text":"Sometimes dataclass field names can clash with language keyword, sometimes the property name is not convenient. Hopefully, field can define an alias which will be used in schema and deserialization/serialization. from dataclasses import dataclass , field from apischema import alias , deserialize , serialize from apischema.json_schema import deserialization_schema @dataclass class Foo : class_ : str = field ( metadata = alias ( \"class\" )) assert deserialization_schema ( Foo ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"additionalProperties\" : False , \"properties\" : { \"class\" : { \"type\" : \"string\" }}, \"required\" : [ \"class\" ], \"type\" : \"object\" , } assert deserialize ( Foo , { \"class\" : \"bar\" }) == Foo ( \"bar\" ) assert serialize ( Foo , Foo ( \"bar\" )) == { \"class\" : \"bar\" }","title":"Field alias"},{"location":"json_schema/#alias-all-fields","text":"Field aliasing can also be done at class level by specifying an aliasing function. This aliaser is applied to field alias if defined or field name, or not applied if override=False is specified. from dataclasses import dataclass , field from typing import Any from apischema import alias from apischema.json_schema import deserialization_schema @alias ( lambda s : f \"foo_ { s } \" ) @dataclass class Foo : field1 : Any field2 : Any = field ( metadata = alias ( override = False )) field3 : Any = field ( metadata = alias ( \"field03\" )) field4 : Any = field ( metadata = alias ( \"field04\" , override = False )) assert deserialization_schema ( Foo ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"additionalProperties\" : False , \"properties\" : { \"foo_field1\" : {}, \"field2\" : {}, \"foo_field03\" : {}, \"field04\" : {}}, \"required\" : [ \"foo_field1\" , \"field2\" , \"foo_field03\" , \"field04\" ], \"type\" : \"object\" , } Class-level aliasing can be used to define a camelCase API.","title":"Alias all fields"},{"location":"json_schema/#dynamic-aliasing-and-default-aliaser","text":"apischema operations deserialize / serialize / deserialization_schema / serialization_schema provide an aliaser parameter which will be applied on every fields being processed in this operation. Similar to strictness configuration , this parameter has a default value controlled by apischema.settings.aliaser . It can be used for example to make all an application use camelCase . Actually, there is a shortcut for that: Otherwise, it's used the same way than settings.coercer . from apischema import settings settings . camel_case = True Note Dynamic aliaser ignores override=False","title":"Dynamic aliasing and default aliaser"},{"location":"json_schema/#schema-annotations","text":"Type annotations are not enough to express a complete schema, but apischema has a function for that; schema can be used both as type decorator or field metadata. from dataclasses import dataclass , field from typing import NewType from apischema import schema from apischema.json_schema import deserialization_schema Tag = NewType ( \"Tag\" , str ) schema ( min_len = 3 , pattern = r \"^\\w*$\" , examples = [ \"available\" , \"EMEA\" ])( Tag ) @dataclass class Resource : id : int tags : list [ Tag ] = field ( default_factory = list , metadata = schema ( description = \"regroup multiple resources\" , max_items = 3 , unique = True ), ) assert deserialization_schema ( Resource ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"additionalProperties\" : False , \"properties\" : { \"id\" : { \"type\" : \"integer\" }, \"tags\" : { \"description\" : \"regroup multiple resources\" , \"items\" : { \"examples\" : [ \"available\" , \"EMEA\" ], \"minLength\" : 3 , \"pattern\" : \"^ \\\\ w*$\" , \"type\" : \"string\" , }, \"maxItems\" : 3 , \"type\" : \"array\" , \"uniqueItems\" : True , \"default\" : [], }, }, \"required\" : [ \"id\" ], \"type\" : \"object\" , } Note Schema are particularly useful with NewType . For example, if you use prefixed ids, you can use a NewType with a pattern schema to validate them, and benefit of more precise type checking. The following keys are available (they are sometimes shorten compared to JSON schema original for code concision and snake_case): Key JSON schema keyword type restriction title / / description / / default / / examples / / min minimum int max maximum int exc_min exclusiveMinimum int exc_max exclusiveMaximum int mult_of multipleOf int format / str media_type contentMediaType str encoding contentEncoding str min_len minLength str max_len maxLength str pattern / str min_items minItems list max_items maxItems list unique / list min_props minProperties dict max_props maxProperties dict Note In case of field schema, field default value will be serialized (if possible) to add default keyword to the schema.","title":"Schema annotations"},{"location":"json_schema/#constraints-validation","text":"JSON schema constrains the data deserialized; this constraints are naturally used for validation. from dataclasses import dataclass , field from typing import NewType from pytest import raises from apischema import ValidationError , deserialize , schema , serialize Tag = NewType ( \"Tag\" , str ) schema ( min_len = 3 , pattern = r \"^\\w*$\" , examples = [ \"available\" , \"EMEA\" ])( Tag ) @dataclass class Resource : id : int tags : list [ Tag ] = field ( default_factory = list , metadata = schema ( description = \"regroup multiple resources\" , max_items = 3 , unique = True ), ) with raises ( ValidationError ) as err : # pytest check exception is raised deserialize ( Resource , { \"id\" : 42 , \"tags\" : [ \"tag\" , \"duplicate\" , \"duplicate\" , \"bad&\" , \"_\" ]} ) assert serialize ( err . value ) == [ { \"loc\" : [ \"tags\" ], \"err\" : [ \"item count greater than 3 (maxItems)\" , \"duplicate items (uniqueItems)\" , ], }, { \"loc\" : [ \"tags\" , 3 ], \"err\" : [ \"not matching '^ \\\\ w*$' (pattern)\" ]}, { \"loc\" : [ \"tags\" , 4 ], \"err\" : [ \"string length lower than 3 (minLength)\" ]}, ]","title":"Constraints validation"},{"location":"json_schema/#default-schema","text":"When no schema are defined, a default schema can be computed using settings.default_schema like this: from typing import Optional from apischema import schema , settings from apischema.schemas import Schema def default_schema ( cls ) -> Optional [ Schema ]: return schema ( ... ) if ... else None settings . default_schema = default_schema Default implementation returns None for every types.","title":"Default schema"},{"location":"json_schema/#extra-schema","text":"schema has two other arguments: extra and override , which give a finer control of the JSON schema generated: extra and override . It can be used for example to build \"strict\" unions (using oneOf instead of anyOf ) from dataclasses import dataclass from typing import Annotated , Any , Union from apischema import schema from apischema.json_schema import deserialization_schema # schema extra can be callable to modify the schema in place def to_one_of ( schema : dict [ str , Any ]): if \"anyOf\" in schema : schema [ \"oneOf\" ] = schema . pop ( \"anyOf\" ) OneOf = schema ( extra = to_one_of ) # or extra can be a dictionary which will update the schema @schema ( extra = { \"$ref\" : \"http://some-domain.org/path/to/schema.json#/$defs/Foo\" }, override = True , # override apischema generated schema, using only extra ) @dataclass class Foo : bar : int # Use Annotated with OneOf to make a \"strict\" Union assert deserialization_schema ( Annotated [ Union [ Foo , int ], OneOf ]) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"oneOf\" : [ # oneOf instead of anyOf { \"$ref\" : \"http://some-domain.org/path/to/schema.json#/$defs/Foo\" }, { \"type\" : \"integer\" }, ], }","title":"Extra schema"},{"location":"json_schema/#required-field-with-default-value","text":"By default, a dataclass/namedtuple field will be tagged required if it doesn't have a default value. However, you may want to have a default value for a field in order to be more convenient in your code, but still make the field required. One could think about some schema model where version is fixed but is required, for example JSON-RPC with \"jsonrpc\": \"2.0\" . That's done with field metadata required . from dataclasses import dataclass , field from typing import Optional from pytest import raises from apischema import ValidationError , deserialize , serialize from apischema.metadata import required @dataclass class Foo : bar : Optional [ int ] = field ( default = None , metadata = required ) with raises ( ValidationError ) as err : deserialize ( Foo , {}) assert serialize ( err . value ) == [{ \"loc\" : [ \"bar\" ], \"err\" : [ \"missing property\" ]}]","title":"Required field with default value"},{"location":"json_schema/#additional-properties-pattern-properties","text":"","title":"Additional properties / pattern properties"},{"location":"json_schema/#with-mapping","text":"Schema of a Mapping / dict type is naturally translated to \"additionalProperties\": <schema of the value type> . However when the schema of the key has a pattern , it will give a \"patternProperties\": {<key pattern>: <schema of the value type>}","title":"With Mapping"},{"location":"json_schema/#with-dataclass","text":"additionalProperties / patternProperties can be added to dataclasses by using fields annotated with properties metadata. Properties not mapped on regular fields will be deserialized into this fields; they must have a Mapping type, or be deserializable from a Mapping , because they are instantiated with a mapping. from collections.abc import Mapping from dataclasses import dataclass , field from typing import Annotated from apischema import deserialize , properties , schema from apischema.json_schema import deserialization_schema @dataclass class Config : active : bool = True server_options : Mapping [ str , bool ] = field ( default_factory = dict , metadata = properties ( pattern = r \"^server_\" ) ) client_options : Mapping [ Annotated [ str , schema ( pattern = r \"^client_\" )], bool # noqa: F722 ] = field ( default_factory = dict , metadata = properties ( ... )) options : Mapping [ str , bool ] = field ( default_factory = dict , metadata = properties ) assert deserialize ( Config , { \"use_lightsaber\" : True , \"server_auto_restart\" : False , \"client_timeout\" : False }, ) == Config ( True , { \"server_auto_restart\" : False }, { \"client_timeout\" : False }, { \"use_lightsaber\" : True }, ) assert deserialization_schema ( Config ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"type\" : \"object\" , \"properties\" : { \"active\" : { \"type\" : \"boolean\" , \"default\" : True }}, \"additionalProperties\" : { \"type\" : \"boolean\" }, \"patternProperties\" : { \"^server_\" : { \"type\" : \"boolean\" }, \"^client_\" : { \"type\" : \"boolean\" }, }, } Note Of course, a dataclass can only have a single properties field without pattern, because it makes no sens to have several additionalProperties .","title":"With dataclass"},{"location":"json_schema/#property-dependencies","text":"apischema support property dependencies for dataclass through a class member. Dependencies are also used in validation. Note JSON schema draft 2019-09 renames properties dependencies dependentRequired to disambiguate with schema dependencies from dataclasses import dataclass , field from pytest import raises from apischema import ValidationError , dependent_required , deserialize , serialize from apischema.json_schema import deserialization_schema from apischema.skip import NotNull @dataclass class Billing : name : str # Fields used in dependencies MUST be declared with `field` credit_card : NotNull [ int ] = field ( default = None ) billing_address : NotNull [ str ] = field ( default = None ) dependencies = dependent_required ({ credit_card : [ billing_address ]}) # it can also be done outside the class with # dependent_required({\"credit_card\": [\"billing_address\"]}, owner=Billing) assert deserialization_schema ( Billing ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"additionalProperties\" : False , \"dependentRequired\" : { \"credit_card\" : [ \"billing_address\" ]}, \"properties\" : { \"name\" : { \"type\" : \"string\" }, \"credit_card\" : { \"type\" : \"integer\" }, \"billing_address\" : { \"type\" : \"string\" }, }, \"required\" : [ \"name\" ], \"type\" : \"object\" , } with raises ( ValidationError ) as err : deserialize ( Billing , { \"name\" : \"Anonymous\" , \"credit_card\" : 1234_5678_9012_3456 }) assert serialize ( err . value ) == [ { \"loc\" : [ \"billing_address\" ], \"err\" : [ \"missing property (required by ['credit_card'])\" ], } ] Because bidirectional dependencies are a common idiom, apischema provides a shortcut notation; it's indeed possible to write dependent_required([credit_card, billing_adress]) .","title":"Property dependencies"},{"location":"json_schema/#json-schema-reference","text":"For complex schema with type reuse, it's convenient to extract definitions of schema components in order to reuse them \u2014 it's even mandatory for recursive types; JSON schema use JSON pointers \"$ref\" to refer to the definitions. apischema handles this feature natively. from dataclasses import dataclass from typing import Optional from apischema.json_schema import deserialization_schema @dataclass class Node : value : int child : Optional [ \"Node\" ] = None assert deserialization_schema ( Node ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"$ref\" : \"#/$defs/Node\" , \"$defs\" : { \"Node\" : { \"type\" : \"object\" , \"properties\" : { \"value\" : { \"type\" : \"integer\" }, \"child\" : { \"anyOf\" : [{ \"$ref\" : \"#/$defs/Node\" }, { \"type\" : \"null\" }], \"default\" : None , }, }, \"required\" : [ \"value\" ], \"additionalProperties\" : False , } }, }","title":"JSON schema reference"},{"location":"json_schema/#use-reference-only-for-reused-types","text":"apischema can control the reference use through the boolean all_ref parameter of deserialization_schema / serialization_schema : all_refs=True -> all types with a reference will be put in the definitions and referenced with $ref ; all_refs=False -> only types which are reused in the schema are put in definitions all_refs default value depends on the JSON schema version : it's False for JSON schema drafts but True for OpenAPI . from dataclasses import dataclass from apischema.json_schema import deserialization_schema @dataclass class Bar : baz : str @dataclass class Foo : bar1 : Bar bar2 : Bar assert deserialization_schema ( Foo , all_refs = False ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"$defs\" : { \"Bar\" : { \"additionalProperties\" : False , \"properties\" : { \"baz\" : { \"type\" : \"string\" }}, \"required\" : [ \"baz\" ], \"type\" : \"object\" , } }, \"additionalProperties\" : False , \"properties\" : { \"bar1\" : { \"$ref\" : \"#/$defs/Bar\" }, \"bar2\" : { \"$ref\" : \"#/$defs/Bar\" }}, \"required\" : [ \"bar1\" , \"bar2\" ], \"type\" : \"object\" , } assert deserialization_schema ( Foo , all_refs = True ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"$defs\" : { \"Bar\" : { \"additionalProperties\" : False , \"properties\" : { \"baz\" : { \"type\" : \"string\" }}, \"required\" : [ \"baz\" ], \"type\" : \"object\" , }, \"Foo\" : { \"additionalProperties\" : False , \"properties\" : { \"bar1\" : { \"$ref\" : \"#/$defs/Bar\" }, \"bar2\" : { \"$ref\" : \"#/$defs/Bar\" }, }, \"required\" : [ \"bar1\" , \"bar2\" ], \"type\" : \"object\" , }, }, \"$ref\" : \"#/$defs/Foo\" , }","title":"Use reference only for reused types"},{"location":"json_schema/#set-reference-name","text":"In the previous examples, types were referenced using their name. This is indeed the default behavior for every classes/ NewType s (except primitive int / str / bool / float ). It's possible to override the default reference name using apischema.type_name ; passing None instead of a string will remove the reference, making the type unable to be referenced as a separate definition in the schema. from dataclasses import dataclass from typing import Annotated from apischema import type_name from apischema.json_schema import deserialization_schema # Type name can be added as a decorator @type_name ( \"Resource\" ) @dataclass class BaseResource : id : int # or using typing.Annotated tags : Annotated [ set [ str ], type_name ( \"ResourceTags\" )] assert deserialization_schema ( BaseResource , all_refs = True ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"$defs\" : { \"Resource\" : { \"type\" : \"object\" , \"properties\" : { \"id\" : { \"type\" : \"integer\" }, \"tags\" : { \"$ref\" : \"#/$defs/ResourceTags\" }, }, \"required\" : [ \"id\" , \"tags\" ], \"additionalProperties\" : False , }, \"ResourceTags\" : { \"type\" : \"array\" , \"items\" : { \"type\" : \"string\" }, \"uniqueItems\" : True , }, }, \"$ref\" : \"#/$defs/Resource\" , } Note Builtin collections are interchangeable when a type_name is registered. For example, if a name is registered for list[Foo] , this name will also be used for Sequence[Foo] or Collection[Foo] . Generic aliases can have a type name, but they need to be specialized; Foo[T, int] cannot have a type name but Foo[str, int] can. However, generic classes can get a dynamic type name depending on their generic argument, passing a name factory to type_name : from dataclasses import dataclass , field from typing import Generic , TypeVar from apischema import type_name from apischema.json_schema import deserialization_schema from apischema.metadata import flattened T = TypeVar ( \"T\" ) # Type name factory takes the type and its arguments as (positional) parameters @type_name ( lambda tp , arg : f \" { arg . __name__ } Resource\" ) @dataclass class Resource ( Generic [ T ]): id : int content : T = field ( metadata = flattened ) ... @dataclass class Foo : bar : str assert deserialization_schema ( Resource [ Foo ], all_refs = True ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"$ref\" : \"#/$defs/FooResource\" , \"$defs\" : { \"FooResource\" : { \"type\" : \"object\" , \"allOf\" : [ { \"type\" : \"object\" , \"properties\" : { \"id\" : { \"type\" : \"integer\" }}, \"required\" : [ \"id\" ], \"additionalProperties\" : False , }, { \"$ref\" : \"#/$defs/Foo\" }, ], \"unevaluatedProperties\" : False , }, \"Foo\" : { \"type\" : \"object\" , \"properties\" : { \"bar\" : { \"type\" : \"string\" }}, \"required\" : [ \"bar\" ], \"additionalProperties\" : False , }, }, } The default behavior can also be customized using apischema.settings.default_type_name :","title":"Set reference name"},{"location":"json_schema/#reference-factory","text":"In JSON schema, $ref looks like #/$defs/Foo , not just Foo . In fact, schema generation use the ref given by type_name / default_type_name and pass it to a ref_factory function (a parameter of schema generation functions) which will convert it to its final form. JSON schema version comes with its default ref_factory , for draft 2019-09, it prefixes the ref with #/$defs/ , while it prefixes with #/components/schema in case of OpenAPI . from dataclasses import dataclass from apischema.json_schema import deserialization_schema @dataclass class Foo : bar : int def ref_factory ( ref : str ) -> str : return f \"http://some-domain.org/path/to/ { ref } .json#\" assert deserialization_schema ( Foo , all_refs = True , ref_factory = ref_factory ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"$ref\" : \"http://some-domain.org/path/to/Foo.json#\" , } Note When ref_factory is passed in arguments, definitions are not added to the generated schema. That's because ref_factory would surely change definitions location, so there would be no interest to add them with a wrong location. These definitions can of course be generated separately with definitions_schema .","title":"Reference factory"},{"location":"json_schema/#definitions-schema","text":"Definitions schemas can also be extracted using apischema.json_schema.definitions_schema . It takes two lists deserialization / serialization of types (or tuple of type + dynamic conversion ) and returns a dictionary of all referenced schemas. Note This is especially useful when it comes to OpenAPI schema to generate the components section. from dataclasses import dataclass from apischema.json_schema import definitions_schema @dataclass class Bar : baz : int = 0 @dataclass class Foo : bar : Bar assert definitions_schema ( deserialization = [ list [ Foo ]], all_refs = True ) == { \"Foo\" : { \"type\" : \"object\" , \"properties\" : { \"bar\" : { \"$ref\" : \"#/$defs/Bar\" }}, \"required\" : [ \"bar\" ], \"additionalProperties\" : False , }, \"Bar\" : { \"type\" : \"object\" , \"properties\" : { \"baz\" : { \"type\" : \"integer\" , \"default\" : 0 }}, \"additionalProperties\" : False , }, }","title":"Definitions schema"},{"location":"json_schema/#json-schema-openapi-version","text":"JSON schema has several versions \u2014 OpenAPI is treated as a JSON schema version. If apischema natively use the last one: draft 2019-09, it is possible to specify a schema version which will be used for the generation. from dataclasses import dataclass from typing import Literal , Optional from apischema.json_schema import ( JsonSchemaVersion , definitions_schema , deserialization_schema , ) @dataclass class Bar : baz : Optional [ int ] constant : Literal [ 0 ] = 0 @dataclass class Foo : bar : Bar assert deserialization_schema ( Foo , all_refs = True ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"$ref\" : \"#/$defs/Foo\" , \"$defs\" : { \"Foo\" : { \"type\" : \"object\" , \"properties\" : { \"bar\" : { \"$ref\" : \"#/$defs/Bar\" }}, \"required\" : [ \"bar\" ], \"additionalProperties\" : False , }, \"Bar\" : { \"type\" : \"object\" , \"properties\" : { \"baz\" : { \"type\" : [ \"integer\" , \"null\" ]}, \"constant\" : { \"type\" : \"integer\" , \"const\" : 0 , \"default\" : 0 }, }, \"required\" : [ \"baz\" ], \"additionalProperties\" : False , }, }, } assert deserialization_schema ( Foo , all_refs = True , version = JsonSchemaVersion . DRAFT_7 ) == { \"$schema\" : \"http://json-schema.org/draft-07/schema#\" , # $ref is isolated in allOf + draft 7 prefix \"allOf\" : [{ \"$ref\" : \"#/definitions/Foo\" }], \"definitions\" : { # not \"$defs\" \"Foo\" : { \"type\" : \"object\" , \"properties\" : { \"bar\" : { \"$ref\" : \"#/definitions/Bar\" }}, \"required\" : [ \"bar\" ], \"additionalProperties\" : False , }, \"Bar\" : { \"type\" : \"object\" , \"properties\" : { \"baz\" : { \"type\" : [ \"integer\" , \"null\" ]}, \"constant\" : { \"type\" : \"integer\" , \"const\" : 0 , \"default\" : 0 }, }, \"required\" : [ \"baz\" ], \"additionalProperties\" : False , }, }, } assert deserialization_schema ( Foo , version = JsonSchemaVersion . OPEN_API_3_0 ) == { # No definitions for OpenAPI, use definitions_schema for it \"$ref\" : \"#/components/schemas/Foo\" # OpenAPI prefix } assert definitions_schema ( deserialization = [ Foo ], version = JsonSchemaVersion . OPEN_API_3_0 ) == { \"Foo\" : { \"type\" : \"object\" , \"properties\" : { \"bar\" : { \"$ref\" : \"#/components/schemas/Bar\" }}, \"required\" : [ \"bar\" ], \"additionalProperties\" : False , }, \"Bar\" : { \"type\" : \"object\" , # \"nullable\" instead of \"type\": \"null\" \"properties\" : { \"baz\" : { \"type\" : \"integer\" , \"nullable\" : True }, \"constant\" : { \"type\" : \"integer\" , \"enum\" : [ 0 ], \"default\" : 0 }, }, \"required\" : [ \"baz\" ], \"additionalProperties\" : False , }, }","title":"JSON schema / OpenAPI version"},{"location":"json_schema/#readonly-writeonly","text":"Dataclasses InitVar and field(init=False) fields will be flagged respectively with \"writeOnly\": true and \"readOnly\": true in the generated schema. In definitions schema , if a type appears both in deserialization and serialization, properties are merged and the resulting schema contains then readOnly and writeOnly properties. By the way, the required is not merged because it can't (it would mess up validation if some not-init field was required), so deserialization required is kept because it's more important as it can be used in validation ( OpenAPI 3.0 semantic which allows the merge has been dropped in 3.1, so it has not been judged useful to be supported)","title":"readOnly / writeOnly"},{"location":"validation/","text":"Validation \u00b6 Validation is an important part of deserialization. By default, apischema validate types of data according to typing annotations, and schema constraints. But custom validators can also be add for a more precise validation. Deserialization and validation error \u00b6 ValidationError is raised when validation fails. This exception will contains all the information about the ill-formed part of the data. By the way this exception is also serializable and can be sent back directly to client. from dataclasses import dataclass , field from typing import NewType from pytest import raises from apischema import ValidationError , deserialize , schema , serialize Tag = NewType ( \"Tag\" , str ) schema ( min_len = 3 , pattern = r \"^\\w*$\" , examples = [ \"available\" , \"EMEA\" ])( Tag ) @dataclass class Resource : id : int tags : list [ Tag ] = field ( default_factory = list , metadata = schema ( description = \"regroup multiple resources\" , max_items = 3 , unique = True ), ) with raises ( ValidationError ) as err : # pytest check exception is raised deserialize ( Resource , { \"id\" : 42 , \"tags\" : [ \"tag\" , \"duplicate\" , \"duplicate\" , \"bad&\" , \"_\" ]} ) assert serialize ( err . value ) == [ { \"loc\" : [ \"tags\" ], \"err\" : [ \"item count greater than 3 (maxItems)\" , \"duplicate items (uniqueItems)\" , ], }, { \"loc\" : [ \"tags\" , 3 ], \"err\" : [ \"not matching '^ \\\\ w*$' (pattern)\" ]}, { \"loc\" : [ \"tags\" , 4 ], \"err\" : [ \"string length lower than 3 (minLength)\" ]}, ] As shown in the example, apischema will not stop at the first error met but tries to validate all parts of the data. Note ValidationError should be serialized using the same serializer that the one used for deserialization, because it can contains some unaliased field path Dataclass validators \u00b6 Dataclass validation can be completed by custom validators. These are simple decorated methods which will be executed during validation, after all fields having been deserialized. from dataclasses import dataclass from pytest import raises from apischema import ValidationError , deserialize , serialize , validator @dataclass class PasswordForm : password : str confirmation : str @validator def password_match ( self ): # DO NOT use assert if self . password != self . confirmation : raise ValueError ( \"password doesn't match its confirmation\" ) with raises ( ValidationError ) as err : deserialize ( PasswordForm , { \"password\" : \"p455w0rd\" , \"confirmation\" : \"...\" }) assert serialize ( err . value ) == [ { \"loc\" : [], \"err\" : [ \"password doesn't match its confirmation\" ]} ] Warning DO NOT use assert statement to validate external data, never. In fact, this statement is made to be disabled when executed in optimized mode (see documentation ), so validation would be disabled too. This warning doesn't concern only apischema ; assert is only for internal assertion in debug/development environment. That's why apischema will not catch AssertionError as a validation error but reraises it, making deserialize fail. Note Validators are always executed in order of declaration. Automatic dependency management \u00b6 It makes no sense to execute a validator using a field that is ill-formed. Hopefully, apischema is able to compute validator dependencies \u2014 the fields used in validator; validator is executed only if the all its dependencies are ok. from dataclasses import dataclass from pytest import raises from apischema import ValidationError , deserialize , serialize , validator @dataclass class PasswordForm : password : str confirmation : str @validator def password_match ( self ): if self . password != self . confirmation : raise ValueError ( \"password doesn't match its confirmation\" ) with raises ( ValidationError ) as err : deserialize ( PasswordForm , { \"password\" : \"p455w0rd\" }) assert serialize ( err . value ) == [ # validator is not executed because confirmation is missing { \"loc\" : [ \"confirmation\" ], \"err\" : [ \"missing property\" ]} ] Note Despite the fact that validator use self argument, it can be called during validation even if all the fields of the class are not ok and the class not really instantiated. In fact, instance is kind of mocked for validation with only the needed field. Raise more than one error with yield \u00b6 Validation of list field can require to raise several exception, one for each bad elements. With raise , this is not possible, because you can raise only once. However, apischema provides a way or raising as many errors as needed by using yield . Moreover, with this syntax, it is possible to add a \"path\" (see below ) to the error to precise its location in the validated data. This path will be added to the loc key of the error. from dataclasses import dataclass from ipaddress import IPv4Address , IPv4Network from pytest import raises from apischema import ValidationError , deserialize , serialize , validator from apischema.objects import get_alias @dataclass class SubnetIps : subnet : IPv4Network ips : list [ IPv4Address ] @validator def check_ips_in_subnet ( self ): for index , ip in enumerate ( self . ips ): if ip not in self . subnet : # yield <error path>, <error message> yield ( get_alias ( self ) . ips , index ), \"ip not in subnet\" with raises ( ValidationError ) as err : deserialize ( SubnetIps , { \"subnet\" : \"126.42.18.0/24\" , \"ips\" : [ \"126.42.18.1\" , \"126.42.19.0\" , \"0.0.0.0\" ]}, ) assert serialize ( err . value ) == [ { \"loc\" : [ \"ips\" , 1 ], \"err\" : [ \"ip not in subnet\" ]}, { \"loc\" : [ \"ips\" , 2 ], \"err\" : [ \"ip not in subnet\" ]}, ] Error path \u00b6 In the example, validator yield a tuple of an \"error path\" and the error message. Error path can be: a field alias (obtained with apischema.objects.get_alias ); an integer, for list indices; a raw string, for dict key (or field); an apischema.objects.AliasedStr , a string subclass which will be aliased by serialization aliaser; an iterable, e.g. a tuple, of this 4 components. yield can also be used with only an error message. Note For dataclass field error path, it's advised to use apischema.objects.get_alias instead of raw string, because it will take in account potential aliasing and it will be better handled by IDE (refactoring, cross-referencing, etc.) Discard \u00b6 If one of your validators fails because a field is corrupted, maybe you don't want following validators to be executed. validator decorator provides a discard parameter to discard fields of the remaining validation. All the remaining validators having discarded fields in dependencies will not be executed. from dataclasses import dataclass , field from pytest import raises from apischema import ValidationError , deserialize , serialize , validator from apischema.objects import get_alias , get_field @dataclass class BoundedValues : # field must be assign to be used, even with empty `field()` bounds : tuple [ int , int ] = field () values : list [ int ] # validator(\"bounds\") would also work, but it's not handled by IDE refactoring, etc. @validator ( discard = bounds ) def bounds_are_sorted ( self ): min_bound , max_bound = self . bounds if min_bound > max_bound : yield get_alias ( self ) . bounds , \"bounds are not sorted\" @validator def values_dont_exceed_bounds ( self ): min_bound , max_bound = self . bounds for index , value in enumerate ( self . values ): if not min_bound <= value <= max_bound : yield ( get_alias ( self ) . values , index ), \"value exceeds bounds\" # Outside class, fields can still be accessed in a \"static\" way, to avoid use raw string @validator ( discard = get_field ( BoundedValues ) . bounds ) def bounds_are_sorted_equivalent ( bounded : BoundedValues ): min_bound , max_bound = bounded . bounds if min_bound > max_bound : yield get_alias ( bounded ) . bounds , \"bounds are not sorted\" with raises ( ValidationError ) as err : deserialize ( BoundedValues , { \"bounds\" : [ 10 , 0 ], \"values\" : [ - 1 , 2 , 4 ]}) assert serialize ( err . value ) == [ { \"loc\" : [ \"bounds\" ], \"err\" : [ \"bounds are not sorted\" ]} # Without discard, there would have been an other error # {\"loc\": [\"values\", 1], \"err\": [\"value exceeds bounds\"]} ] You can notice in this example that apischema tries avoiding using raw strings to identify fields. In every function of the library using fields identifier ( apischema.validator , apischema.dependent_required , apischema.fields.set_fields , etc.), you have always three ways to pass them: - using field object, preferred in dataclass definition; - using apischema.objects.get_field , to be used outside of class definition; it works with NamedTuple too \u2014 the object returned is the apischema internal field representation, common to dataclass , NamedTuple and TypedDict ; - using raw strings, thus not handled by static tools like refactoring, but it works; Field validators \u00b6 At field level \u00b6 Fields are validated according to their types and schema. But it's also possible to add validators to fields from dataclasses import dataclass , field from pytest import raises from apischema import ValidationError , deserialize , serialize from apischema.metadata import validators def check_no_duplicate_digits ( n : int ): if len ( str ( n )) != len ( set ( str ( n ))): raise ValueError ( \"number has duplicate digits\" ) @dataclass class Foo : bar : str = field ( metadata = validators ( check_no_duplicate_digits )) with raises ( ValidationError ) as err : deserialize ( Foo , { \"bar\" : \"11\" }) assert serialize ( err . value ) == [ { \"loc\" : [ \"bar\" ], \"err\" : [ \"number has duplicate digits\" ]} ] When validation fails for a field, it is discarded and cannot be used in class validators, as it is the case when field schema validation fails. Note field_validator allow to reuse the the same validator for several fields. However in this case, using a custom type (for example a NewType ) with validators (see next section ) could often be a better solution. Using other fields \u00b6 A common pattern can be to validate a field using other fields values. This is achieved with dataclass validators seen above. However, there is a shortcut for this use case: from dataclasses import dataclass , field from enum import Enum from pytest import raises from apischema import ValidationError , deserialize , serialize , validator from apischema.objects import get_alias , get_field class Parity ( Enum ): EVEN = \"even\" ODD = \"odd\" @dataclass class NumberWithParity : parity : Parity number : int = field () @validator ( number ) def check_parity ( self ): if ( self . parity == Parity . EVEN ) != ( self . number % 2 == 0 ): yield \"number doesn't respect parity\" # A field validator is equivalent to a discard argument and all error paths prefixed # with the field alias @validator ( discard = number ) def check_parity_equivalent ( self ): if ( self . parity == Parity . EVEN ) != ( self . number % 2 == 0 ): yield get_alias ( self ) . number , \"number doesn't respect parity\" @validator ( get_field ( NumberWithParity ) . number ) def check_parity_other_equivalent ( number2 : NumberWithParity ): if ( number2 . parity == Parity . EVEN ) != ( number2 . number % 2 == 0 ): yield \"number doesn't respect parity\" with raises ( ValidationError ) as err : deserialize ( NumberWithParity , { \"parity\" : \"even\" , \"number\" : 1 }) assert serialize ( err . value ) == [ { \"loc\" : [ \"number\" ], \"err\" : [ \"number doesn't respect parity\" ]} ] Validators inheritance \u00b6 Validators are inherited just like other class fields. from dataclasses import dataclass from pytest import raises from apischema import ValidationError , deserialize , serialize , validator @dataclass class PasswordForm : password : str confirmation : str @validator def password_match ( self ): if self . password != self . confirmation : raise ValueError ( \"password doesn't match its confirmation\" ) @dataclass class CompleteForm ( PasswordForm ): username : str with raises ( ValidationError ) as err : deserialize ( CompleteForm , { \"username\" : \"wyfo\" , \"password\" : \"p455w0rd\" , \"confirmation\" : \"...\" }, ) assert serialize ( err . value ) == [ { \"loc\" : [], \"err\" : [ \"password doesn't match its confirmation\" ]} ] Validator with InitVar \u00b6 Dataclasses InitVar are accessible in validators by using parameters the same way __post_init__ does. Only the needed fields has to be put in parameters, they are then added to validator dependencies. from dataclasses import InitVar , dataclass , field from pytest import raises from apischema import ValidationError , deserialize , serialize , validator from apischema.metadata import init_var @dataclass class Foo : bar : InitVar [ int ] = field ( metadata = init_var ( int )) @validator ( bar ) def validate ( self , bar : int ): if bar < 0 : yield \"negative\" with raises ( ValidationError ) as err : deserialize ( Foo , { \"bar\" : - 1 }) assert serialize ( err . value ) == [{ \"loc\" : [ \"bar\" ], \"err\" : [ \"negative\" ]}] Validators are not run on default values \u00b6 If all validator dependencies are initialized with their default values, they are not run. from dataclasses import dataclass , field from apischema import deserialize , validator validator_run = False @dataclass class Foo : bar : int = field ( default = 0 ) @validator ( bar ) def password_match ( self ): global validator_run validator_run = True if self . bar < 0 : raise ValueError ( \"negative\" ) deserialize ( Foo , {}) assert not validator_run Validators for every type \u00b6 Validators can also be declared as regular function, in which case annotation of the first param is used to associate it to the validated type (you can also use the owner parameter); this allows to add validator to every type. Last but not least, validators can be embedded directly into Annotated arguments using validators metadata. from typing import Annotated , NewType from pytest import raises from apischema import ValidationError , deserialize , serialize , validator from apischema.metadata import validators Palindrome = NewType ( \"Palindrome\" , str ) @validator # could also use @validator(owner=Palindrome) def check_palindrome ( s : Palindrome ): for i in range ( len ( s ) // 2 ): if s [ i ] != s [ - 1 - i ]: raise ValueError ( \"Not a palindrome\" ) assert deserialize ( Palindrome , \"tacocat\" ) == \"tacocat\" with raises ( ValidationError ) as err : deserialize ( Palindrome , \"palindrome\" ) assert serialize ( err . value ) == [{ \"loc\" : [], \"err\" : [ \"Not a palindrome\" ]}] # Using Annotated with raises ( ValidationError ) as err : deserialize ( Annotated [ str , validators ( check_palindrome )], \"palindrom\" ) assert serialize ( err . value ) == [{ \"loc\" : [], \"err\" : [ \"Not a palindrome\" ]}] FAQ \u00b6 How are computed validator dependencies? \u00b6 ast.NodeVisitor and the Python black magic begins... Why only validate at deserialization and not at instantiation? \u00b6 apischema uses type annotations, so every objects used can already be statically type-checked (with Mypy / Pycharm /etc.) at instantiation but also at modification. Why use validators for dataclasses instead of doing validation in __post_init__ ? \u00b6 Actually, validation can completely be done in __post_init__ , there is no problem with that. However, validators offers one thing that cannot be achieved with __post_init__ : they are run before __init__ , so they can validate incomplete data. Moreover, they are only run during deserialization, so they don't add overhead to normal class instantiation.","title":"Validation"},{"location":"validation/#validation","text":"Validation is an important part of deserialization. By default, apischema validate types of data according to typing annotations, and schema constraints. But custom validators can also be add for a more precise validation.","title":"Validation"},{"location":"validation/#deserialization-and-validation-error","text":"ValidationError is raised when validation fails. This exception will contains all the information about the ill-formed part of the data. By the way this exception is also serializable and can be sent back directly to client. from dataclasses import dataclass , field from typing import NewType from pytest import raises from apischema import ValidationError , deserialize , schema , serialize Tag = NewType ( \"Tag\" , str ) schema ( min_len = 3 , pattern = r \"^\\w*$\" , examples = [ \"available\" , \"EMEA\" ])( Tag ) @dataclass class Resource : id : int tags : list [ Tag ] = field ( default_factory = list , metadata = schema ( description = \"regroup multiple resources\" , max_items = 3 , unique = True ), ) with raises ( ValidationError ) as err : # pytest check exception is raised deserialize ( Resource , { \"id\" : 42 , \"tags\" : [ \"tag\" , \"duplicate\" , \"duplicate\" , \"bad&\" , \"_\" ]} ) assert serialize ( err . value ) == [ { \"loc\" : [ \"tags\" ], \"err\" : [ \"item count greater than 3 (maxItems)\" , \"duplicate items (uniqueItems)\" , ], }, { \"loc\" : [ \"tags\" , 3 ], \"err\" : [ \"not matching '^ \\\\ w*$' (pattern)\" ]}, { \"loc\" : [ \"tags\" , 4 ], \"err\" : [ \"string length lower than 3 (minLength)\" ]}, ] As shown in the example, apischema will not stop at the first error met but tries to validate all parts of the data. Note ValidationError should be serialized using the same serializer that the one used for deserialization, because it can contains some unaliased field path","title":"Deserialization and validation error"},{"location":"validation/#dataclass-validators","text":"Dataclass validation can be completed by custom validators. These are simple decorated methods which will be executed during validation, after all fields having been deserialized. from dataclasses import dataclass from pytest import raises from apischema import ValidationError , deserialize , serialize , validator @dataclass class PasswordForm : password : str confirmation : str @validator def password_match ( self ): # DO NOT use assert if self . password != self . confirmation : raise ValueError ( \"password doesn't match its confirmation\" ) with raises ( ValidationError ) as err : deserialize ( PasswordForm , { \"password\" : \"p455w0rd\" , \"confirmation\" : \"...\" }) assert serialize ( err . value ) == [ { \"loc\" : [], \"err\" : [ \"password doesn't match its confirmation\" ]} ] Warning DO NOT use assert statement to validate external data, never. In fact, this statement is made to be disabled when executed in optimized mode (see documentation ), so validation would be disabled too. This warning doesn't concern only apischema ; assert is only for internal assertion in debug/development environment. That's why apischema will not catch AssertionError as a validation error but reraises it, making deserialize fail. Note Validators are always executed in order of declaration.","title":"Dataclass validators"},{"location":"validation/#automatic-dependency-management","text":"It makes no sense to execute a validator using a field that is ill-formed. Hopefully, apischema is able to compute validator dependencies \u2014 the fields used in validator; validator is executed only if the all its dependencies are ok. from dataclasses import dataclass from pytest import raises from apischema import ValidationError , deserialize , serialize , validator @dataclass class PasswordForm : password : str confirmation : str @validator def password_match ( self ): if self . password != self . confirmation : raise ValueError ( \"password doesn't match its confirmation\" ) with raises ( ValidationError ) as err : deserialize ( PasswordForm , { \"password\" : \"p455w0rd\" }) assert serialize ( err . value ) == [ # validator is not executed because confirmation is missing { \"loc\" : [ \"confirmation\" ], \"err\" : [ \"missing property\" ]} ] Note Despite the fact that validator use self argument, it can be called during validation even if all the fields of the class are not ok and the class not really instantiated. In fact, instance is kind of mocked for validation with only the needed field.","title":"Automatic dependency management"},{"location":"validation/#raise-more-than-one-error-with-yield","text":"Validation of list field can require to raise several exception, one for each bad elements. With raise , this is not possible, because you can raise only once. However, apischema provides a way or raising as many errors as needed by using yield . Moreover, with this syntax, it is possible to add a \"path\" (see below ) to the error to precise its location in the validated data. This path will be added to the loc key of the error. from dataclasses import dataclass from ipaddress import IPv4Address , IPv4Network from pytest import raises from apischema import ValidationError , deserialize , serialize , validator from apischema.objects import get_alias @dataclass class SubnetIps : subnet : IPv4Network ips : list [ IPv4Address ] @validator def check_ips_in_subnet ( self ): for index , ip in enumerate ( self . ips ): if ip not in self . subnet : # yield <error path>, <error message> yield ( get_alias ( self ) . ips , index ), \"ip not in subnet\" with raises ( ValidationError ) as err : deserialize ( SubnetIps , { \"subnet\" : \"126.42.18.0/24\" , \"ips\" : [ \"126.42.18.1\" , \"126.42.19.0\" , \"0.0.0.0\" ]}, ) assert serialize ( err . value ) == [ { \"loc\" : [ \"ips\" , 1 ], \"err\" : [ \"ip not in subnet\" ]}, { \"loc\" : [ \"ips\" , 2 ], \"err\" : [ \"ip not in subnet\" ]}, ]","title":"Raise more than one error with yield"},{"location":"validation/#error-path","text":"In the example, validator yield a tuple of an \"error path\" and the error message. Error path can be: a field alias (obtained with apischema.objects.get_alias ); an integer, for list indices; a raw string, for dict key (or field); an apischema.objects.AliasedStr , a string subclass which will be aliased by serialization aliaser; an iterable, e.g. a tuple, of this 4 components. yield can also be used with only an error message. Note For dataclass field error path, it's advised to use apischema.objects.get_alias instead of raw string, because it will take in account potential aliasing and it will be better handled by IDE (refactoring, cross-referencing, etc.)","title":"Error path"},{"location":"validation/#discard","text":"If one of your validators fails because a field is corrupted, maybe you don't want following validators to be executed. validator decorator provides a discard parameter to discard fields of the remaining validation. All the remaining validators having discarded fields in dependencies will not be executed. from dataclasses import dataclass , field from pytest import raises from apischema import ValidationError , deserialize , serialize , validator from apischema.objects import get_alias , get_field @dataclass class BoundedValues : # field must be assign to be used, even with empty `field()` bounds : tuple [ int , int ] = field () values : list [ int ] # validator(\"bounds\") would also work, but it's not handled by IDE refactoring, etc. @validator ( discard = bounds ) def bounds_are_sorted ( self ): min_bound , max_bound = self . bounds if min_bound > max_bound : yield get_alias ( self ) . bounds , \"bounds are not sorted\" @validator def values_dont_exceed_bounds ( self ): min_bound , max_bound = self . bounds for index , value in enumerate ( self . values ): if not min_bound <= value <= max_bound : yield ( get_alias ( self ) . values , index ), \"value exceeds bounds\" # Outside class, fields can still be accessed in a \"static\" way, to avoid use raw string @validator ( discard = get_field ( BoundedValues ) . bounds ) def bounds_are_sorted_equivalent ( bounded : BoundedValues ): min_bound , max_bound = bounded . bounds if min_bound > max_bound : yield get_alias ( bounded ) . bounds , \"bounds are not sorted\" with raises ( ValidationError ) as err : deserialize ( BoundedValues , { \"bounds\" : [ 10 , 0 ], \"values\" : [ - 1 , 2 , 4 ]}) assert serialize ( err . value ) == [ { \"loc\" : [ \"bounds\" ], \"err\" : [ \"bounds are not sorted\" ]} # Without discard, there would have been an other error # {\"loc\": [\"values\", 1], \"err\": [\"value exceeds bounds\"]} ] You can notice in this example that apischema tries avoiding using raw strings to identify fields. In every function of the library using fields identifier ( apischema.validator , apischema.dependent_required , apischema.fields.set_fields , etc.), you have always three ways to pass them: - using field object, preferred in dataclass definition; - using apischema.objects.get_field , to be used outside of class definition; it works with NamedTuple too \u2014 the object returned is the apischema internal field representation, common to dataclass , NamedTuple and TypedDict ; - using raw strings, thus not handled by static tools like refactoring, but it works;","title":"Discard"},{"location":"validation/#field-validators","text":"","title":"Field validators"},{"location":"validation/#at-field-level","text":"Fields are validated according to their types and schema. But it's also possible to add validators to fields from dataclasses import dataclass , field from pytest import raises from apischema import ValidationError , deserialize , serialize from apischema.metadata import validators def check_no_duplicate_digits ( n : int ): if len ( str ( n )) != len ( set ( str ( n ))): raise ValueError ( \"number has duplicate digits\" ) @dataclass class Foo : bar : str = field ( metadata = validators ( check_no_duplicate_digits )) with raises ( ValidationError ) as err : deserialize ( Foo , { \"bar\" : \"11\" }) assert serialize ( err . value ) == [ { \"loc\" : [ \"bar\" ], \"err\" : [ \"number has duplicate digits\" ]} ] When validation fails for a field, it is discarded and cannot be used in class validators, as it is the case when field schema validation fails. Note field_validator allow to reuse the the same validator for several fields. However in this case, using a custom type (for example a NewType ) with validators (see next section ) could often be a better solution.","title":"At field level"},{"location":"validation/#using-other-fields","text":"A common pattern can be to validate a field using other fields values. This is achieved with dataclass validators seen above. However, there is a shortcut for this use case: from dataclasses import dataclass , field from enum import Enum from pytest import raises from apischema import ValidationError , deserialize , serialize , validator from apischema.objects import get_alias , get_field class Parity ( Enum ): EVEN = \"even\" ODD = \"odd\" @dataclass class NumberWithParity : parity : Parity number : int = field () @validator ( number ) def check_parity ( self ): if ( self . parity == Parity . EVEN ) != ( self . number % 2 == 0 ): yield \"number doesn't respect parity\" # A field validator is equivalent to a discard argument and all error paths prefixed # with the field alias @validator ( discard = number ) def check_parity_equivalent ( self ): if ( self . parity == Parity . EVEN ) != ( self . number % 2 == 0 ): yield get_alias ( self ) . number , \"number doesn't respect parity\" @validator ( get_field ( NumberWithParity ) . number ) def check_parity_other_equivalent ( number2 : NumberWithParity ): if ( number2 . parity == Parity . EVEN ) != ( number2 . number % 2 == 0 ): yield \"number doesn't respect parity\" with raises ( ValidationError ) as err : deserialize ( NumberWithParity , { \"parity\" : \"even\" , \"number\" : 1 }) assert serialize ( err . value ) == [ { \"loc\" : [ \"number\" ], \"err\" : [ \"number doesn't respect parity\" ]} ]","title":"Using other fields"},{"location":"validation/#validators-inheritance","text":"Validators are inherited just like other class fields. from dataclasses import dataclass from pytest import raises from apischema import ValidationError , deserialize , serialize , validator @dataclass class PasswordForm : password : str confirmation : str @validator def password_match ( self ): if self . password != self . confirmation : raise ValueError ( \"password doesn't match its confirmation\" ) @dataclass class CompleteForm ( PasswordForm ): username : str with raises ( ValidationError ) as err : deserialize ( CompleteForm , { \"username\" : \"wyfo\" , \"password\" : \"p455w0rd\" , \"confirmation\" : \"...\" }, ) assert serialize ( err . value ) == [ { \"loc\" : [], \"err\" : [ \"password doesn't match its confirmation\" ]} ]","title":"Validators inheritance"},{"location":"validation/#validator-with-initvar","text":"Dataclasses InitVar are accessible in validators by using parameters the same way __post_init__ does. Only the needed fields has to be put in parameters, they are then added to validator dependencies. from dataclasses import InitVar , dataclass , field from pytest import raises from apischema import ValidationError , deserialize , serialize , validator from apischema.metadata import init_var @dataclass class Foo : bar : InitVar [ int ] = field ( metadata = init_var ( int )) @validator ( bar ) def validate ( self , bar : int ): if bar < 0 : yield \"negative\" with raises ( ValidationError ) as err : deserialize ( Foo , { \"bar\" : - 1 }) assert serialize ( err . value ) == [{ \"loc\" : [ \"bar\" ], \"err\" : [ \"negative\" ]}]","title":"Validator with InitVar"},{"location":"validation/#validators-are-not-run-on-default-values","text":"If all validator dependencies are initialized with their default values, they are not run. from dataclasses import dataclass , field from apischema import deserialize , validator validator_run = False @dataclass class Foo : bar : int = field ( default = 0 ) @validator ( bar ) def password_match ( self ): global validator_run validator_run = True if self . bar < 0 : raise ValueError ( \"negative\" ) deserialize ( Foo , {}) assert not validator_run","title":"Validators are not run on default values"},{"location":"validation/#validators-for-every-type","text":"Validators can also be declared as regular function, in which case annotation of the first param is used to associate it to the validated type (you can also use the owner parameter); this allows to add validator to every type. Last but not least, validators can be embedded directly into Annotated arguments using validators metadata. from typing import Annotated , NewType from pytest import raises from apischema import ValidationError , deserialize , serialize , validator from apischema.metadata import validators Palindrome = NewType ( \"Palindrome\" , str ) @validator # could also use @validator(owner=Palindrome) def check_palindrome ( s : Palindrome ): for i in range ( len ( s ) // 2 ): if s [ i ] != s [ - 1 - i ]: raise ValueError ( \"Not a palindrome\" ) assert deserialize ( Palindrome , \"tacocat\" ) == \"tacocat\" with raises ( ValidationError ) as err : deserialize ( Palindrome , \"palindrome\" ) assert serialize ( err . value ) == [{ \"loc\" : [], \"err\" : [ \"Not a palindrome\" ]}] # Using Annotated with raises ( ValidationError ) as err : deserialize ( Annotated [ str , validators ( check_palindrome )], \"palindrom\" ) assert serialize ( err . value ) == [{ \"loc\" : [], \"err\" : [ \"Not a palindrome\" ]}]","title":"Validators for every type"},{"location":"validation/#faq","text":"","title":"FAQ"},{"location":"validation/#how-are-computed-validator-dependencies","text":"ast.NodeVisitor and the Python black magic begins...","title":"How are computed validator dependencies?"},{"location":"validation/#why-only-validate-at-deserialization-and-not-at-instantiation","text":"apischema uses type annotations, so every objects used can already be statically type-checked (with Mypy / Pycharm /etc.) at instantiation but also at modification.","title":"Why only validate at deserialization and not at instantiation?"},{"location":"validation/#why-use-validators-for-dataclasses-instead-of-doing-validation-in-__post_init__","text":"Actually, validation can completely be done in __post_init__ , there is no problem with that. However, validators offers one thing that cannot be achieved with __post_init__ : they are run before __init__ , so they can validate incomplete data. Moreover, they are only run during deserialization, so they don't add overhead to normal class instantiation.","title":"Why use validators for dataclasses instead of doing validation in __post_init__?"},{"location":"examples/attrs_support/","text":"Attrs support \u00b6 from typing import Optional , Sequence import attr from apischema import deserialize , serialize , settings from apischema.json_schema import deserialization_schema from apischema.objects import ObjectField prev_default_object_fields = settings . default_object_fields def attrs_fields ( cls : type ) -> Optional [ Sequence [ ObjectField ]]: if hasattr ( cls , \"__attrs_attrs__\" ): return [ ObjectField ( a . name , a . type , required = a . default == attr . NOTHING , default = a . default ) for a in getattr ( cls , \"__attrs_attrs__\" ) ] else : return prev_default_object_fields ( cls ) settings . default_object_fields = attrs_fields @attr . s class Foo : bar : int = attr . ib () assert deserialize ( Foo , { \"bar\" : 0 }) == Foo ( 0 ) assert serialize ( Foo , Foo ( 0 )) == { \"bar\" : 0 } assert deserialization_schema ( Foo ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"type\" : \"object\" , \"properties\" : { \"bar\" : { \"type\" : \"integer\" }}, \"required\" : [ \"bar\" ], \"additionalProperties\" : False , }","title":"Attrs support"},{"location":"examples/attrs_support/#attrs-support","text":"from typing import Optional , Sequence import attr from apischema import deserialize , serialize , settings from apischema.json_schema import deserialization_schema from apischema.objects import ObjectField prev_default_object_fields = settings . default_object_fields def attrs_fields ( cls : type ) -> Optional [ Sequence [ ObjectField ]]: if hasattr ( cls , \"__attrs_attrs__\" ): return [ ObjectField ( a . name , a . type , required = a . default == attr . NOTHING , default = a . default ) for a in getattr ( cls , \"__attrs_attrs__\" ) ] else : return prev_default_object_fields ( cls ) settings . default_object_fields = attrs_fields @attr . s class Foo : bar : int = attr . ib () assert deserialize ( Foo , { \"bar\" : 0 }) == Foo ( 0 ) assert serialize ( Foo , Foo ( 0 )) == { \"bar\" : 0 } assert deserialization_schema ( Foo ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"type\" : \"object\" , \"properties\" : { \"bar\" : { \"type\" : \"integer\" }}, \"required\" : [ \"bar\" ], \"additionalProperties\" : False , }","title":"Attrs support"},{"location":"examples/inherited_deserializer/","text":"Inherited deserializer \u00b6 from collections.abc import Iterator from dataclasses import dataclass from typing import TypeVar from apischema import deserialize , deserializer from apischema.conversions import Conversion Foo_ = TypeVar ( \"Foo_\" , bound = \"Foo\" ) # Use a dataclass in order to be easily testable with == @dataclass class Foo : value : int @classmethod def deserialize ( cls : type [ Foo_ ], value : int ) -> Foo_ : return cls ( value ) def __init_subclass__ ( cls , ** kwargs ): super () . __init_subclass__ ( ** kwargs ) # Register subclasses' conversion in __init_subclass__ deserializer ( Conversion ( cls . deserialize , target = cls )) # Register main conversion after the class definition deserializer ( Conversion ( Foo . deserialize , target = Foo )) class Bar ( Foo ): pass assert deserialize ( Foo , 0 ) == Foo ( 0 ) assert deserialize ( Bar , 0 ) == Bar ( 0 ) # For external types (defines in imported library) @dataclass class ForeignType : value : int class ForeignSubtype ( ForeignType ): pass T = TypeVar ( \"T\" ) # Recursive implementation of type.__subclasses__ def rec_subclasses ( cls : type [ T ]) -> Iterator [ type [ T ]]: for sub_cls in cls . __subclasses__ (): yield sub_cls yield from rec_subclasses ( sub_cls ) # Register deserializers for all subclasses for cls in ( ForeignType , * rec_subclasses ( ForeignType )): # cls=cls is an lambda idiom to capture variable by value inside loop deserializer ( Conversion ( lambda value , cls = cls : cls ( value ), source = int , target = cls )) assert deserialize ( ForeignType , 0 ) == ForeignType ( 0 ) assert deserialize ( ForeignSubtype , 0 ) == ForeignSubtype ( 0 )","title":"Inherited deserializer"},{"location":"examples/inherited_deserializer/#inherited-deserializer","text":"from collections.abc import Iterator from dataclasses import dataclass from typing import TypeVar from apischema import deserialize , deserializer from apischema.conversions import Conversion Foo_ = TypeVar ( \"Foo_\" , bound = \"Foo\" ) # Use a dataclass in order to be easily testable with == @dataclass class Foo : value : int @classmethod def deserialize ( cls : type [ Foo_ ], value : int ) -> Foo_ : return cls ( value ) def __init_subclass__ ( cls , ** kwargs ): super () . __init_subclass__ ( ** kwargs ) # Register subclasses' conversion in __init_subclass__ deserializer ( Conversion ( cls . deserialize , target = cls )) # Register main conversion after the class definition deserializer ( Conversion ( Foo . deserialize , target = Foo )) class Bar ( Foo ): pass assert deserialize ( Foo , 0 ) == Foo ( 0 ) assert deserialize ( Bar , 0 ) == Bar ( 0 ) # For external types (defines in imported library) @dataclass class ForeignType : value : int class ForeignSubtype ( ForeignType ): pass T = TypeVar ( \"T\" ) # Recursive implementation of type.__subclasses__ def rec_subclasses ( cls : type [ T ]) -> Iterator [ type [ T ]]: for sub_cls in cls . __subclasses__ (): yield sub_cls yield from rec_subclasses ( sub_cls ) # Register deserializers for all subclasses for cls in ( ForeignType , * rec_subclasses ( ForeignType )): # cls=cls is an lambda idiom to capture variable by value inside loop deserializer ( Conversion ( lambda value , cls = cls : cls ( value ), source = int , target = cls )) assert deserialize ( ForeignType , 0 ) == ForeignType ( 0 ) assert deserialize ( ForeignSubtype , 0 ) == ForeignSubtype ( 0 )","title":"Inherited deserializer"},{"location":"examples/open_rpc/","text":"OpenRPC \u00b6 from dataclasses import dataclass from typing import Generic , TypeVar , Union from pytest import raises from apischema import ( Undefined , UndefinedType , ValidationError , deserialize , schema , serialize , ) from apischema.json_schema import deserialization_schema T = TypeVar ( \"T\" ) @dataclass class Error ( Exception , Generic [ T ]): code : int description : str data : Union [ T , UndefinedType ] = Undefined @schema ( min_props = 1 , max_props = 1 ) @dataclass class Result ( Generic [ T ]): result : Union [ T , UndefinedType ] = Undefined error : Union [ Error , UndefinedType ] = Undefined def get ( self ) -> T : if self . error is not Undefined : raise self . error else : assert self . result is not Undefined return self . result assert deserialization_schema ( Result [ list [ int ]]) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"additionalProperties\" : False , \"maxProperties\" : 1 , \"minProperties\" : 1 , \"properties\" : { \"result\" : { \"type\" : \"array\" , \"items\" : { \"type\" : \"integer\" }}, \"error\" : { \"additionalProperties\" : False , \"properties\" : { \"code\" : { \"type\" : \"integer\" }, \"description\" : { \"type\" : \"string\" }, \"data\" : {}, }, \"required\" : [ \"code\" , \"description\" ], \"type\" : \"object\" , }, }, \"type\" : \"object\" , } data = { \"result\" : 0 } with raises ( ValidationError ): deserialize ( Result [ str ], data ) result = deserialize ( Result [ int ], data ) assert result == Result ( 0 ) assert result . get () == 0 assert serialize ( Result [ int ], result ) == { \"result\" : 0 } error = deserialize ( Result , { \"error\" : { \"code\" : 42 , \"description\" : \"...\" }}) with raises ( Error ) as err : error . get () assert err . value == Error ( 42 , \"...\" )","title":"OpenRPC"},{"location":"examples/open_rpc/#openrpc","text":"from dataclasses import dataclass from typing import Generic , TypeVar , Union from pytest import raises from apischema import ( Undefined , UndefinedType , ValidationError , deserialize , schema , serialize , ) from apischema.json_schema import deserialization_schema T = TypeVar ( \"T\" ) @dataclass class Error ( Exception , Generic [ T ]): code : int description : str data : Union [ T , UndefinedType ] = Undefined @schema ( min_props = 1 , max_props = 1 ) @dataclass class Result ( Generic [ T ]): result : Union [ T , UndefinedType ] = Undefined error : Union [ Error , UndefinedType ] = Undefined def get ( self ) -> T : if self . error is not Undefined : raise self . error else : assert self . result is not Undefined return self . result assert deserialization_schema ( Result [ list [ int ]]) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"additionalProperties\" : False , \"maxProperties\" : 1 , \"minProperties\" : 1 , \"properties\" : { \"result\" : { \"type\" : \"array\" , \"items\" : { \"type\" : \"integer\" }}, \"error\" : { \"additionalProperties\" : False , \"properties\" : { \"code\" : { \"type\" : \"integer\" }, \"description\" : { \"type\" : \"string\" }, \"data\" : {}, }, \"required\" : [ \"code\" , \"description\" ], \"type\" : \"object\" , }, }, \"type\" : \"object\" , } data = { \"result\" : 0 } with raises ( ValidationError ): deserialize ( Result [ str ], data ) result = deserialize ( Result [ int ], data ) assert result == Result ( 0 ) assert result . get () == 0 assert serialize ( Result [ int ], result ) == { \"result\" : 0 } error = deserialize ( Result , { \"error\" : { \"code\" : 42 , \"description\" : \"...\" }}) with raises ( Error ) as err : error . get () assert err . value == Error ( 42 , \"...\" )","title":"OpenRPC"},{"location":"examples/pydantic_support/","text":"Pydantic support \u00b6 It takes only 30 lines of code to support pydantic.BaseModel and all of its subclasses. You could add these lines to your project using pydantic and start to benefit of apischema features. This example deliberately doesn't use set_object_fields but conversions feature in order to roughly include pydantic \"as is\": it will reuse pydantic coercion, error messages, JSON schema, etc. This makes a full retro-compatible support. As a result, lot of apischema features like GraphQL schema generation or NewType validation cannot be supported using this method \u2014 but they could be by using set_object_fields instead. import inspect from collections.abc import Mapping from typing import Any , Optional import pydantic from pytest import raises from apischema import ( ValidationError , deserialize , schema , serialize , serializer , settings , ) from apischema.conversions import AnyConversion , Conversion from apischema.json_schema import deserialization_schema from apischema.schemas import Schema from apischema.validation.errors import LocalizedError #################### Pydantic support code starts here prev_deserialization = settings . deserialization . default_conversion def default_deserialization ( tp : Any ) -> Optional [ AnyConversion ]: if inspect . isclass ( tp ) and issubclass ( tp , pydantic . BaseModel ): def deserialize_pydantic ( data ): try : return tp . parse_obj ( data ) except pydantic . ValidationError as error : raise ValidationError . deserialize ( [ LocalizedError ( err [ \"loc\" ], [ err [ \"msg\" ]]) for err in error . errors ()] ) return Conversion ( deserialize_pydantic , source = tp . __annotations__ . get ( \"__root__\" , Mapping [ str , Any ]), target = tp , ) else : return prev_deserialization ( tp ) settings . deserialization . default_conversion = default_deserialization prev_schema = settings . default_schema def default_schema ( tp : Any ) -> Optional [ Schema ]: if inspect . isclass ( tp ) and issubclass ( tp , pydantic . BaseModel ): return schema ( extra = tp . schema (), override = True ) else : return prev_schema ( tp ) settings . default_schema = default_schema # No need to use settings.serialization because serializer is inherited @serializer def serialize_pydantic ( obj : pydantic . BaseModel ) -> Any : # There is currently no way to retrieve `serialize` parameters inside converters, # so exclude_unset is set to True as it's the default apischema setting return getattr ( obj , \"__root__\" , obj . dict ( exclude_unset = True )) #################### Pydantic support code ends here class Foo ( pydantic . BaseModel ): bar : int assert deserialize ( Foo , { \"bar\" : 0 }) == Foo ( bar = 0 ) assert serialize ( Foo , Foo ( bar = 0 )) == { \"bar\" : 0 } assert deserialization_schema ( Foo ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"title\" : \"Foo\" , # pydantic title \"type\" : \"object\" , \"properties\" : { \"bar\" : { \"title\" : \"Bar\" , \"type\" : \"integer\" }}, \"required\" : [ \"bar\" ], } with raises ( ValidationError ) as err : deserialize ( Foo , { \"bar\" : \"not an int\" }) assert serialize ( err . value ) == [ { \"loc\" : [ \"bar\" ], \"err\" : [ \"value is not a valid integer\" ]} # pydantic error message ]","title":"Pydantic support"},{"location":"examples/pydantic_support/#pydantic-support","text":"It takes only 30 lines of code to support pydantic.BaseModel and all of its subclasses. You could add these lines to your project using pydantic and start to benefit of apischema features. This example deliberately doesn't use set_object_fields but conversions feature in order to roughly include pydantic \"as is\": it will reuse pydantic coercion, error messages, JSON schema, etc. This makes a full retro-compatible support. As a result, lot of apischema features like GraphQL schema generation or NewType validation cannot be supported using this method \u2014 but they could be by using set_object_fields instead. import inspect from collections.abc import Mapping from typing import Any , Optional import pydantic from pytest import raises from apischema import ( ValidationError , deserialize , schema , serialize , serializer , settings , ) from apischema.conversions import AnyConversion , Conversion from apischema.json_schema import deserialization_schema from apischema.schemas import Schema from apischema.validation.errors import LocalizedError #################### Pydantic support code starts here prev_deserialization = settings . deserialization . default_conversion def default_deserialization ( tp : Any ) -> Optional [ AnyConversion ]: if inspect . isclass ( tp ) and issubclass ( tp , pydantic . BaseModel ): def deserialize_pydantic ( data ): try : return tp . parse_obj ( data ) except pydantic . ValidationError as error : raise ValidationError . deserialize ( [ LocalizedError ( err [ \"loc\" ], [ err [ \"msg\" ]]) for err in error . errors ()] ) return Conversion ( deserialize_pydantic , source = tp . __annotations__ . get ( \"__root__\" , Mapping [ str , Any ]), target = tp , ) else : return prev_deserialization ( tp ) settings . deserialization . default_conversion = default_deserialization prev_schema = settings . default_schema def default_schema ( tp : Any ) -> Optional [ Schema ]: if inspect . isclass ( tp ) and issubclass ( tp , pydantic . BaseModel ): return schema ( extra = tp . schema (), override = True ) else : return prev_schema ( tp ) settings . default_schema = default_schema # No need to use settings.serialization because serializer is inherited @serializer def serialize_pydantic ( obj : pydantic . BaseModel ) -> Any : # There is currently no way to retrieve `serialize` parameters inside converters, # so exclude_unset is set to True as it's the default apischema setting return getattr ( obj , \"__root__\" , obj . dict ( exclude_unset = True )) #################### Pydantic support code ends here class Foo ( pydantic . BaseModel ): bar : int assert deserialize ( Foo , { \"bar\" : 0 }) == Foo ( bar = 0 ) assert serialize ( Foo , Foo ( bar = 0 )) == { \"bar\" : 0 } assert deserialization_schema ( Foo ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"title\" : \"Foo\" , # pydantic title \"type\" : \"object\" , \"properties\" : { \"bar\" : { \"title\" : \"Bar\" , \"type\" : \"integer\" }}, \"required\" : [ \"bar\" ], } with raises ( ValidationError ) as err : deserialize ( Foo , { \"bar\" : \"not an int\" }) assert serialize ( err . value ) == [ { \"loc\" : [ \"bar\" ], \"err\" : [ \"value is not a valid integer\" ]} # pydantic error message ]","title":"Pydantic support"},{"location":"examples/recoverable_fields/","text":"Recoverable fields \u00b6 Inspired by https://github.com/samuelcolvin/pydantic/issues/800 from typing import Any , Dict , Generic , TypeVar , Union from pytest import raises from apischema import deserialize , deserializer , schema , serialize , serializer from apischema.json_schema import deserialization_schema , serialization_schema # Add a dummy placeholder comment in order to not have an empty schema # (because Union member with empty schema would \"contaminate\" whole Union schema) @schema ( extra = { \"$comment\" : \"recoverable\" }) class RecoverableRaw ( Exception ): def __init__ ( self , raw : Any ): self . raw = raw deserializer ( RecoverableRaw ) T = TypeVar ( \"T\" ) def remove_recoverable_schema ( json_schema : Dict [ str , Any ]): if \"anyOf\" in json_schema : # deserialization schema value_schema , recoverable_comment = json_schema . pop ( \"anyOf\" ) assert recoverable_comment == { \"$comment\" : \"recoverable\" } json_schema . update ( value_schema ) @schema ( extra = remove_recoverable_schema ) class Recoverable ( Generic [ T ]): def __init__ ( self , value : Union [ T , RecoverableRaw ]): self . _value = value @property def value ( self ) -> T : if isinstance ( self . _value , RecoverableRaw ): raise self . _value return self . _value @value . setter def value ( self , value : T ): self . _value = value deserializer ( Recoverable ) serializer ( Recoverable . value ) assert deserialize ( Recoverable [ int ], 0 ) . value == 0 with raises ( RecoverableRaw ) as err : _ = deserialize ( Recoverable [ int ], \"bad\" ) . value assert err . value . raw == \"bad\" assert serialize ( Recoverable [ int ], Recoverable ( 0 )) == 0 with raises ( RecoverableRaw ) as err : serialize ( Recoverable [ int ], Recoverable ( RecoverableRaw ( \"bad\" ))) assert err . value . raw == \"bad\" assert ( deserialization_schema ( Recoverable [ int ]) == serialization_schema ( Recoverable [ int ]) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"type\" : \"integer\" } )","title":"Recoverable fields"},{"location":"examples/recoverable_fields/#recoverable-fields","text":"Inspired by https://github.com/samuelcolvin/pydantic/issues/800 from typing import Any , Dict , Generic , TypeVar , Union from pytest import raises from apischema import deserialize , deserializer , schema , serialize , serializer from apischema.json_schema import deserialization_schema , serialization_schema # Add a dummy placeholder comment in order to not have an empty schema # (because Union member with empty schema would \"contaminate\" whole Union schema) @schema ( extra = { \"$comment\" : \"recoverable\" }) class RecoverableRaw ( Exception ): def __init__ ( self , raw : Any ): self . raw = raw deserializer ( RecoverableRaw ) T = TypeVar ( \"T\" ) def remove_recoverable_schema ( json_schema : Dict [ str , Any ]): if \"anyOf\" in json_schema : # deserialization schema value_schema , recoverable_comment = json_schema . pop ( \"anyOf\" ) assert recoverable_comment == { \"$comment\" : \"recoverable\" } json_schema . update ( value_schema ) @schema ( extra = remove_recoverable_schema ) class Recoverable ( Generic [ T ]): def __init__ ( self , value : Union [ T , RecoverableRaw ]): self . _value = value @property def value ( self ) -> T : if isinstance ( self . _value , RecoverableRaw ): raise self . _value return self . _value @value . setter def value ( self , value : T ): self . _value = value deserializer ( Recoverable ) serializer ( Recoverable . value ) assert deserialize ( Recoverable [ int ], 0 ) . value == 0 with raises ( RecoverableRaw ) as err : _ = deserialize ( Recoverable [ int ], \"bad\" ) . value assert err . value . raw == \"bad\" assert serialize ( Recoverable [ int ], Recoverable ( 0 )) == 0 with raises ( RecoverableRaw ) as err : serialize ( Recoverable [ int ], Recoverable ( RecoverableRaw ( \"bad\" ))) assert err . value . raw == \"bad\" assert ( deserialization_schema ( Recoverable [ int ]) == serialization_schema ( Recoverable [ int ]) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"type\" : \"integer\" } )","title":"Recoverable fields"},{"location":"examples/sqlalchemy_support/","text":"SQLAlchemy support \u00b6 This example shows a simple support of SQLAlchemy . from collections.abc import Collection from inspect import getmembers from itertools import starmap from typing import Any , Optional from graphql import print_schema from sqlalchemy import Column , Integer , String from sqlalchemy.ext.declarative import as_declarative from apischema import Undefined , deserialize , serialize from apischema.graphql import graphql_schema from apischema.json_schema import serialization_schema from apischema.objects import ObjectField , set_object_fields def column_field ( name : str , column : Column ) -> ObjectField : required = False default : Any = ... if column . default is not None : default = column . default elif column . server_default is not None : default = Undefined elif column . nullable : default = None else : required = True col_type = column . type . python_type col_type = Optional [ col_type ] if column . nullable else col_type return ObjectField ( column . name or name , col_type , required , default = default ) # Very basic SQLAlchemy support @as_declarative () class Base : def __init_subclass__ ( cls ): columns = getmembers ( cls , lambda m : isinstance ( m , Column )) if not columns : return set_object_fields ( cls , starmap ( column_field , columns )) class Foo ( Base ): __tablename__ = \"foo\" bar = Column ( Integer , primary_key = True ) baz = Column ( String ) foo = deserialize ( Foo , { \"bar\" : 0 }) assert isinstance ( foo , Foo ) assert foo . bar == 0 assert serialize ( Foo , foo ) == { \"bar\" : 0 , \"baz\" : None } assert serialization_schema ( Foo ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"type\" : \"object\" , \"properties\" : { \"bar\" : { \"type\" : \"integer\" }, \"baz\" : { \"type\" : [ \"string\" , \"null\" ], \"default\" : None }, }, \"required\" : [ \"bar\" ], \"additionalProperties\" : False , } def foos () -> Optional [ Collection [ Foo ]]: ... schema = graphql_schema ( query = [ foos ]) schema_str = \"\"\" \\ type Query { foos: [Foo!] } type Foo { bar: Int! baz: String } \"\"\" assert print_schema ( schema ) == schema_str","title":"SQLAlchemy support"},{"location":"examples/sqlalchemy_support/#sqlalchemy-support","text":"This example shows a simple support of SQLAlchemy . from collections.abc import Collection from inspect import getmembers from itertools import starmap from typing import Any , Optional from graphql import print_schema from sqlalchemy import Column , Integer , String from sqlalchemy.ext.declarative import as_declarative from apischema import Undefined , deserialize , serialize from apischema.graphql import graphql_schema from apischema.json_schema import serialization_schema from apischema.objects import ObjectField , set_object_fields def column_field ( name : str , column : Column ) -> ObjectField : required = False default : Any = ... if column . default is not None : default = column . default elif column . server_default is not None : default = Undefined elif column . nullable : default = None else : required = True col_type = column . type . python_type col_type = Optional [ col_type ] if column . nullable else col_type return ObjectField ( column . name or name , col_type , required , default = default ) # Very basic SQLAlchemy support @as_declarative () class Base : def __init_subclass__ ( cls ): columns = getmembers ( cls , lambda m : isinstance ( m , Column )) if not columns : return set_object_fields ( cls , starmap ( column_field , columns )) class Foo ( Base ): __tablename__ = \"foo\" bar = Column ( Integer , primary_key = True ) baz = Column ( String ) foo = deserialize ( Foo , { \"bar\" : 0 }) assert isinstance ( foo , Foo ) assert foo . bar == 0 assert serialize ( Foo , foo ) == { \"bar\" : 0 , \"baz\" : None } assert serialization_schema ( Foo ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"type\" : \"object\" , \"properties\" : { \"bar\" : { \"type\" : \"integer\" }, \"baz\" : { \"type\" : [ \"string\" , \"null\" ], \"default\" : None }, }, \"required\" : [ \"bar\" ], \"additionalProperties\" : False , } def foos () -> Optional [ Collection [ Foo ]]: ... schema = graphql_schema ( query = [ foos ]) schema_str = \"\"\" \\ type Query { foos: [Foo!] } type Foo { bar: Int! baz: String } \"\"\" assert print_schema ( schema ) == schema_str","title":"SQLAlchemy support"},{"location":"examples/subclass_tagged_union/","text":"Class as tagged union of its subclasses \u00b6 From https://github.com/wyfo/apischema/discussions/56 Tagged unions are useful when it comes to GraphQL input (or even output). from collections import defaultdict from collections.abc import AsyncIterable , Callable , Iterator from dataclasses import dataclass , field from types import new_class from typing import Annotated , Any , Optional , TypeVar , get_type_hints import graphql from apischema import deserializer , schema , serializer , type_name from apischema.conversions import Conversion from apischema.graphql import graphql_schema from apischema.metadata import conversion from apischema.objects import object_deserialization from apischema.tagged_unions import Tagged , TaggedUnion , get_tagged from apischema.utils import to_pascal_case _alternative_constructors : dict [ type , list [ Callable ]] = defaultdict ( list ) Func = TypeVar ( \"Func\" , bound = Callable ) def alternative_constructor ( func : Func ) -> Func : _alternative_constructors [ get_type_hints ( func )[ \"return\" ]] . append ( func ) return func def rec_subclasses ( cls : type ) -> Iterator [ type ]: \"\"\"Recursive implementation of type.__subclasses__\"\"\" for sub_cls in cls . __subclasses__ (): yield sub_cls yield from rec_subclasses ( sub_cls ) Cls = TypeVar ( \"Cls\" , bound = type ) def as_tagged_union ( cls : Cls ) -> Cls : def serialization () -> Conversion : serialization_union = new_class ( cls . __name__ , ( TaggedUnion ,), exec_body = lambda ns : ns . update ( { \"__annotations__\" : { sub . __name__ : Tagged [ sub ] for sub in rec_subclasses ( cls ) # type: ignore } } ), ) return Conversion ( lambda obj : serialization_union ( ** { obj . __class__ . __name__ : obj }), source = cls , target = serialization_union , # Conversion must not be inherited because it would lead to infinite # recursion otherwise inherited = False , ) def deserialization () -> Conversion : annotations : dict [ str , Any ] = {} deserialization_namespace : dict [ str , Any ] = { \"__annotations__\" : annotations } for sub in rec_subclasses ( cls ): annotations [ sub . __name__ ] = Tagged [ sub ] # type: ignore # Add tagged fields for all its alternative constructors for constructor in _alternative_constructors . get ( sub , ()): # Build the alias of the field alias = to_pascal_case ( constructor . __name__ ) # object_deserialization uses get_type_hints, but the constructor # return type is stringified and the class not defined yet, # so it must be assigned manually constructor . __annotations__ [ \"return\" ] = sub # Add constructor tagged field with its conversion annotations [ alias ] = Tagged [ sub ] # type: ignore deserialization_namespace [ alias ] = Tagged ( conversion ( # Use object_deserialization to wrap constructor as deserializer deserialization = object_deserialization ( constructor , type_name ( alias ) ) ) ) # Create the deserialization tagged union class deserialization_union = new_class ( cls . __name__ , ( TaggedUnion ,), exec_body = lambda ns : ns . update ( deserialization_namespace ), ) return Conversion ( lambda obj : get_tagged ( obj )[ 1 ], source = deserialization_union , target = cls ) deserializer ( lazy = deserialization , target = cls ) serializer ( lazy = serialization , source = cls ) return cls @as_tagged_union class Drawing : def points ( self ) -> AsyncIterable [ float ]: raise NotImplementedError @dataclass class Line ( Drawing ): start : float stop : float step : float = field ( default = 1 , metadata = schema ( exc_min = 0 )) async def points ( self ) -> AsyncIterable [ float ]: point = self . start while point <= self . stop : yield point point += self . step @alternative_constructor def sized_line ( start : float , stop : float , size : Annotated [ float , schema ( min = 1 )] ) -> \"Line\" : return Line ( start = start , stop = stop , step = ( stop - start ) / ( size - 1 )) @dataclass class Concat ( Drawing ): left : Drawing right : Drawing async def points ( self ) -> AsyncIterable [ float ]: async for point in self . left . points (): yield point async for point in self . right . points (): yield point def echo ( drawing : Drawing = None ) -> Optional [ Drawing ]: return drawing drawing_schema = graphql_schema ( query = [ echo ]) assert ( graphql . utilities . print_schema ( drawing_schema ) == \"\"\" \\ type Query { echo(drawing: DrawingInput): Drawing } type Drawing { Line: Line Concat: Concat } type Line { start: Float! stop: Float! step: Float! } type Concat { left: Drawing! right: Drawing! } input DrawingInput { Line: LineInput SizedLine: SizedLineInput Concat: ConcatInput } input LineInput { start: Float! stop: Float! step: Float! = 1 } input SizedLineInput { start: Float! stop: Float! size: Float! } input ConcatInput { left: DrawingInput! right: DrawingInput! } \"\"\" ) query = \"\"\" \\ { echo(drawing: { Concat: { left: { SizedLine: { start: 0, stop: 12, size: 3 }, }, right: { Line: { start: 12, stop: 13 }, } } }) { Concat { left { Line { start stop step } } right { Line { start stop step } } } } } \"\"\" assert graphql . graphql_sync ( drawing_schema , query ) . data == { \"echo\" : { \"Concat\" : { \"left\" : { \"Line\" : { \"start\" : 0.0 , \"stop\" : 12.0 , \"step\" : 6.0 }}, \"right\" : { \"Line\" : { \"start\" : 12.0 , \"stop\" : 13.0 , \"step\" : 1.0 }}, } } }","title":"Class as tagged union of its subclasses"},{"location":"examples/subclass_tagged_union/#class-as-tagged-union-of-its-subclasses","text":"From https://github.com/wyfo/apischema/discussions/56 Tagged unions are useful when it comes to GraphQL input (or even output). from collections import defaultdict from collections.abc import AsyncIterable , Callable , Iterator from dataclasses import dataclass , field from types import new_class from typing import Annotated , Any , Optional , TypeVar , get_type_hints import graphql from apischema import deserializer , schema , serializer , type_name from apischema.conversions import Conversion from apischema.graphql import graphql_schema from apischema.metadata import conversion from apischema.objects import object_deserialization from apischema.tagged_unions import Tagged , TaggedUnion , get_tagged from apischema.utils import to_pascal_case _alternative_constructors : dict [ type , list [ Callable ]] = defaultdict ( list ) Func = TypeVar ( \"Func\" , bound = Callable ) def alternative_constructor ( func : Func ) -> Func : _alternative_constructors [ get_type_hints ( func )[ \"return\" ]] . append ( func ) return func def rec_subclasses ( cls : type ) -> Iterator [ type ]: \"\"\"Recursive implementation of type.__subclasses__\"\"\" for sub_cls in cls . __subclasses__ (): yield sub_cls yield from rec_subclasses ( sub_cls ) Cls = TypeVar ( \"Cls\" , bound = type ) def as_tagged_union ( cls : Cls ) -> Cls : def serialization () -> Conversion : serialization_union = new_class ( cls . __name__ , ( TaggedUnion ,), exec_body = lambda ns : ns . update ( { \"__annotations__\" : { sub . __name__ : Tagged [ sub ] for sub in rec_subclasses ( cls ) # type: ignore } } ), ) return Conversion ( lambda obj : serialization_union ( ** { obj . __class__ . __name__ : obj }), source = cls , target = serialization_union , # Conversion must not be inherited because it would lead to infinite # recursion otherwise inherited = False , ) def deserialization () -> Conversion : annotations : dict [ str , Any ] = {} deserialization_namespace : dict [ str , Any ] = { \"__annotations__\" : annotations } for sub in rec_subclasses ( cls ): annotations [ sub . __name__ ] = Tagged [ sub ] # type: ignore # Add tagged fields for all its alternative constructors for constructor in _alternative_constructors . get ( sub , ()): # Build the alias of the field alias = to_pascal_case ( constructor . __name__ ) # object_deserialization uses get_type_hints, but the constructor # return type is stringified and the class not defined yet, # so it must be assigned manually constructor . __annotations__ [ \"return\" ] = sub # Add constructor tagged field with its conversion annotations [ alias ] = Tagged [ sub ] # type: ignore deserialization_namespace [ alias ] = Tagged ( conversion ( # Use object_deserialization to wrap constructor as deserializer deserialization = object_deserialization ( constructor , type_name ( alias ) ) ) ) # Create the deserialization tagged union class deserialization_union = new_class ( cls . __name__ , ( TaggedUnion ,), exec_body = lambda ns : ns . update ( deserialization_namespace ), ) return Conversion ( lambda obj : get_tagged ( obj )[ 1 ], source = deserialization_union , target = cls ) deserializer ( lazy = deserialization , target = cls ) serializer ( lazy = serialization , source = cls ) return cls @as_tagged_union class Drawing : def points ( self ) -> AsyncIterable [ float ]: raise NotImplementedError @dataclass class Line ( Drawing ): start : float stop : float step : float = field ( default = 1 , metadata = schema ( exc_min = 0 )) async def points ( self ) -> AsyncIterable [ float ]: point = self . start while point <= self . stop : yield point point += self . step @alternative_constructor def sized_line ( start : float , stop : float , size : Annotated [ float , schema ( min = 1 )] ) -> \"Line\" : return Line ( start = start , stop = stop , step = ( stop - start ) / ( size - 1 )) @dataclass class Concat ( Drawing ): left : Drawing right : Drawing async def points ( self ) -> AsyncIterable [ float ]: async for point in self . left . points (): yield point async for point in self . right . points (): yield point def echo ( drawing : Drawing = None ) -> Optional [ Drawing ]: return drawing drawing_schema = graphql_schema ( query = [ echo ]) assert ( graphql . utilities . print_schema ( drawing_schema ) == \"\"\" \\ type Query { echo(drawing: DrawingInput): Drawing } type Drawing { Line: Line Concat: Concat } type Line { start: Float! stop: Float! step: Float! } type Concat { left: Drawing! right: Drawing! } input DrawingInput { Line: LineInput SizedLine: SizedLineInput Concat: ConcatInput } input LineInput { start: Float! stop: Float! step: Float! = 1 } input SizedLineInput { start: Float! stop: Float! size: Float! } input ConcatInput { left: DrawingInput! right: DrawingInput! } \"\"\" ) query = \"\"\" \\ { echo(drawing: { Concat: { left: { SizedLine: { start: 0, stop: 12, size: 3 }, }, right: { Line: { start: 12, stop: 13 }, } } }) { Concat { left { Line { start stop step } } right { Line { start stop step } } } } } \"\"\" assert graphql . graphql_sync ( drawing_schema , query ) . data == { \"echo\" : { \"Concat\" : { \"left\" : { \"Line\" : { \"start\" : 0.0 , \"stop\" : 12.0 , \"step\" : 6.0 }}, \"right\" : { \"Line\" : { \"start\" : 12.0 , \"stop\" : 13.0 , \"step\" : 1.0 }}, } } }","title":"Class as tagged union of its subclasses"},{"location":"examples/subclass_union/","text":"Class as union of its subclasses \u00b6 Inspired by https://github.com/samuelcolvin/pydantic/issues/2036 A class can easily be deserialized as a union of its subclasses using deserializers. Indeed, when more than one deserializer are registered, it results in a union. from dataclasses import dataclass from typing import Any , Union from apischema import deserialize , deserializer , serializer from apischema.conversions import Conversion , identity from apischema.json_schema import deserialization_schema , serialization_schema class Base : _union : Any = None # You can use __init_subclass__ to register new subclass automatically def __init_subclass__ ( cls , ** kwargs ): # Deserializers stack directly as a Union deserializer ( Conversion ( identity , source = cls , target = Base )) # Only Base serializer must be registered (and updated for each subclass) as # a Union, and not be inherited Base . _union = cls if Base . _union is None else Union [ Base . _union , cls ] serializer ( Conversion ( identity , source = Base , target = Base . _union , inherited = False ) ) @dataclass class Foo ( Base ): foo : int @dataclass class Bar ( Base ): bar : str assert ( deserialization_schema ( Base ) == serialization_schema ( Base ) == { \"anyOf\" : [ { \"type\" : \"object\" , \"properties\" : { \"foo\" : { \"type\" : \"integer\" }}, \"required\" : [ \"foo\" ], \"additionalProperties\" : False , }, { \"type\" : \"object\" , \"properties\" : { \"bar\" : { \"type\" : \"string\" }}, \"required\" : [ \"bar\" ], \"additionalProperties\" : False , }, ], \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , } ) assert deserialize ( Base , { \"foo\" : 0 }) == Foo ( 0 )","title":"Class as union of its subclasses"},{"location":"examples/subclass_union/#class-as-union-of-its-subclasses","text":"Inspired by https://github.com/samuelcolvin/pydantic/issues/2036 A class can easily be deserialized as a union of its subclasses using deserializers. Indeed, when more than one deserializer are registered, it results in a union. from dataclasses import dataclass from typing import Any , Union from apischema import deserialize , deserializer , serializer from apischema.conversions import Conversion , identity from apischema.json_schema import deserialization_schema , serialization_schema class Base : _union : Any = None # You can use __init_subclass__ to register new subclass automatically def __init_subclass__ ( cls , ** kwargs ): # Deserializers stack directly as a Union deserializer ( Conversion ( identity , source = cls , target = Base )) # Only Base serializer must be registered (and updated for each subclass) as # a Union, and not be inherited Base . _union = cls if Base . _union is None else Union [ Base . _union , cls ] serializer ( Conversion ( identity , source = Base , target = Base . _union , inherited = False ) ) @dataclass class Foo ( Base ): foo : int @dataclass class Bar ( Base ): bar : str assert ( deserialization_schema ( Base ) == serialization_schema ( Base ) == { \"anyOf\" : [ { \"type\" : \"object\" , \"properties\" : { \"foo\" : { \"type\" : \"integer\" }}, \"required\" : [ \"foo\" ], \"additionalProperties\" : False , }, { \"type\" : \"object\" , \"properties\" : { \"bar\" : { \"type\" : \"string\" }}, \"required\" : [ \"bar\" ], \"additionalProperties\" : False , }, ], \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , } ) assert deserialize ( Base , { \"foo\" : 0 }) == Foo ( 0 )","title":"Class as union of its subclasses"},{"location":"graphql/data_model_and_resolvers/","text":"Data model and resolvers \u00b6 Almost everything of the Data model section remains valid in GraphQL integration. Restrictions \u00b6 TypedDict \u00b6 TypedDict is not supported in output type. In fact, typed dicts are not real classes, so their type can not be checked at runtime, but this is required to disambiguate unions/interfaces. Union \u00b6 Unions are only supported between output object type, which means dataclass and NamedTuple (and conversions / dataclass model ). There are 2 exceptions which can be always be used in Union : None / Optional : Types are non-null (marked with an exclamation mark ! in GraphQL schema) by default; Optional types however results in normal GraphQL types (without ! ). apischema.UndefinedType : it is simply ignored. It is useful in resolvers, see following section Non-null \u00b6 Types are assumed to be non-null by default, as in Python typing. Nullable types are obtained using typing.Optional (or typing.Union with a None argument). Note There is one exception, when resolver parameter default value is not serializable (and thus cannot be included in the schema), parameter type is then set as nullable to make the parameter non-required. For example parameters not Optional but with Undefined default value will be marked as nullable. This is only for the schema, default value is still used at execution. Undefined \u00b6 In output, Undefined is converted to None ; so in the schema, Union[T, UndefinedType] will be nullable. In input, fields become nullable when Undefined is their default value. Interfaces \u00b6 Interfaces are simply classes marked with apischema.graphql.interface decorator. An object type implements an interface when its class inherits of interface-marked class, or when it has flattened fields of interface-marked dataclass. from dataclasses import dataclass from typing import Optional from graphql import print_schema from apischema.graphql import graphql_schema , interface @interface @dataclass class Bar : bar : int @dataclass class Foo ( Bar ): baz : str def foo () -> Optional [ Foo ]: ... schema = graphql_schema ( query = [ foo ]) schema_str = \"\"\" \\ type Query { foo: Foo } type Foo implements Bar { bar: Int! baz: String! } interface Bar { bar: Int! } \"\"\" assert print_schema ( schema ) == schema_str Resolvers \u00b6 All dataclass / NamedTuple fields (excepted skipped ) are resolved with their alias in the GraphQL schema. Custom resolvers can also be added by marking methods with apischema.graphql.resolver decorator \u2014 resolvers share a common interface with apischema.serialized , with a few differences. Methods can be synchronous or asynchronous (defined with async def or annotated with an typing.Awaitable return type). Resolvers parameters are included in the schema with their type, and their default value. from dataclasses import dataclass from typing import Optional from graphql import print_schema from apischema.graphql import graphql_schema , resolver @dataclass class Bar : baz : int @dataclass class Foo : @resolver async def bar ( self , arg : int = 0 ) -> Bar : ... async def foo () -> Optional [ Foo ]: ... schema = graphql_schema ( query = [ foo ]) schema_str = \"\"\" \\ type Query { foo: Foo } type Foo { bar(arg: Int! = 0): Bar! } type Bar { baz: Int! } \"\"\" assert print_schema ( schema ) == schema_str GraphQLResolveInfo parameter \u00b6 Resolvers can have an additional parameter of type graphql.GraphQLResolveInfo (or Optional[graphql.GraphQLResolveInfo] ), which is automatically injected when the resolver is executed in the context of a GraphQL request. This parameter contains the info about the current GraphQL request being executed. Undefined parameter default \u2014 null vs. undefined \u00b6 Undefined can be used as default value of resolver parameters. It can be to distinguish a null input from an absent/ undefined input. In fact, null value will result in a None argument where no value will use the default value, Undefined so. from typing import Optional , Union from graphql import graphql_sync from apischema import Undefined , UndefinedType from apischema.graphql import graphql_schema def arg_is_absent ( arg : Optional [ Union [ int , UndefinedType ]] = Undefined ) -> bool : return arg is Undefined schema = graphql_schema ( query = [ arg_is_absent ]) assert graphql_sync ( schema , \"{argIsAbsent(arg: null)}\" ) . data == { \"argIsAbsent\" : False } assert graphql_sync ( schema , \" {argIsAbsent} \" ) . data == { \"argIsAbsent\" : True } Error handling \u00b6 Errors occurring in resolvers can be caught in a dedicated error handler registered with error_handler parameter. This function takes in parameters the exception, the object, the info and the kwargs of the failing resolver; it can return a new value or raise the current or another exception \u2014 it can for example be used to log errors without throwing the complete serialization. The resulting serialization type will be a Union of the normal type and the error handling type ; if the error handler always raises, use typing.NoReturn annotation. error_handler=None correspond to a default handler which only return None \u2014 exception is thus discarded and the resolver type becomes Optional . The error handler is only executed by apischema serialization process, it's not added to the function, so this one can be executed normally and raise an exception in the rest of your code. Error handler can be synchronous or asynchronous. from dataclasses import dataclass from logging import getLogger from typing import Any import graphql from graphql.utilities import print_schema from apischema.graphql import graphql_schema , resolver logger = getLogger ( __name__ ) def log_error ( error : Exception , obj : Any , info : graphql . GraphQLResolveInfo , ** kwargs ) -> None : logger . error ( \"Resolve error in %s \" , \".\" . join ( map ( str , info . path . as_list ())), exc_info = error ) return None @dataclass class Foo : @resolver ( error_handler = log_error ) def bar ( self ) -> int : raise RuntimeError ( \"Bar error\" ) @resolver def baz ( self ) -> int : raise RuntimeError ( \"Baz error\" ) def foo ( info : graphql . GraphQLResolveInfo ) -> Foo : return Foo () schema = graphql_schema ( query = [ foo ]) # Notice that bar is Int while baz is Int! schema_str = \"\"\" \\ type Query { foo: Foo! } type Foo { bar: Int baz: Int! } \"\"\" assert print_schema ( schema ) == schema_str # Logs \"Resolve error in foo.bar\", no error raised assert graphql . graphql_sync ( schema , \"{foo {bar} }\" ) . data == { \"foo\" : { \"bar\" : None }} # Error is raised assert graphql . graphql_sync ( schema , \"{foo {baz} }\" ) . errors [ 0 ] . message == \"Baz error\" Parameters metadata \u00b6 Resolvers parameters can have metadata like dataclass fields. They can be passed using typing.Annotated . from dataclasses import dataclass from typing import Annotated from graphql.utilities import print_schema from apischema import alias , schema from apischema.graphql import graphql_schema , resolver @dataclass class Foo : @resolver def bar ( self , param : Annotated [ int , alias ( \"arg\" ) | schema ( description = \"argument\" )] ) -> int : return param def foo () -> Foo : return Foo () schema_ = graphql_schema ( query = [ foo ]) # Notice that bar is Int while baz is Int! schema_str = ''' \\ type Query { foo: Foo! } type Foo { bar( \"\"\"argument\"\"\" arg: Int! ): Int! } ''' assert print_schema ( schema_ ) == schema_str Note Metadata can also be passed with parameters_metadata parameter; it takes a mapping of parameter names as key and mapped metadata as value. ID type \u00b6 GraphQL ID has no precise specification and is defined according API needs; it can be a UUID or and ObjectId, etc. apischema.graphql_schema has a parameter id_types which can be used to define which types will be marked as ID in the generated schema. Parameter value can be either a collection of types (each type will then be mapped to ID scalar), or a predicate returning if the given type must be marked as ID . from dataclasses import dataclass from typing import Optional from uuid import UUID from graphql import print_schema from apischema.graphql import graphql_schema @dataclass class Foo : bar : UUID def foo () -> Optional [ Foo ]: ... # id_types={UUID} is equivalent to id_types=lambda t: t in {UUID} schema = graphql_schema ( query = [ foo ], id_types = { UUID }) schema_str = \"\"\" \\ type Query { foo: Foo } type Foo { bar: ID! } \"\"\" assert print_schema ( schema ) == schema_str Note ID type could also be identified using typing.Annotated and a predicate looking into annotations. apischema also provides a simple ID type with apischema.graphql.ID . It is just defined as a NewType of string, so you can use it when you want to manipulate raw ID strings in your resolvers. ID encoding \u00b6 ID encoding can directly be controlled the id_encoding parameters of graphql_schema . A current practice is to use base64 encoding for ID . from base64 import b64decode , b64encode from dataclasses import dataclass from typing import Optional from uuid import UUID from graphql import graphql_sync from apischema.graphql import graphql_schema @dataclass class Foo : id : UUID def foo () -> Optional [ Foo ]: return Foo ( UUID ( \"58c88e87-5769-4723-8974-f9ec5007a38b\" )) schema = graphql_schema ( query = [ foo ], id_types = { UUID }, id_encoding = ( lambda s : b64decode ( s ) . decode (), lambda s : b64encode ( s . encode ()) . decode (), ), ) assert graphql_sync ( schema , \"{foo {id} }\" ) . data == { \"foo\" : { \"id\" : \"NThjODhlODctNTc2OS00NzIzLTg5NzQtZjllYzUwMDdhMzhi\" } } Note You can also use relay.base64_encoding (see next section ) Note ID serialization (respectively deserialization) is applied after apischema conversions (respectively before apischema conversion): in the example, uuid is already converted into string before being passed to id_serializer . If you use base64 encodeing and an ID type which is converted by apischema to a base64 str, you will get a double encoded base64 string Tagged unions \u00b6 Important This feature has a provisional status, as the concerned GraphQL RFC is not finalized. apischema provides a apischema.tagged_unions.TaggedUnion base class which helps to implement the tagged union pattern. It's fields must be typed using apischema.tagged_unions.Tagged generic type. from dataclasses import dataclass from pytest import raises from apischema import Undefined , ValidationError , alias , deserialize , schema , serialize from apischema.tagged_unions import Tagged , TaggedUnion , get_tagged @dataclass class Bar : field : str class Foo ( TaggedUnion ): bar : Tagged [ Bar ] # Tagged can have metadata like a dataclass fields i : Tagged [ int ] = Tagged ( alias ( \"baz\" ) | schema ( min = 0 )) # Instantiate using class fields tagged_bar = Foo . bar ( Bar ( \"value\" )) # you can also use default constructor, but it's not typed-checked assert tagged_bar == Foo ( bar = Bar ( \"value\" )) # All fields that are not tagged are Undefined assert tagged_bar . bar is not Undefined and tagged_bar . i is Undefined # get_tagged allows to retrieve the tag and it's value # (but the value is not typed-checked) assert get_tagged ( tagged_bar ) == ( \"bar\" , Bar ( \"value\" )) # (De)serialization works as expected assert deserialize ( Foo , { \"bar\" : { \"field\" : \"value\" }}) == tagged_bar assert serialize ( Foo , tagged_bar ) == { \"bar\" : { \"field\" : \"value\" }} with raises ( ValidationError ) as err : deserialize ( Foo , { \"unknown\" : 42 }) assert serialize ( err . value ) == [{ \"loc\" : [ \"unknown\" ], \"err\" : [ \"unexpected property\" ]}] with raises ( ValidationError ) as err : deserialize ( Foo , { \"bar\" : { \"field\" : \"value\" }, \"baz\" : 0 }) assert serialize ( err . value ) == [ { \"loc\" : [], \"err\" : [ \"property count greater than 1 (maxProperties)\" ]} ] JSON schema \u00b6 Tagged unions JSON schema uses minProperties: 1 and maxProperties: 1 . from dataclasses import dataclass from apischema.json_schema import deserialization_schema , serialization_schema from apischema.tagged_unions import Tagged , TaggedUnion @dataclass class Bar : field : str class Foo ( TaggedUnion ): bar : Tagged [ Bar ] baz : Tagged [ int ] assert ( deserialization_schema ( Foo ) == serialization_schema ( Foo ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"type\" : \"object\" , \"properties\" : { \"bar\" : { \"type\" : \"object\" , \"properties\" : { \"field\" : { \"type\" : \"string\" }}, \"required\" : [ \"field\" ], \"additionalProperties\" : False , }, \"baz\" : { \"type\" : \"integer\" }, }, \"additionalProperties\" : False , \"minProperties\" : 1 , \"maxProperties\" : 1 , } ) GraphQL schema \u00b6 As tagged union are not (yet?) part of the GraphQL spec, so they are just implemented as normal (input) object type with nullable fields. An error is raised if several tags are passed in input. from dataclasses import dataclass from graphql import graphql_sync from graphql.utilities import print_schema from apischema.graphql import graphql_schema from apischema.tagged_unions import Tagged , TaggedUnion @dataclass class Bar : field : str class Foo ( TaggedUnion ): bar : Tagged [ Bar ] baz : Tagged [ int ] def query ( foo : Foo ) -> Foo : return foo schema = graphql_schema ( query = [ query ]) schema_str = \"\"\" \\ type Query { query(foo: FooInput!): Foo! } type Foo { bar: Bar baz: Int } type Bar { field: String! } input FooInput { bar: BarInput baz: Int } input BarInput { field: String! } \"\"\" assert print_schema ( schema ) == schema_str query_str = \"\"\" { query(foo: {bar: {field: \"value\"}}) { bar { field } baz } } \"\"\" assert graphql_sync ( schema , query_str ) . data == { \"query\" : { \"bar\" : { \"field\" : \"value\" }, \"baz\" : None } }","title":"Data model and resolvers"},{"location":"graphql/data_model_and_resolvers/#data-model-and-resolvers","text":"Almost everything of the Data model section remains valid in GraphQL integration.","title":"Data model and resolvers"},{"location":"graphql/data_model_and_resolvers/#restrictions","text":"","title":"Restrictions"},{"location":"graphql/data_model_and_resolvers/#typeddict","text":"TypedDict is not supported in output type. In fact, typed dicts are not real classes, so their type can not be checked at runtime, but this is required to disambiguate unions/interfaces.","title":"TypedDict"},{"location":"graphql/data_model_and_resolvers/#union","text":"Unions are only supported between output object type, which means dataclass and NamedTuple (and conversions / dataclass model ). There are 2 exceptions which can be always be used in Union : None / Optional : Types are non-null (marked with an exclamation mark ! in GraphQL schema) by default; Optional types however results in normal GraphQL types (without ! ). apischema.UndefinedType : it is simply ignored. It is useful in resolvers, see following section","title":"Union"},{"location":"graphql/data_model_and_resolvers/#non-null","text":"Types are assumed to be non-null by default, as in Python typing. Nullable types are obtained using typing.Optional (or typing.Union with a None argument). Note There is one exception, when resolver parameter default value is not serializable (and thus cannot be included in the schema), parameter type is then set as nullable to make the parameter non-required. For example parameters not Optional but with Undefined default value will be marked as nullable. This is only for the schema, default value is still used at execution.","title":"Non-null"},{"location":"graphql/data_model_and_resolvers/#undefined","text":"In output, Undefined is converted to None ; so in the schema, Union[T, UndefinedType] will be nullable. In input, fields become nullable when Undefined is their default value.","title":"Undefined"},{"location":"graphql/data_model_and_resolvers/#interfaces","text":"Interfaces are simply classes marked with apischema.graphql.interface decorator. An object type implements an interface when its class inherits of interface-marked class, or when it has flattened fields of interface-marked dataclass. from dataclasses import dataclass from typing import Optional from graphql import print_schema from apischema.graphql import graphql_schema , interface @interface @dataclass class Bar : bar : int @dataclass class Foo ( Bar ): baz : str def foo () -> Optional [ Foo ]: ... schema = graphql_schema ( query = [ foo ]) schema_str = \"\"\" \\ type Query { foo: Foo } type Foo implements Bar { bar: Int! baz: String! } interface Bar { bar: Int! } \"\"\" assert print_schema ( schema ) == schema_str","title":"Interfaces"},{"location":"graphql/data_model_and_resolvers/#resolvers","text":"All dataclass / NamedTuple fields (excepted skipped ) are resolved with their alias in the GraphQL schema. Custom resolvers can also be added by marking methods with apischema.graphql.resolver decorator \u2014 resolvers share a common interface with apischema.serialized , with a few differences. Methods can be synchronous or asynchronous (defined with async def or annotated with an typing.Awaitable return type). Resolvers parameters are included in the schema with their type, and their default value. from dataclasses import dataclass from typing import Optional from graphql import print_schema from apischema.graphql import graphql_schema , resolver @dataclass class Bar : baz : int @dataclass class Foo : @resolver async def bar ( self , arg : int = 0 ) -> Bar : ... async def foo () -> Optional [ Foo ]: ... schema = graphql_schema ( query = [ foo ]) schema_str = \"\"\" \\ type Query { foo: Foo } type Foo { bar(arg: Int! = 0): Bar! } type Bar { baz: Int! } \"\"\" assert print_schema ( schema ) == schema_str","title":"Resolvers"},{"location":"graphql/data_model_and_resolvers/#graphqlresolveinfo-parameter","text":"Resolvers can have an additional parameter of type graphql.GraphQLResolveInfo (or Optional[graphql.GraphQLResolveInfo] ), which is automatically injected when the resolver is executed in the context of a GraphQL request. This parameter contains the info about the current GraphQL request being executed.","title":"GraphQLResolveInfo parameter"},{"location":"graphql/data_model_and_resolvers/#undefined-parameter-default-null-vs-undefined","text":"Undefined can be used as default value of resolver parameters. It can be to distinguish a null input from an absent/ undefined input. In fact, null value will result in a None argument where no value will use the default value, Undefined so. from typing import Optional , Union from graphql import graphql_sync from apischema import Undefined , UndefinedType from apischema.graphql import graphql_schema def arg_is_absent ( arg : Optional [ Union [ int , UndefinedType ]] = Undefined ) -> bool : return arg is Undefined schema = graphql_schema ( query = [ arg_is_absent ]) assert graphql_sync ( schema , \"{argIsAbsent(arg: null)}\" ) . data == { \"argIsAbsent\" : False } assert graphql_sync ( schema , \" {argIsAbsent} \" ) . data == { \"argIsAbsent\" : True }","title":"Undefined parameter default \u2014 null vs. undefined"},{"location":"graphql/data_model_and_resolvers/#error-handling","text":"Errors occurring in resolvers can be caught in a dedicated error handler registered with error_handler parameter. This function takes in parameters the exception, the object, the info and the kwargs of the failing resolver; it can return a new value or raise the current or another exception \u2014 it can for example be used to log errors without throwing the complete serialization. The resulting serialization type will be a Union of the normal type and the error handling type ; if the error handler always raises, use typing.NoReturn annotation. error_handler=None correspond to a default handler which only return None \u2014 exception is thus discarded and the resolver type becomes Optional . The error handler is only executed by apischema serialization process, it's not added to the function, so this one can be executed normally and raise an exception in the rest of your code. Error handler can be synchronous or asynchronous. from dataclasses import dataclass from logging import getLogger from typing import Any import graphql from graphql.utilities import print_schema from apischema.graphql import graphql_schema , resolver logger = getLogger ( __name__ ) def log_error ( error : Exception , obj : Any , info : graphql . GraphQLResolveInfo , ** kwargs ) -> None : logger . error ( \"Resolve error in %s \" , \".\" . join ( map ( str , info . path . as_list ())), exc_info = error ) return None @dataclass class Foo : @resolver ( error_handler = log_error ) def bar ( self ) -> int : raise RuntimeError ( \"Bar error\" ) @resolver def baz ( self ) -> int : raise RuntimeError ( \"Baz error\" ) def foo ( info : graphql . GraphQLResolveInfo ) -> Foo : return Foo () schema = graphql_schema ( query = [ foo ]) # Notice that bar is Int while baz is Int! schema_str = \"\"\" \\ type Query { foo: Foo! } type Foo { bar: Int baz: Int! } \"\"\" assert print_schema ( schema ) == schema_str # Logs \"Resolve error in foo.bar\", no error raised assert graphql . graphql_sync ( schema , \"{foo {bar} }\" ) . data == { \"foo\" : { \"bar\" : None }} # Error is raised assert graphql . graphql_sync ( schema , \"{foo {baz} }\" ) . errors [ 0 ] . message == \"Baz error\"","title":"Error handling"},{"location":"graphql/data_model_and_resolvers/#parameters-metadata","text":"Resolvers parameters can have metadata like dataclass fields. They can be passed using typing.Annotated . from dataclasses import dataclass from typing import Annotated from graphql.utilities import print_schema from apischema import alias , schema from apischema.graphql import graphql_schema , resolver @dataclass class Foo : @resolver def bar ( self , param : Annotated [ int , alias ( \"arg\" ) | schema ( description = \"argument\" )] ) -> int : return param def foo () -> Foo : return Foo () schema_ = graphql_schema ( query = [ foo ]) # Notice that bar is Int while baz is Int! schema_str = ''' \\ type Query { foo: Foo! } type Foo { bar( \"\"\"argument\"\"\" arg: Int! ): Int! } ''' assert print_schema ( schema_ ) == schema_str Note Metadata can also be passed with parameters_metadata parameter; it takes a mapping of parameter names as key and mapped metadata as value.","title":"Parameters metadata"},{"location":"graphql/data_model_and_resolvers/#id-type","text":"GraphQL ID has no precise specification and is defined according API needs; it can be a UUID or and ObjectId, etc. apischema.graphql_schema has a parameter id_types which can be used to define which types will be marked as ID in the generated schema. Parameter value can be either a collection of types (each type will then be mapped to ID scalar), or a predicate returning if the given type must be marked as ID . from dataclasses import dataclass from typing import Optional from uuid import UUID from graphql import print_schema from apischema.graphql import graphql_schema @dataclass class Foo : bar : UUID def foo () -> Optional [ Foo ]: ... # id_types={UUID} is equivalent to id_types=lambda t: t in {UUID} schema = graphql_schema ( query = [ foo ], id_types = { UUID }) schema_str = \"\"\" \\ type Query { foo: Foo } type Foo { bar: ID! } \"\"\" assert print_schema ( schema ) == schema_str Note ID type could also be identified using typing.Annotated and a predicate looking into annotations. apischema also provides a simple ID type with apischema.graphql.ID . It is just defined as a NewType of string, so you can use it when you want to manipulate raw ID strings in your resolvers.","title":"ID type"},{"location":"graphql/data_model_and_resolvers/#id-encoding","text":"ID encoding can directly be controlled the id_encoding parameters of graphql_schema . A current practice is to use base64 encoding for ID . from base64 import b64decode , b64encode from dataclasses import dataclass from typing import Optional from uuid import UUID from graphql import graphql_sync from apischema.graphql import graphql_schema @dataclass class Foo : id : UUID def foo () -> Optional [ Foo ]: return Foo ( UUID ( \"58c88e87-5769-4723-8974-f9ec5007a38b\" )) schema = graphql_schema ( query = [ foo ], id_types = { UUID }, id_encoding = ( lambda s : b64decode ( s ) . decode (), lambda s : b64encode ( s . encode ()) . decode (), ), ) assert graphql_sync ( schema , \"{foo {id} }\" ) . data == { \"foo\" : { \"id\" : \"NThjODhlODctNTc2OS00NzIzLTg5NzQtZjllYzUwMDdhMzhi\" } } Note You can also use relay.base64_encoding (see next section ) Note ID serialization (respectively deserialization) is applied after apischema conversions (respectively before apischema conversion): in the example, uuid is already converted into string before being passed to id_serializer . If you use base64 encodeing and an ID type which is converted by apischema to a base64 str, you will get a double encoded base64 string","title":"ID encoding"},{"location":"graphql/data_model_and_resolvers/#tagged-unions","text":"Important This feature has a provisional status, as the concerned GraphQL RFC is not finalized. apischema provides a apischema.tagged_unions.TaggedUnion base class which helps to implement the tagged union pattern. It's fields must be typed using apischema.tagged_unions.Tagged generic type. from dataclasses import dataclass from pytest import raises from apischema import Undefined , ValidationError , alias , deserialize , schema , serialize from apischema.tagged_unions import Tagged , TaggedUnion , get_tagged @dataclass class Bar : field : str class Foo ( TaggedUnion ): bar : Tagged [ Bar ] # Tagged can have metadata like a dataclass fields i : Tagged [ int ] = Tagged ( alias ( \"baz\" ) | schema ( min = 0 )) # Instantiate using class fields tagged_bar = Foo . bar ( Bar ( \"value\" )) # you can also use default constructor, but it's not typed-checked assert tagged_bar == Foo ( bar = Bar ( \"value\" )) # All fields that are not tagged are Undefined assert tagged_bar . bar is not Undefined and tagged_bar . i is Undefined # get_tagged allows to retrieve the tag and it's value # (but the value is not typed-checked) assert get_tagged ( tagged_bar ) == ( \"bar\" , Bar ( \"value\" )) # (De)serialization works as expected assert deserialize ( Foo , { \"bar\" : { \"field\" : \"value\" }}) == tagged_bar assert serialize ( Foo , tagged_bar ) == { \"bar\" : { \"field\" : \"value\" }} with raises ( ValidationError ) as err : deserialize ( Foo , { \"unknown\" : 42 }) assert serialize ( err . value ) == [{ \"loc\" : [ \"unknown\" ], \"err\" : [ \"unexpected property\" ]}] with raises ( ValidationError ) as err : deserialize ( Foo , { \"bar\" : { \"field\" : \"value\" }, \"baz\" : 0 }) assert serialize ( err . value ) == [ { \"loc\" : [], \"err\" : [ \"property count greater than 1 (maxProperties)\" ]} ]","title":"Tagged unions"},{"location":"graphql/data_model_and_resolvers/#json-schema","text":"Tagged unions JSON schema uses minProperties: 1 and maxProperties: 1 . from dataclasses import dataclass from apischema.json_schema import deserialization_schema , serialization_schema from apischema.tagged_unions import Tagged , TaggedUnion @dataclass class Bar : field : str class Foo ( TaggedUnion ): bar : Tagged [ Bar ] baz : Tagged [ int ] assert ( deserialization_schema ( Foo ) == serialization_schema ( Foo ) == { \"$schema\" : \"http://json-schema.org/draft/2019-09/schema#\" , \"type\" : \"object\" , \"properties\" : { \"bar\" : { \"type\" : \"object\" , \"properties\" : { \"field\" : { \"type\" : \"string\" }}, \"required\" : [ \"field\" ], \"additionalProperties\" : False , }, \"baz\" : { \"type\" : \"integer\" }, }, \"additionalProperties\" : False , \"minProperties\" : 1 , \"maxProperties\" : 1 , } )","title":"JSON schema"},{"location":"graphql/data_model_and_resolvers/#graphql-schema","text":"As tagged union are not (yet?) part of the GraphQL spec, so they are just implemented as normal (input) object type with nullable fields. An error is raised if several tags are passed in input. from dataclasses import dataclass from graphql import graphql_sync from graphql.utilities import print_schema from apischema.graphql import graphql_schema from apischema.tagged_unions import Tagged , TaggedUnion @dataclass class Bar : field : str class Foo ( TaggedUnion ): bar : Tagged [ Bar ] baz : Tagged [ int ] def query ( foo : Foo ) -> Foo : return foo schema = graphql_schema ( query = [ query ]) schema_str = \"\"\" \\ type Query { query(foo: FooInput!): Foo! } type Foo { bar: Bar baz: Int } type Bar { field: String! } input FooInput { bar: BarInput baz: Int } input BarInput { field: String! } \"\"\" assert print_schema ( schema ) == schema_str query_str = \"\"\" { query(foo: {bar: {field: \"value\"}}) { bar { field } baz } } \"\"\" assert graphql_sync ( schema , query_str ) . data == { \"query\" : { \"bar\" : { \"field\" : \"value\" }, \"baz\" : None } }","title":"GraphQL schema"},{"location":"graphql/overview/","text":"GraphQL Overview \u00b6 apischema supports GraphQL through graphql-core library. You can install this dependency directly with apischema using the following extra requirement: pip install apischema [ graphql ] GraphQL supports consists of generating a GraphQL schema graphql.GraphQLSchema from your data model and endpoints (queries/mutations/subscribtions), in a similar way than the JSON schema generation. This schema can then be used through graphql-core library to query/mutate/subscribe. from dataclasses import dataclass from datetime import date , datetime from typing import Collection , Optional from uuid import UUID , uuid4 from graphql import graphql_sync , print_schema from apischema.graphql import graphql_schema , resolver @dataclass class User : id : UUID username : str birthday : Optional [ date ] = None @resolver def posts ( self ) -> Collection [ \"Post\" ]: return [ post for post in POSTS if post . author . id == self . id ] @dataclass class Post : id : UUID author : User date : datetime content : str USERS = [ User ( uuid4 (), \"foo\" ), User ( uuid4 (), \"bar\" )] POSTS = [ Post ( uuid4 (), USERS [ 0 ], datetime . now (), \"Hello world!\" )] def users () -> Collection [ User ]: return USERS def posts () -> Collection [ Post ]: return POSTS def user ( username : str ) -> Optional [ User ]: for user in users (): if user . username == username : return user else : return None schema = graphql_schema ( query = [ users , user , posts ], id_types = { UUID }) schema_str = \"\"\" \\ type Query { users: [User!]! user(username: String!): User posts: [Post!]! } type User { id: ID! username: String! birthday: Date posts: [Post!]! } scalar Date type Post { id: ID! author: User! date: Datetime! content: String! } scalar Datetime \"\"\" assert print_schema ( schema ) == schema_str query = \"\"\" { users { username posts { content } } } \"\"\" assert graphql_sync ( schema , query ) . data == { \"users\" : [ { \"username\" : \"foo\" , \"posts\" : [{ \"content\" : \"Hello world!\" }]}, { \"username\" : \"bar\" , \"posts\" : []}, ] } GraphQL is fully integrated with the rest of apischema features, especially conversions , so it's easy to integrate ORM and other custom types in the generated schema; this concerns query results but also arguments. By the way, while GraphQL doesn't support constraints, apischema still offers you all the power of its validation feature . In fact, apischema deserialize and validate all the arguments passed to resolvers. FAQ \u00b6 Is it possible to use the same classes to do both GraphQL and REST-API? \u00b6 Yes it is. GraphQL has some restrictions in comparison to JSON schema (see next section ), but this taken in account, all of your code can be reused. In fact, GraphQL endpoints can also be used both by a GraphQL API and a more traditional REST or RPC API.","title":"Overview"},{"location":"graphql/overview/#graphql-overview","text":"apischema supports GraphQL through graphql-core library. You can install this dependency directly with apischema using the following extra requirement: pip install apischema [ graphql ] GraphQL supports consists of generating a GraphQL schema graphql.GraphQLSchema from your data model and endpoints (queries/mutations/subscribtions), in a similar way than the JSON schema generation. This schema can then be used through graphql-core library to query/mutate/subscribe. from dataclasses import dataclass from datetime import date , datetime from typing import Collection , Optional from uuid import UUID , uuid4 from graphql import graphql_sync , print_schema from apischema.graphql import graphql_schema , resolver @dataclass class User : id : UUID username : str birthday : Optional [ date ] = None @resolver def posts ( self ) -> Collection [ \"Post\" ]: return [ post for post in POSTS if post . author . id == self . id ] @dataclass class Post : id : UUID author : User date : datetime content : str USERS = [ User ( uuid4 (), \"foo\" ), User ( uuid4 (), \"bar\" )] POSTS = [ Post ( uuid4 (), USERS [ 0 ], datetime . now (), \"Hello world!\" )] def users () -> Collection [ User ]: return USERS def posts () -> Collection [ Post ]: return POSTS def user ( username : str ) -> Optional [ User ]: for user in users (): if user . username == username : return user else : return None schema = graphql_schema ( query = [ users , user , posts ], id_types = { UUID }) schema_str = \"\"\" \\ type Query { users: [User!]! user(username: String!): User posts: [Post!]! } type User { id: ID! username: String! birthday: Date posts: [Post!]! } scalar Date type Post { id: ID! author: User! date: Datetime! content: String! } scalar Datetime \"\"\" assert print_schema ( schema ) == schema_str query = \"\"\" { users { username posts { content } } } \"\"\" assert graphql_sync ( schema , query ) . data == { \"users\" : [ { \"username\" : \"foo\" , \"posts\" : [{ \"content\" : \"Hello world!\" }]}, { \"username\" : \"bar\" , \"posts\" : []}, ] } GraphQL is fully integrated with the rest of apischema features, especially conversions , so it's easy to integrate ORM and other custom types in the generated schema; this concerns query results but also arguments. By the way, while GraphQL doesn't support constraints, apischema still offers you all the power of its validation feature . In fact, apischema deserialize and validate all the arguments passed to resolvers.","title":"GraphQL Overview"},{"location":"graphql/overview/#faq","text":"","title":"FAQ"},{"location":"graphql/overview/#is-it-possible-to-use-the-same-classes-to-do-both-graphql-and-rest-api","text":"Yes it is. GraphQL has some restrictions in comparison to JSON schema (see next section ), but this taken in account, all of your code can be reused. In fact, GraphQL endpoints can also be used both by a GraphQL API and a more traditional REST or RPC API.","title":"Is it possible to use the same classes to do both GraphQL and REST-API?"},{"location":"graphql/relay/","text":"Relay \u00b6 apischema provides some facilities to implement a GraphQL server following Relay GraphQL server specification . They are included in the module apischema.graphql.relay . Note These facilities are independent of each others \u2014 you could keep only mutations part and use your own identification and connection system for example. (Global) Object Identification \u00b6 apischema defines a generic relay.Node[Id] interface which can be used which can be used as base class of all identified resources. This class contains a unique generic field of type Id , which will be automatically converted into an ID! in the schema. The Id type chosen has to be serializable into a string-convertible value (it can register conversions if needed). Each node has to implement the classmethod get_by_id(cls: type[T], id: Id, info: graphql.GraphQLResolveInfo=None) -> T . All nodes defined can be retrieved using relay.nodes , while the node query is defined as relay.node . relay.nodes() can be passed to graphql_schema types parameter in order to add them in the schema even if they don't appear in any resolvers. from uuid import UUID import graphql from dataclasses import dataclass from graphql.utilities import print_schema from apischema.graphql import graphql_schema , relay @dataclass class Ship ( relay . Node [ UUID ]): # Let's use an UUID for Ship id name : str @classmethod async def get_by_id ( cls , id : UUID , info : graphql . GraphQLResolveInfo = None ): ... @dataclass class Faction ( relay . Node [ int ]): # Nodes can have different id types name : str @classmethod def get_by_id ( cls , id : int , info : graphql . GraphQLResolveInfo = None ) -> \"Faction\" : ... schema = graphql_schema ( query = [ relay . node ], types = relay . nodes ()) schema_str = \"\"\" \\ type Ship implements Node { id: ID! name: String! } interface Node { id: ID! } type Faction implements Node { id: ID! name: String! } type Query { node(id: ID!): Node! } \"\"\" assert print_schema ( schema ) == schema_str Warning For now, even if its result is note used, relay.nodes must be called before generating the schema. Global ID \u00b6 apischema defines a relay.GlobalId type with the following signature : @dataclass class GlobalId ( Generic [ Node ]): id : str node_class : type [ Node ] In fact, it is GlobalId type which is serialized and deserialized as an ID! , not the Id parameter of the Node class; apischema automatically add a field converter to make the conversion between the Id (for example an UUID ) of a given node and the corresponding GlobalId . Node instance global id can be retrieved with global_id property. from dataclasses import dataclass import graphql from apischema import serialize from apischema.graphql import graphql_schema , relay @dataclass class Faction ( relay . Node [ int ]): name : str @classmethod def get_by_id ( cls , id : int , info : graphql . GraphQLResolveInfo = None ) -> \"Faction\" : return [ Faction ( 0 , \"Empire\" ), Faction ( 1 , \"Rebels\" )][ id ] schema = graphql_schema ( query = [ relay . node ], types = relay . nodes ()) some_global_id = Faction . get_by_id ( 0 ) . global_id # Let's pick a global id ... assert some_global_id == relay . GlobalId ( \"0\" , Faction ) query = \"\"\" query factionName($id: ID!) { node(id: $id) { ... on Faction { name } } } \"\"\" assert graphql . graphql_sync ( # ... and use it in a query schema , query , variable_values = { \"id\" : serialize ( relay . GlobalId , some_global_id )} ) . data == { \"node\" : { \"name\" : \"Empire\" }} Id encoding \u00b6 Relay specifications encourage the use of base64 encoding, so apischema defines a relay.base64_encoding that you can pass to graphql_schema id_encoding parameter. Connections \u00b6 apischema provides a generic relay.Connection[Node, Cursor, Edge] type, which can be used directly without subclassing it; it's also possible to subclass it to add fields to a given connection (or to all the connection which will subclass the subclass). relay.Edge[Node, Cursor] can also be subclassed to add fields to the edges. Connection dataclass has the following declaration: @dataclass class Connection ( Generic [ Node , Cursor , Edge ]): edges : Optional [ Sequence [ Optional [ Edge ]]] has_previous_page : bool = field ( default = False , metadata = skip ) has_next_page : bool = field ( default = False , metadata = skip ) start_cursor : Optional [ Cursor ] = field ( default = None , metadata = skip ) end_cursor : Optional [ Cursor ] = field ( default = None , metadata = skip ) @resolver def page_info ( self ) -> PageInfo [ Cursor ]: ... The pageInfo field is computed by a resolver; it uses the cursors of the first and the last edge when they are not provided. Here is an example of Connection use: from typing import Optional , TypeVar import graphql from dataclasses import dataclass from graphql.utilities import print_schema from apischema.graphql import graphql_schema , relay , resolver Cursor = int # let's use an integer cursor in all our connection Node = TypeVar ( \"Node\" ) Connection = relay . Connection [ Node , Cursor , relay . Edge [ Node , Cursor ]] # Connection can now be used just like Connection[Ship] or Connection[Optional[Faction]] @dataclass class Ship : name : str @dataclass class Faction : @resolver def ships ( self , first : Optional [ int ], after : Optional [ Cursor ] ) -> Optional [ Connection [ Optional [ Ship ]]]: edges = [ relay . Edge ( Ship ( \"X-Wing\" ), 0 ), relay . Edge ( Ship ( \"B-Wing\" ), 1 )] return Connection ( edges , relay . PageInfo . from_edges ( edges )) def faction () -> Optional [ Faction ]: return Faction () schema = graphql_schema ( query = [ faction ]) schema_str = \"\"\" \\ type Query { faction: Faction } type Faction { ships(first: Int, after: Int): ShipConnection } type ShipConnection { edges: [ShipEdge] pageInfo: PageInfo! } type ShipEdge { node: Ship cursor: Int! } type Ship { name: String! } type PageInfo { hasPreviousPage: Boolean! hasNextPage: Boolean! startCursor: Int endCursor: Int } \"\"\" assert print_schema ( schema ) == schema_str query = \"\"\" { faction { ships { pageInfo { endCursor hasNextPage } edges { cursor node { name } } } } } \"\"\" assert graphql . graphql_sync ( schema , query ) . data == { \"faction\" : { \"ships\" : { \"pageInfo\" : { \"endCursor\" : 1 , \"hasNextPage\" : False }, \"edges\" : [ { \"cursor\" : 0 , \"node\" : { \"name\" : \"X-Wing\" }}, { \"cursor\" : 1 , \"node\" : { \"name\" : \"B-Wing\" }}, ], } } } Custom connections/edges \u00b6 Connections can be customizes by simply subclassing relay.Connection class and adding the additional fields. For the edges, relay.Edge can be subclassed too, and the subclass has then to be passed as type argument to the generic connection. from dataclasses import dataclass from typing import Optional , TypeVar from graphql import print_schema from apischema.graphql import graphql_schema , relay , resolver Cursor = int Node = TypeVar ( \"Node\" ) Edge = TypeVar ( \"Edge\" , bound = relay . Edge ) @dataclass class MyConnection ( relay . Connection [ Node , Cursor , Edge ]): connection_field : bool @dataclass class MyEdge ( relay . Edge [ Node , Cursor ]): edge_field : Optional [ int ] Connection = MyConnection [ Node , MyEdge [ Node ]] @dataclass class Ship : name : str @dataclass class Faction : @resolver def ships ( self , first : Optional [ int ], after : Optional [ Cursor ] ) -> Optional [ Connection [ Optional [ Ship ]]]: ... def faction () -> Optional [ Faction ]: return Faction () schema = graphql_schema ( query = [ faction ]) schema_str = \"\"\" \\ type Query { faction: Faction } type Faction { ships(first: Int, after: Int): ShipConnection } type ShipConnection { edges: [ShipEdge] pageInfo: PageInfo! connectionField: Boolean! } type ShipEdge { node: Ship cursor: Int! edgeField: Int } type Ship { name: String! } type PageInfo { hasPreviousPage: Boolean! hasNextPage: Boolean! startCursor: Int endCursor: Int } \"\"\" assert print_schema ( schema ) == schema_str Mutations \u00b6 Relay compliant mutations can be declared with a dataclass subclassing the relay.Mutation class; its fields will be put in the payload type of the mutation. This class must implement a classmethod / staticmethod name mutate ; it can be synchronous or asynchronous. The arguments of the method will correspond to the input type fields. The mutation will be named after the name of the mutation class. All the mutations declared can be retrieved with relay.mutations , in order to be passed to graphql_schema . from dataclasses import dataclass from graphql.utilities import print_schema from apischema.graphql import graphql_schema , relay @dataclass class Ship : ... @dataclass class Faction : ... @dataclass class IntroduceShip ( relay . Mutation ): ship : Ship faction : Faction @staticmethod def mutate ( faction_id : str , ship_name : str ) -> \"IntroduceShip\" : ... def hello () -> str : return \"world\" schema = graphql_schema ( query = [ hello ], mutation = relay . mutations ()) schema_str = \"\"\" \\ type Query { hello: String! } type Mutation { introduceShip(input: IntroduceShipInput!): IntroduceShipPayload! } type IntroduceShipPayload { ship: Ship! faction: Faction! clientMutationId: String } type Ship type Faction input IntroduceShipInput { factionId: String! shipName: String! clientMutationId: String } \"\"\" assert print_schema ( schema ) == schema_str ClientMutationId \u00b6 As you can see in the previous example, the field named clientMutationId is automatically added to the input and the payload types. The forward of the mutation id from the input to the payload is automatically handled. It's value can be accessed by declaring a parameter of type relay.ClientMutationId \u2014 even if the parameter is not named client_mutation_id , it will be renamed internally. This feature is controlled by a Mutation class variable _client_mutation_id , with 3 possible values: None (automatic, the default): clientMutationId field will be nullable unless it's declared as a required parameter (without default value) in the mutate method. False : their will be no clientMutationId field added (having a dedicated parameter will raise an error) True : clientMutationId is added and forced to be non-null. from dataclasses import dataclass from graphql.utilities import print_schema from apischema.graphql import graphql_schema , relay @dataclass class Ship : ... @dataclass class Faction : ... @dataclass class IntroduceShip ( relay . Mutation ): ship : Ship faction : Faction @staticmethod def mutate ( # mut_id is required because no default value faction_id : str , ship_name : str , mut_id : relay . ClientMutationId , ) -> \"IntroduceShip\" : ... def hello () -> str : return \"world\" schema = graphql_schema ( query = [ hello ], mutation = relay . mutations ()) # clientMutationId field becomes non nullable in introduceShip types schema_str = \"\"\" \\ type Query { hello: String! } type Mutation { introduceShip(input: IntroduceShipInput!): IntroduceShipPayload! } type IntroduceShipPayload { ship: Ship! faction: Faction! clientMutationId: String! } type Ship type Faction input IntroduceShipInput { factionId: String! shipName: String! clientMutationId: String! } \"\"\" assert print_schema ( schema ) == schema_str Error handling and other resolver arguments \u00b6 Relay mutation are operations , so they can be configured with the same parameters. As they are declared as classes, parameters will be passed as class variables, prefixed by _ ( error_handler becomes _error_handler ) Note Because parameters are class variables, you can reuse them by setting their value in a base class; for example, to share a same error_handler in a group of mutations.","title":"Relay"},{"location":"graphql/relay/#relay","text":"apischema provides some facilities to implement a GraphQL server following Relay GraphQL server specification . They are included in the module apischema.graphql.relay . Note These facilities are independent of each others \u2014 you could keep only mutations part and use your own identification and connection system for example.","title":"Relay"},{"location":"graphql/relay/#global-object-identification","text":"apischema defines a generic relay.Node[Id] interface which can be used which can be used as base class of all identified resources. This class contains a unique generic field of type Id , which will be automatically converted into an ID! in the schema. The Id type chosen has to be serializable into a string-convertible value (it can register conversions if needed). Each node has to implement the classmethod get_by_id(cls: type[T], id: Id, info: graphql.GraphQLResolveInfo=None) -> T . All nodes defined can be retrieved using relay.nodes , while the node query is defined as relay.node . relay.nodes() can be passed to graphql_schema types parameter in order to add them in the schema even if they don't appear in any resolvers. from uuid import UUID import graphql from dataclasses import dataclass from graphql.utilities import print_schema from apischema.graphql import graphql_schema , relay @dataclass class Ship ( relay . Node [ UUID ]): # Let's use an UUID for Ship id name : str @classmethod async def get_by_id ( cls , id : UUID , info : graphql . GraphQLResolveInfo = None ): ... @dataclass class Faction ( relay . Node [ int ]): # Nodes can have different id types name : str @classmethod def get_by_id ( cls , id : int , info : graphql . GraphQLResolveInfo = None ) -> \"Faction\" : ... schema = graphql_schema ( query = [ relay . node ], types = relay . nodes ()) schema_str = \"\"\" \\ type Ship implements Node { id: ID! name: String! } interface Node { id: ID! } type Faction implements Node { id: ID! name: String! } type Query { node(id: ID!): Node! } \"\"\" assert print_schema ( schema ) == schema_str Warning For now, even if its result is note used, relay.nodes must be called before generating the schema.","title":"(Global) Object Identification"},{"location":"graphql/relay/#global-id","text":"apischema defines a relay.GlobalId type with the following signature : @dataclass class GlobalId ( Generic [ Node ]): id : str node_class : type [ Node ] In fact, it is GlobalId type which is serialized and deserialized as an ID! , not the Id parameter of the Node class; apischema automatically add a field converter to make the conversion between the Id (for example an UUID ) of a given node and the corresponding GlobalId . Node instance global id can be retrieved with global_id property. from dataclasses import dataclass import graphql from apischema import serialize from apischema.graphql import graphql_schema , relay @dataclass class Faction ( relay . Node [ int ]): name : str @classmethod def get_by_id ( cls , id : int , info : graphql . GraphQLResolveInfo = None ) -> \"Faction\" : return [ Faction ( 0 , \"Empire\" ), Faction ( 1 , \"Rebels\" )][ id ] schema = graphql_schema ( query = [ relay . node ], types = relay . nodes ()) some_global_id = Faction . get_by_id ( 0 ) . global_id # Let's pick a global id ... assert some_global_id == relay . GlobalId ( \"0\" , Faction ) query = \"\"\" query factionName($id: ID!) { node(id: $id) { ... on Faction { name } } } \"\"\" assert graphql . graphql_sync ( # ... and use it in a query schema , query , variable_values = { \"id\" : serialize ( relay . GlobalId , some_global_id )} ) . data == { \"node\" : { \"name\" : \"Empire\" }}","title":"Global ID"},{"location":"graphql/relay/#id-encoding","text":"Relay specifications encourage the use of base64 encoding, so apischema defines a relay.base64_encoding that you can pass to graphql_schema id_encoding parameter.","title":"Id encoding"},{"location":"graphql/relay/#connections","text":"apischema provides a generic relay.Connection[Node, Cursor, Edge] type, which can be used directly without subclassing it; it's also possible to subclass it to add fields to a given connection (or to all the connection which will subclass the subclass). relay.Edge[Node, Cursor] can also be subclassed to add fields to the edges. Connection dataclass has the following declaration: @dataclass class Connection ( Generic [ Node , Cursor , Edge ]): edges : Optional [ Sequence [ Optional [ Edge ]]] has_previous_page : bool = field ( default = False , metadata = skip ) has_next_page : bool = field ( default = False , metadata = skip ) start_cursor : Optional [ Cursor ] = field ( default = None , metadata = skip ) end_cursor : Optional [ Cursor ] = field ( default = None , metadata = skip ) @resolver def page_info ( self ) -> PageInfo [ Cursor ]: ... The pageInfo field is computed by a resolver; it uses the cursors of the first and the last edge when they are not provided. Here is an example of Connection use: from typing import Optional , TypeVar import graphql from dataclasses import dataclass from graphql.utilities import print_schema from apischema.graphql import graphql_schema , relay , resolver Cursor = int # let's use an integer cursor in all our connection Node = TypeVar ( \"Node\" ) Connection = relay . Connection [ Node , Cursor , relay . Edge [ Node , Cursor ]] # Connection can now be used just like Connection[Ship] or Connection[Optional[Faction]] @dataclass class Ship : name : str @dataclass class Faction : @resolver def ships ( self , first : Optional [ int ], after : Optional [ Cursor ] ) -> Optional [ Connection [ Optional [ Ship ]]]: edges = [ relay . Edge ( Ship ( \"X-Wing\" ), 0 ), relay . Edge ( Ship ( \"B-Wing\" ), 1 )] return Connection ( edges , relay . PageInfo . from_edges ( edges )) def faction () -> Optional [ Faction ]: return Faction () schema = graphql_schema ( query = [ faction ]) schema_str = \"\"\" \\ type Query { faction: Faction } type Faction { ships(first: Int, after: Int): ShipConnection } type ShipConnection { edges: [ShipEdge] pageInfo: PageInfo! } type ShipEdge { node: Ship cursor: Int! } type Ship { name: String! } type PageInfo { hasPreviousPage: Boolean! hasNextPage: Boolean! startCursor: Int endCursor: Int } \"\"\" assert print_schema ( schema ) == schema_str query = \"\"\" { faction { ships { pageInfo { endCursor hasNextPage } edges { cursor node { name } } } } } \"\"\" assert graphql . graphql_sync ( schema , query ) . data == { \"faction\" : { \"ships\" : { \"pageInfo\" : { \"endCursor\" : 1 , \"hasNextPage\" : False }, \"edges\" : [ { \"cursor\" : 0 , \"node\" : { \"name\" : \"X-Wing\" }}, { \"cursor\" : 1 , \"node\" : { \"name\" : \"B-Wing\" }}, ], } } }","title":"Connections"},{"location":"graphql/relay/#custom-connectionsedges","text":"Connections can be customizes by simply subclassing relay.Connection class and adding the additional fields. For the edges, relay.Edge can be subclassed too, and the subclass has then to be passed as type argument to the generic connection. from dataclasses import dataclass from typing import Optional , TypeVar from graphql import print_schema from apischema.graphql import graphql_schema , relay , resolver Cursor = int Node = TypeVar ( \"Node\" ) Edge = TypeVar ( \"Edge\" , bound = relay . Edge ) @dataclass class MyConnection ( relay . Connection [ Node , Cursor , Edge ]): connection_field : bool @dataclass class MyEdge ( relay . Edge [ Node , Cursor ]): edge_field : Optional [ int ] Connection = MyConnection [ Node , MyEdge [ Node ]] @dataclass class Ship : name : str @dataclass class Faction : @resolver def ships ( self , first : Optional [ int ], after : Optional [ Cursor ] ) -> Optional [ Connection [ Optional [ Ship ]]]: ... def faction () -> Optional [ Faction ]: return Faction () schema = graphql_schema ( query = [ faction ]) schema_str = \"\"\" \\ type Query { faction: Faction } type Faction { ships(first: Int, after: Int): ShipConnection } type ShipConnection { edges: [ShipEdge] pageInfo: PageInfo! connectionField: Boolean! } type ShipEdge { node: Ship cursor: Int! edgeField: Int } type Ship { name: String! } type PageInfo { hasPreviousPage: Boolean! hasNextPage: Boolean! startCursor: Int endCursor: Int } \"\"\" assert print_schema ( schema ) == schema_str","title":"Custom connections/edges"},{"location":"graphql/relay/#mutations","text":"Relay compliant mutations can be declared with a dataclass subclassing the relay.Mutation class; its fields will be put in the payload type of the mutation. This class must implement a classmethod / staticmethod name mutate ; it can be synchronous or asynchronous. The arguments of the method will correspond to the input type fields. The mutation will be named after the name of the mutation class. All the mutations declared can be retrieved with relay.mutations , in order to be passed to graphql_schema . from dataclasses import dataclass from graphql.utilities import print_schema from apischema.graphql import graphql_schema , relay @dataclass class Ship : ... @dataclass class Faction : ... @dataclass class IntroduceShip ( relay . Mutation ): ship : Ship faction : Faction @staticmethod def mutate ( faction_id : str , ship_name : str ) -> \"IntroduceShip\" : ... def hello () -> str : return \"world\" schema = graphql_schema ( query = [ hello ], mutation = relay . mutations ()) schema_str = \"\"\" \\ type Query { hello: String! } type Mutation { introduceShip(input: IntroduceShipInput!): IntroduceShipPayload! } type IntroduceShipPayload { ship: Ship! faction: Faction! clientMutationId: String } type Ship type Faction input IntroduceShipInput { factionId: String! shipName: String! clientMutationId: String } \"\"\" assert print_schema ( schema ) == schema_str","title":"Mutations"},{"location":"graphql/relay/#clientmutationid","text":"As you can see in the previous example, the field named clientMutationId is automatically added to the input and the payload types. The forward of the mutation id from the input to the payload is automatically handled. It's value can be accessed by declaring a parameter of type relay.ClientMutationId \u2014 even if the parameter is not named client_mutation_id , it will be renamed internally. This feature is controlled by a Mutation class variable _client_mutation_id , with 3 possible values: None (automatic, the default): clientMutationId field will be nullable unless it's declared as a required parameter (without default value) in the mutate method. False : their will be no clientMutationId field added (having a dedicated parameter will raise an error) True : clientMutationId is added and forced to be non-null. from dataclasses import dataclass from graphql.utilities import print_schema from apischema.graphql import graphql_schema , relay @dataclass class Ship : ... @dataclass class Faction : ... @dataclass class IntroduceShip ( relay . Mutation ): ship : Ship faction : Faction @staticmethod def mutate ( # mut_id is required because no default value faction_id : str , ship_name : str , mut_id : relay . ClientMutationId , ) -> \"IntroduceShip\" : ... def hello () -> str : return \"world\" schema = graphql_schema ( query = [ hello ], mutation = relay . mutations ()) # clientMutationId field becomes non nullable in introduceShip types schema_str = \"\"\" \\ type Query { hello: String! } type Mutation { introduceShip(input: IntroduceShipInput!): IntroduceShipPayload! } type IntroduceShipPayload { ship: Ship! faction: Faction! clientMutationId: String! } type Ship type Faction input IntroduceShipInput { factionId: String! shipName: String! clientMutationId: String! } \"\"\" assert print_schema ( schema ) == schema_str","title":"ClientMutationId"},{"location":"graphql/relay/#error-handling-and-other-resolver-arguments","text":"Relay mutation are operations , so they can be configured with the same parameters. As they are declared as classes, parameters will be passed as class variables, prefixed by _ ( error_handler becomes _error_handler ) Note Because parameters are class variables, you can reuse them by setting their value in a base class; for example, to share a same error_handler in a group of mutations.","title":"Error handling and other resolver arguments"},{"location":"graphql/schema/","text":"GraphQL schema \u00b6 GraphQL schema is generated by passing all the operations (query/mutation/subscription) functions to apischema.graphql.graphql_schema . Functions parameters and return types are then processed by apischema to generate the Query / Mutation / Subscription types with their resolvers/subscribers, which are then passed to graphql.GraphQLSchema . In fact, graphql_schema is just a wrapper around graphql.GraphQLSchema (same parameters plus a few extras); it just uses apischema abstraction to build GraphQL object types directly from your code. Operations metadata \u00b6 GraphQL operations can be passed to graphql_schema either using simple functions or wrapping it into apischema.graphql.Query / apischema.graphql.Mutation / apischema.graphql.Subscription . These wrappers have the same parameters as apischema.graphql.resolver : alias , conversions , error_handler and schema ( Subscription has an additional parameter ). from dataclasses import dataclass from graphql import print_schema from apischema.graphql import Query , graphql_schema , resolver @dataclass class Foo : @resolver async def bar ( self , arg : int = 0 ) -> str : ... async def get_foo () -> Foo : ... schema = graphql_schema ( query = [ Query ( get_foo , alias = \"foo\" , error_handler = None )]) schema_str = \"\"\" \\ type Query { foo: Foo } type Foo { bar(arg: Int! = 0): String! } \"\"\" assert print_schema ( schema ) == schema_str camelCase \u00b6 GraphQL use camelCase as a convention for resolvers; apischema follows this convention by automatically convert all resolver names (and their parameters) to camelCase . graphql_schema has an aliaser parameter if you want to use another case. Type names \u00b6 Schema types are named the same way they are in generated JSON schema: type name is used by default, and it can be overridden using apischema.type_name from dataclasses import dataclass from typing import Optional from graphql import print_schema from apischema import type_name from apischema.graphql import graphql_schema @type_name ( \"Foo\" ) @dataclass class FooFoo : bar : int def foo () -> Optional [ FooFoo ]: ... schema = graphql_schema ( query = [ foo ]) schema_str = \"\"\" \\ type Query { foo: Foo } type Foo { bar: Int! } \"\"\" assert print_schema ( schema ) == schema_str Note Type names can be distinguished between JSON schema and GraphQL schema using type_name named parameter. Indeed, type_name(\"foo\") is equivalent to type_name(json_schema=\"foo\", graphql=\"foo\") . However, in GraphQL schema, unions must be named, so typing.Union used should be annotated with apischema.type_name . graphql_schema also provides a union_ref parameter which can be passed as a function to generate a type name from the union argument. Default union_ref is \"Or\".join meaning typing.Union[Foo, Bar] will result in union FooOrBar = Foo | Bar from dataclasses import dataclass from typing import Union from graphql import print_schema from apischema.graphql import graphql_schema @dataclass class Foo : foo : int @dataclass class Bar : bar : int def foo_or_bar () -> Union [ Foo , Bar ]: ... # union_ref default value is made explicit here schema = graphql_schema ( query = [ foo_or_bar ], union_name = \"Or\" . join ) schema_str = \"\"\" \\ type Query { fooOrBar: FooOrBar! } union FooOrBar = Foo | Bar type Foo { foo: Int! } type Bar { bar: Int! } \"\"\" assert print_schema ( schema ) == schema_str Additional types \u00b6 apischema will only include in the schema the types annotating resolvers. However, it is possible to add other types by using the types parameter of graphql_schema . This is especially useful to add interface implementations where only interface is used in resolver types. from dataclasses import dataclass from graphql import print_schema from apischema.graphql import graphql_schema , interface @interface @dataclass class Bar : bar : int @dataclass class Foo ( Bar ): baz : str def bar () -> Bar : ... schema = graphql_schema ( query = [ bar ], types = [ Foo ]) # type Foo would have not been present if Foo was not put in types schema_str = \"\"\" \\ type Foo implements Bar { bar: Int! baz: String! } interface Bar { bar: Int! } type Query { bar: Bar! } \"\"\" assert print_schema ( schema ) == schema_str Subscriptions \u00b6 Subscriptions are particular operations which must return an AsyncIterable ; this event generator can come with a dedicated resolver to post process the event. Event generator only \u00b6 import asyncio from typing import AsyncIterable import graphql from graphql import print_schema from apischema.graphql import graphql_schema def hello () -> str : return \"world\" async def events () -> AsyncIterable [ str ]: yield \"bonjour\" yield \"au revoir\" schema = graphql_schema ( query = [ hello ], subscription = [ events ]) schema_str = \"\"\" \\ type Query { hello: String! } type Subscription { events: String! } \"\"\" assert print_schema ( schema ) == schema_str async def test (): subscription = await graphql . subscribe ( schema , graphql . parse ( \"subscription {events} \" ) ) assert [ event . data async for event in subscription ] == [ { \"events\" : \"bonjour\" }, { \"events\" : \"au revoir\" }, ] asyncio . run ( test ()) Note Because there is no post-processing of generated event in a dedicated resolver, error_handler cannot be called, but it will still modify the type of the event. Event generator + resolver \u00b6 A resolver can be added by using the resolver parameter of Subscription . In this case, apischema will map subscription name, parameters and return type on the resolver instead of the event generator. It allows using the same event generator with several resolvers to create different subscriptions. The first resolver argument will be the event yielded by the event generator. import asyncio from dataclasses import dataclass from typing import AsyncIterable import graphql from graphql import print_schema from apischema.graphql import Subscription , graphql_schema def hello () -> str : return \"world\" async def events () -> AsyncIterable [ str ]: yield \"bonjour\" yield \"au revoir\" @dataclass class Message : body : str # Message can also be used directly as a function schema = graphql_schema ( query = [ hello ], subscription = [ Subscription ( events , alias = \"messageReceived\" , resolver = Message )], ) schema_str = \"\"\" \\ type Query { hello: String! } type Subscription { messageReceived: Message! } type Message { body: String! } \"\"\" assert print_schema ( schema ) == schema_str async def test (): subscription = await graphql . subscribe ( schema , graphql . parse ( \"subscription {messageReceived {body} }\" ) ) assert [ event . data async for event in subscription ] == [ { \"messageReceived\" : { \"body\" : \"bonjour\" }}, { \"messageReceived\" : { \"body\" : \"au revoir\" }}, ] asyncio . run ( test ())","title":"GraphQL schema"},{"location":"graphql/schema/#graphql-schema","text":"GraphQL schema is generated by passing all the operations (query/mutation/subscription) functions to apischema.graphql.graphql_schema . Functions parameters and return types are then processed by apischema to generate the Query / Mutation / Subscription types with their resolvers/subscribers, which are then passed to graphql.GraphQLSchema . In fact, graphql_schema is just a wrapper around graphql.GraphQLSchema (same parameters plus a few extras); it just uses apischema abstraction to build GraphQL object types directly from your code.","title":"GraphQL schema"},{"location":"graphql/schema/#operations-metadata","text":"GraphQL operations can be passed to graphql_schema either using simple functions or wrapping it into apischema.graphql.Query / apischema.graphql.Mutation / apischema.graphql.Subscription . These wrappers have the same parameters as apischema.graphql.resolver : alias , conversions , error_handler and schema ( Subscription has an additional parameter ). from dataclasses import dataclass from graphql import print_schema from apischema.graphql import Query , graphql_schema , resolver @dataclass class Foo : @resolver async def bar ( self , arg : int = 0 ) -> str : ... async def get_foo () -> Foo : ... schema = graphql_schema ( query = [ Query ( get_foo , alias = \"foo\" , error_handler = None )]) schema_str = \"\"\" \\ type Query { foo: Foo } type Foo { bar(arg: Int! = 0): String! } \"\"\" assert print_schema ( schema ) == schema_str","title":"Operations metadata"},{"location":"graphql/schema/#camelcase","text":"GraphQL use camelCase as a convention for resolvers; apischema follows this convention by automatically convert all resolver names (and their parameters) to camelCase . graphql_schema has an aliaser parameter if you want to use another case.","title":"camelCase"},{"location":"graphql/schema/#type-names","text":"Schema types are named the same way they are in generated JSON schema: type name is used by default, and it can be overridden using apischema.type_name from dataclasses import dataclass from typing import Optional from graphql import print_schema from apischema import type_name from apischema.graphql import graphql_schema @type_name ( \"Foo\" ) @dataclass class FooFoo : bar : int def foo () -> Optional [ FooFoo ]: ... schema = graphql_schema ( query = [ foo ]) schema_str = \"\"\" \\ type Query { foo: Foo } type Foo { bar: Int! } \"\"\" assert print_schema ( schema ) == schema_str Note Type names can be distinguished between JSON schema and GraphQL schema using type_name named parameter. Indeed, type_name(\"foo\") is equivalent to type_name(json_schema=\"foo\", graphql=\"foo\") . However, in GraphQL schema, unions must be named, so typing.Union used should be annotated with apischema.type_name . graphql_schema also provides a union_ref parameter which can be passed as a function to generate a type name from the union argument. Default union_ref is \"Or\".join meaning typing.Union[Foo, Bar] will result in union FooOrBar = Foo | Bar from dataclasses import dataclass from typing import Union from graphql import print_schema from apischema.graphql import graphql_schema @dataclass class Foo : foo : int @dataclass class Bar : bar : int def foo_or_bar () -> Union [ Foo , Bar ]: ... # union_ref default value is made explicit here schema = graphql_schema ( query = [ foo_or_bar ], union_name = \"Or\" . join ) schema_str = \"\"\" \\ type Query { fooOrBar: FooOrBar! } union FooOrBar = Foo | Bar type Foo { foo: Int! } type Bar { bar: Int! } \"\"\" assert print_schema ( schema ) == schema_str","title":"Type names"},{"location":"graphql/schema/#additional-types","text":"apischema will only include in the schema the types annotating resolvers. However, it is possible to add other types by using the types parameter of graphql_schema . This is especially useful to add interface implementations where only interface is used in resolver types. from dataclasses import dataclass from graphql import print_schema from apischema.graphql import graphql_schema , interface @interface @dataclass class Bar : bar : int @dataclass class Foo ( Bar ): baz : str def bar () -> Bar : ... schema = graphql_schema ( query = [ bar ], types = [ Foo ]) # type Foo would have not been present if Foo was not put in types schema_str = \"\"\" \\ type Foo implements Bar { bar: Int! baz: String! } interface Bar { bar: Int! } type Query { bar: Bar! } \"\"\" assert print_schema ( schema ) == schema_str","title":"Additional types"},{"location":"graphql/schema/#subscriptions","text":"Subscriptions are particular operations which must return an AsyncIterable ; this event generator can come with a dedicated resolver to post process the event.","title":"Subscriptions"},{"location":"graphql/schema/#event-generator-only","text":"import asyncio from typing import AsyncIterable import graphql from graphql import print_schema from apischema.graphql import graphql_schema def hello () -> str : return \"world\" async def events () -> AsyncIterable [ str ]: yield \"bonjour\" yield \"au revoir\" schema = graphql_schema ( query = [ hello ], subscription = [ events ]) schema_str = \"\"\" \\ type Query { hello: String! } type Subscription { events: String! } \"\"\" assert print_schema ( schema ) == schema_str async def test (): subscription = await graphql . subscribe ( schema , graphql . parse ( \"subscription {events} \" ) ) assert [ event . data async for event in subscription ] == [ { \"events\" : \"bonjour\" }, { \"events\" : \"au revoir\" }, ] asyncio . run ( test ()) Note Because there is no post-processing of generated event in a dedicated resolver, error_handler cannot be called, but it will still modify the type of the event.","title":"Event generator only"},{"location":"graphql/schema/#event-generator-resolver","text":"A resolver can be added by using the resolver parameter of Subscription . In this case, apischema will map subscription name, parameters and return type on the resolver instead of the event generator. It allows using the same event generator with several resolvers to create different subscriptions. The first resolver argument will be the event yielded by the event generator. import asyncio from dataclasses import dataclass from typing import AsyncIterable import graphql from graphql import print_schema from apischema.graphql import Subscription , graphql_schema def hello () -> str : return \"world\" async def events () -> AsyncIterable [ str ]: yield \"bonjour\" yield \"au revoir\" @dataclass class Message : body : str # Message can also be used directly as a function schema = graphql_schema ( query = [ hello ], subscription = [ Subscription ( events , alias = \"messageReceived\" , resolver = Message )], ) schema_str = \"\"\" \\ type Query { hello: String! } type Subscription { messageReceived: Message! } type Message { body: String! } \"\"\" assert print_schema ( schema ) == schema_str async def test (): subscription = await graphql . subscribe ( schema , graphql . parse ( \"subscription {messageReceived {body} }\" ) ) assert [ event . data async for event in subscription ] == [ { \"messageReceived\" : { \"body\" : \"bonjour\" }}, { \"messageReceived\" : { \"body\" : \"au revoir\" }}, ] asyncio . run ( test ())","title":"Event generator + resolver"}]}